Applications/CausalLM/res/gemma3/embedding
Applications/CausalLM/res/gemma3/embedding/nntr_gemma3_emb.bin
[Embedding Output] Shape: Shape: 1:1:1024:768 [ FP32 : NCHW ]

[Embedding Output] Token 0 First 3: [-2.4773 -1.26365 0.382693 ]
[Embedding Output] Token 0 Last 3: [0.5694 -0.491872 0.0579476 ]
[Embedding Output] Token 1 First 3: [-0.472067 -0.118361 -2.42951 ]
[Embedding Output] Token 1 Last 3: [2.04454 -0.339647 -0.597494 ]
[Embedding Output] Token 2 First 3: [-0.406472 1.58556 -3.36334 ]
[Embedding Output] Token 2 Last 3: [5.72101 3.19602 -1.24842 ]
[Embedding Output] Token 3 First 3: [0 0 0 ]
[Embedding Output] Token 3 Last 3: [0 0 0 ]
[Embedding Output] Token 4 First 3: [0 0 0 ]
[Embedding Output] Token 4 Last 3: [0 0 0 ]
[Embedding Output] Token 5 First 3: [0 0 0 ]
[Embedding Output] Token 5 Last 3: [0 0 0 ]
[Embedding Output] Token 6 First 3: [0 0 0 ]
[Embedding Output] Token 6 Last 3: [0 0 0 ]
[Embedding Output] Token 7 First 3: [0 0 0 ]
[Embedding Output] Token 7 Last 3: [0 0 0 ]
[Embedding Output] Token 8 First 3: [0 0 0 ]
[Embedding Output] Token 8 Last 3: [0 0 0 ]
[Embedding Output] Token 9 First 3: [0 0 0 ]
[Embedding Output] Token 9 Last 3: [0 0 0 ]
[Embedding Output] Token 10 First 3: [0 0 0 ]
[Embedding Output] Token 10 Last 3: [0 0 0 ]
[Embedding Output] Token 11 First 3: [0 0 0 ]
[Embedding Output] Token 11 Last 3: [0 0 0 ]
[Embedding Output] Token 12 First 3: [0 0 0 ]
[Embedding Output] Token 12 Last 3: [0 0 0 ]
[Embedding Output] Token 13 First 3: [0 0 0 ]
[Embedding Output] Token 13 Last 3: [0 0 0 ]
[Embedding Output] Token 14 First 3: [0 0 0 ]
[Embedding Output] Token 14 Last 3: [0 0 0 ]
[Embedding Output] Token 15 First 3: [0 0 0 ]
[Embedding Output] Token 15 Last 3: [0 0 0 ]
[Embedding Output] Token 16 First 3: [0 0 0 ]
[Embedding Output] Token 16 Last 3: [0 0 0 ]
[Embedding Output] Token 17 First 3: [0 0 0 ]
[Embedding Output] Token 17 Last 3: [0 0 0 ]
[Embedding Output] Token 18 First 3: [0 0 0 ]
[Embedding Output] Token 18 Last 3: [0 0 0 ]
[Embedding Output] Token 19 First 3: [0 0 0 ]
[Embedding Output] Token 19 Last 3: [0 0 0 ]
[Embedding Output] Token 20 First 3: [0 0 0 ]
[Embedding Output] Token 20 Last 3: [0 0 0 ]
[Embedding Output] Token 21 First 3: [0 0 0 ]
[Embedding Output] Token 21 Last 3: [0 0 0 ]
[Embedding Output] Token 22 First 3: [0 0 0 ]
[Embedding Output] Token 22 Last 3: [0 0 0 ]
[Embedding Output] Token 23 First 3: [0 0 0 ]
[Embedding Output] Token 23 Last 3: [0 0 0 ]
[Embedding Output] Token 24 First 3: [0 0 0 ]
[Embedding Output] Token 24 Last 3: [0 0 0 ]
[Embedding Output] Token 25 First 3: [0 0 0 ]
[Embedding Output] Token 25 Last 3: [0 0 0 ]
[Embedding Output] Token 26 First 3: [0 0 0 ]
[Embedding Output] Token 26 Last 3: [0 0 0 ]
[Embedding Output] Token 27 First 3: [0 0 0 ]
[Embedding Output] Token 27 Last 3: [0 0 0 ]
[Embedding Output] Token 28 First 3: [0 0 0 ]
[Embedding Output] Token 28 Last 3: [0 0 0 ]
[Embedding Output] Token 29 First 3: [0 0 0 ]
[Embedding Output] Token 29 Last 3: [0 0 0 ]
[Embedding Output] Token 30 First 3: [0 0 0 ]
[Embedding Output] Token 30 Last 3: [0 0 0 ]
[Embedding Output] Token 31 First 3: [0 0 0 ]
[Embedding Output] Token 31 Last 3: [0 0 0 ]
[Embedding Output] Token 32 First 3: [0 0 0 ]
[Embedding Output] Token 32 Last 3: [0 0 0 ]
[Embedding Output] Token 33 First 3: [0 0 0 ]
[Embedding Output] Token 33 Last 3: [0 0 0 ]
[Embedding Output] Token 34 First 3: [0 0 0 ]
[Embedding Output] Token 34 Last 3: [0 0 0 ]
[Embedding Output] Token 35 First 3: [0 0 0 ]
[Embedding Output] Token 35 Last 3: [0 0 0 ]
[Embedding Output] Token 36 First 3: [0 0 0 ]
[Embedding Output] Token 36 Last 3: [0 0 0 ]
[Embedding Output] Token 37 First 3: [0 0 0 ]
[Embedding Output] Token 37 Last 3: [0 0 0 ]
[Embedding Output] Token 38 First 3: [0 0 0 ]
[Embedding Output] Token 38 Last 3: [0 0 0 ]
[Embedding Output] Token 39 First 3: [0 0 0 ]
[Embedding Output] Token 39 Last 3: [0 0 0 ]
[Embedding Output] Token 40 First 3: [0 0 0 ]
[Embedding Output] Token 40 Last 3: [0 0 0 ]
[Embedding Output] Token 41 First 3: [0 0 0 ]
[Embedding Output] Token 41 Last 3: [0 0 0 ]
[Embedding Output] Token 42 First 3: [0 0 0 ]
[Embedding Output] Token 42 Last 3: [0 0 0 ]
[Embedding Output] Token 43 First 3: [0 0 0 ]
[Embedding Output] Token 43 Last 3: [0 0 0 ]
[Embedding Output] Token 44 First 3: [0 0 0 ]
[Embedding Output] Token 44 Last 3: [0 0 0 ]
[Embedding Output] Token 45 First 3: [0 0 0 ]
[Embedding Output] Token 45 Last 3: [0 0 0 ]
[Embedding Output] Token 46 First 3: [0 0 0 ]
[Embedding Output] Token 46 Last 3: [0 0 0 ]
[Embedding Output] Token 47 First 3: [0 0 0 ]
[Embedding Output] Token 47 Last 3: [0 0 0 ]
[Embedding Output] Token 48 First 3: [0 0 0 ]
[Embedding Output] Token 48 Last 3: [0 0 0 ]
[Embedding Output] Token 49 First 3: [0 0 0 ]
[Embedding Output] Token 49 Last 3: [0 0 0 ]
[Embedding Output] Token 50 First 3: [0 0 0 ]
[Embedding Output] Token 50 Last 3: [0 0 0 ]
[Embedding Output] Token 51 First 3: [0 0 0 ]
[Embedding Output] Token 51 Last 3: [0 0 0 ]
[Embedding Output] Token 52 First 3: [0 0 0 ]
[Embedding Output] Token 52 Last 3: [0 0 0 ]
[Embedding Output] Token 53 First 3: [0 0 0 ]
[Embedding Output] Token 53 Last 3: [0 0 0 ]
[Embedding Output] Token 54 First 3: [0 0 0 ]
[Embedding Output] Token 54 Last 3: [0 0 0 ]
[Embedding Output] Token 55 First 3: [0 0 0 ]
[Embedding Output] Token 55 Last 3: [0 0 0 ]
[Embedding Output] Token 56 First 3: [0 0 0 ]
[Embedding Output] Token 56 Last 3: [0 0 0 ]
[Embedding Output] Token 57 First 3: [0 0 0 ]
[Embedding Output] Token 57 Last 3: [0 0 0 ]
[Embedding Output] Token 58 First 3: [0 0 0 ]
[Embedding Output] Token 58 Last 3: [0 0 0 ]
[Embedding Output] Token 59 First 3: [0 0 0 ]
[Embedding Output] Token 59 Last 3: [0 0 0 ]
[Embedding Output] Token 60 First 3: [0 0 0 ]
[Embedding Output] Token 60 Last 3: [0 0 0 ]
[Embedding Output] Token 61 First 3: [0 0 0 ]
[Embedding Output] Token 61 Last 3: [0 0 0 ]
[Embedding Output] Token 62 First 3: [0 0 0 ]
[Embedding Output] Token 62 Last 3: [0 0 0 ]
[Embedding Output] Token 63 First 3: [0 0 0 ]
[Embedding Output] Token 63 Last 3: [0 0 0 ]
[Embedding Output] Token 64 First 3: [0 0 0 ]
[Embedding Output] Token 64 Last 3: [0 0 0 ]
[Embedding Output] Token 65 First 3: [0 0 0 ]
[Embedding Output] Token 65 Last 3: [0 0 0 ]
[Embedding Output] Token 66 First 3: [0 0 0 ]
[Embedding Output] Token 66 Last 3: [0 0 0 ]
[Embedding Output] Token 67 First 3: [0 0 0 ]
[Embedding Output] Token 67 Last 3: [0 0 0 ]
[Embedding Output] Token 68 First 3: [0 0 0 ]
[Embedding Output] Token 68 Last 3: [0 0 0 ]
[Embedding Output] Token 69 First 3: [0 0 0 ]
[Embedding Output] Token 69 Last 3: [0 0 0 ]
[Embedding Output] Token 70 First 3: [0 0 0 ]
[Embedding Output] Token 70 Last 3: [0 0 0 ]
[Embedding Output] Token 71 First 3: [0 0 0 ]
[Embedding Output] Token 71 Last 3: [0 0 0 ]
[Embedding Output] Token 72 First 3: [0 0 0 ]
[Embedding Output] Token 72 Last 3: [0 0 0 ]
[Embedding Output] Token 73 First 3: [0 0 0 ]
[Embedding Output] Token 73 Last 3: [0 0 0 ]
[Embedding Output] Token 74 First 3: [0 0 0 ]
[Embedding Output] Token 74 Last 3: [0 0 0 ]
[Embedding Output] Token 75 First 3: [0 0 0 ]
[Embedding Output] Token 75 Last 3: [0 0 0 ]
[Embedding Output] Token 76 First 3: [0 0 0 ]
[Embedding Output] Token 76 Last 3: [0 0 0 ]
[Embedding Output] Token 77 First 3: [0 0 0 ]
[Embedding Output] Token 77 Last 3: [0 0 0 ]
[Embedding Output] Token 78 First 3: [0 0 0 ]
[Embedding Output] Token 78 Last 3: [0 0 0 ]
[Embedding Output] Token 79 First 3: [0 0 0 ]
[Embedding Output] Token 79 Last 3: [0 0 0 ]
[Embedding Output] Token 80 First 3: [0 0 0 ]
[Embedding Output] Token 80 Last 3: [0 0 0 ]
[Embedding Output] Token 81 First 3: [0 0 0 ]
[Embedding Output] Token 81 Last 3: [0 0 0 ]
[Embedding Output] Token 82 First 3: [0 0 0 ]
[Embedding Output] Token 82 Last 3: [0 0 0 ]
[Embedding Output] Token 83 First 3: [0 0 0 ]
[Embedding Output] Token 83 Last 3: [0 0 0 ]
[Embedding Output] Token 84 First 3: [0 0 0 ]
[Embedding Output] Token 84 Last 3: [0 0 0 ]
[Embedding Output] Token 85 First 3: [0 0 0 ]
[Embedding Output] Token 85 Last 3: [0 0 0 ]
[Embedding Output] Token 86 First 3: [0 0 0 ]
[Embedding Output] Token 86 Last 3: [0 0 0 ]
[Embedding Output] Token 87 First 3: [0 0 0 ]
[Embedding Output] Token 87 Last 3: [0 0 0 ]
[Embedding Output] Token 88 First 3: [0 0 0 ]
[Embedding Output] Token 88 Last 3: [0 0 0 ]
[Embedding Output] Token 89 First 3: [0 0 0 ]
[Embedding Output] Token 89 Last 3: [0 0 0 ]
[Embedding Output] Token 90 First 3: [0 0 0 ]
[Embedding Output] Token 90 Last 3: [0 0 0 ]
[Embedding Output] Token 91 First 3: [0 0 0 ]
[Embedding Output] Token 91 Last 3: [0 0 0 ]
[Embedding Output] Token 92 First 3: [0 0 0 ]
[Embedding Output] Token 92 Last 3: [0 0 0 ]
[Embedding Output] Token 93 First 3: [0 0 0 ]
[Embedding Output] Token 93 Last 3: [0 0 0 ]
[Embedding Output] Token 94 First 3: [0 0 0 ]
[Embedding Output] Token 94 Last 3: [0 0 0 ]
[Embedding Output] Token 95 First 3: [0 0 0 ]
[Embedding Output] Token 95 Last 3: [0 0 0 ]
[Embedding Output] Token 96 First 3: [0 0 0 ]
[Embedding Output] Token 96 Last 3: [0 0 0 ]
[Embedding Output] Token 97 First 3: [0 0 0 ]
[Embedding Output] Token 97 Last 3: [0 0 0 ]
[Embedding Output] Token 98 First 3: [0 0 0 ]
[Embedding Output] Token 98 Last 3: [0 0 0 ]
[Embedding Output] Token 99 First 3: [0 0 0 ]
[Embedding Output] Token 99 Last 3: [0 0 0 ]
[Embedding Output] Token 100 First 3: [0 0 0 ]
[Embedding Output] Token 100 Last 3: [0 0 0 ]
[Embedding Output] Token 101 First 3: [0 0 0 ]
[Embedding Output] Token 101 Last 3: [0 0 0 ]
[Embedding Output] Token 102 First 3: [0 0 0 ]
[Embedding Output] Token 102 Last 3: [0 0 0 ]
[Embedding Output] Token 103 First 3: [0 0 0 ]
[Embedding Output] Token 103 Last 3: [0 0 0 ]
[Embedding Output] Token 104 First 3: [0 0 0 ]
[Embedding Output] Token 104 Last 3: [0 0 0 ]
[Embedding Output] Token 105 First 3: [0 0 0 ]
[Embedding Output] Token 105 Last 3: [0 0 0 ]
[Embedding Output] Token 106 First 3: [0 0 0 ]
[Embedding Output] Token 106 Last 3: [0 0 0 ]
[Embedding Output] Token 107 First 3: [0 0 0 ]
[Embedding Output] Token 107 Last 3: [0 0 0 ]
[Embedding Output] Token 108 First 3: [0 0 0 ]
[Embedding Output] Token 108 Last 3: [0 0 0 ]
[Embedding Output] Token 109 First 3: [0 0 0 ]
[Embedding Output] Token 109 Last 3: [0 0 0 ]
[Embedding Output] Token 110 First 3: [0 0 0 ]
[Embedding Output] Token 110 Last 3: [0 0 0 ]
[Embedding Output] Token 111 First 3: [0 0 0 ]
[Embedding Output] Token 111 Last 3: [0 0 0 ]
[Embedding Output] Token 112 First 3: [0 0 0 ]
[Embedding Output] Token 112 Last 3: [0 0 0 ]
[Embedding Output] Token 113 First 3: [0 0 0 ]
[Embedding Output] Token 113 Last 3: [0 0 0 ]
[Embedding Output] Token 114 First 3: [0 0 0 ]
[Embedding Output] Token 114 Last 3: [0 0 0 ]
[Embedding Output] Token 115 First 3: [0 0 0 ]
[Embedding Output] Token 115 Last 3: [0 0 0 ]
[Embedding Output] Token 116 First 3: [0 0 0 ]
[Embedding Output] Token 116 Last 3: [0 0 0 ]
[Embedding Output] Token 117 First 3: [0 0 0 ]
[Embedding Output] Token 117 Last 3: [0 0 0 ]
[Embedding Output] Token 118 First 3: [0 0 0 ]
[Embedding Output] Token 118 Last 3: [0 0 0 ]
[Embedding Output] Token 119 First 3: [0 0 0 ]
[Embedding Output] Token 119 Last 3: [0 0 0 ]
[Embedding Output] Token 120 First 3: [0 0 0 ]
[Embedding Output] Token 120 Last 3: [0 0 0 ]
[Embedding Output] Token 121 First 3: [0 0 0 ]
[Embedding Output] Token 121 Last 3: [0 0 0 ]
[Embedding Output] Token 122 First 3: [0 0 0 ]
[Embedding Output] Token 122 Last 3: [0 0 0 ]
[Embedding Output] Token 123 First 3: [0 0 0 ]
[Embedding Output] Token 123 Last 3: [0 0 0 ]
[Embedding Output] Token 124 First 3: [0 0 0 ]
[Embedding Output] Token 124 Last 3: [0 0 0 ]
[Embedding Output] Token 125 First 3: [0 0 0 ]
[Embedding Output] Token 125 Last 3: [0 0 0 ]
[Embedding Output] Token 126 First 3: [0 0 0 ]
[Embedding Output] Token 126 Last 3: [0 0 0 ]
[Embedding Output] Token 127 First 3: [0 0 0 ]
[Embedding Output] Token 127 Last 3: [0 0 0 ]
[Embedding Output] Token 128 First 3: [0 0 0 ]
[Embedding Output] Token 128 Last 3: [0 0 0 ]
[Embedding Output] Token 129 First 3: [0 0 0 ]
[Embedding Output] Token 129 Last 3: [0 0 0 ]
[Embedding Output] Token 130 First 3: [0 0 0 ]
[Embedding Output] Token 130 Last 3: [0 0 0 ]
[Embedding Output] Token 131 First 3: [0 0 0 ]
[Embedding Output] Token 131 Last 3: [0 0 0 ]
[Embedding Output] Token 132 First 3: [0 0 0 ]
[Embedding Output] Token 132 Last 3: [0 0 0 ]
[Embedding Output] Token 133 First 3: [0 0 0 ]
[Embedding Output] Token 133 Last 3: [0 0 0 ]
[Embedding Output] Token 134 First 3: [0 0 0 ]
[Embedding Output] Token 134 Last 3: [0 0 0 ]
[Embedding Output] Token 135 First 3: [0 0 0 ]
[Embedding Output] Token 135 Last 3: [0 0 0 ]
[Embedding Output] Token 136 First 3: [0 0 0 ]
[Embedding Output] Token 136 Last 3: [0 0 0 ]
[Embedding Output] Token 137 First 3: [0 0 0 ]
[Embedding Output] Token 137 Last 3: [0 0 0 ]
[Embedding Output] Token 138 First 3: [0 0 0 ]
[Embedding Output] Token 138 Last 3: [0 0 0 ]
[Embedding Output] Token 139 First 3: [0 0 0 ]
[Embedding Output] Token 139 Last 3: [0 0 0 ]
[Embedding Output] Token 140 First 3: [0 0 0 ]
[Embedding Output] Token 140 Last 3: [0 0 0 ]
[Embedding Output] Token 141 First 3: [0 0 0 ]
[Embedding Output] Token 141 Last 3: [0 0 0 ]
[Embedding Output] Token 142 First 3: [0 0 0 ]
[Embedding Output] Token 142 Last 3: [0 0 0 ]
[Embedding Output] Token 143 First 3: [0 0 0 ]
[Embedding Output] Token 143 Last 3: [0 0 0 ]
[Embedding Output] Token 144 First 3: [0 0 0 ]
[Embedding Output] Token 144 Last 3: [0 0 0 ]
[Embedding Output] Token 145 First 3: [0 0 0 ]
[Embedding Output] Token 145 Last 3: [0 0 0 ]
[Embedding Output] Token 146 First 3: [0 0 0 ]
[Embedding Output] Token 146 Last 3: [0 0 0 ]
[Embedding Output] Token 147 First 3: [0 0 0 ]
[Embedding Output] Token 147 Last 3: [0 0 0 ]
[Embedding Output] Token 148 First 3: [0 0 0 ]
[Embedding Output] Token 148 Last 3: [0 0 0 ]
[Embedding Output] Token 149 First 3: [0 0 0 ]
[Embedding Output] Token 149 Last 3: [0 0 0 ]
[Embedding Output] Token 150 First 3: [0 0 0 ]
[Embedding Output] Token 150 Last 3: [0 0 0 ]
[Embedding Output] Token 151 First 3: [0 0 0 ]
[Embedding Output] Token 151 Last 3: [0 0 0 ]
[Embedding Output] Token 152 First 3: [0 0 0 ]
[Embedding Output] Token 152 Last 3: [0 0 0 ]
[Embedding Output] Token 153 First 3: [0 0 0 ]
[Embedding Output] Token 153 Last 3: [0 0 0 ]
[Embedding Output] Token 154 First 3: [0 0 0 ]
[Embedding Output] Token 154 Last 3: [0 0 0 ]
[Embedding Output] Token 155 First 3: [0 0 0 ]
[Embedding Output] Token 155 Last 3: [0 0 0 ]
[Embedding Output] Token 156 First 3: [0 0 0 ]
[Embedding Output] Token 156 Last 3: [0 0 0 ]
[Embedding Output] Token 157 First 3: [0 0 0 ]
[Embedding Output] Token 157 Last 3: [0 0 0 ]
[Embedding Output] Token 158 First 3: [0 0 0 ]
[Embedding Output] Token 158 Last 3: [0 0 0 ]
[Embedding Output] Token 159 First 3: [0 0 0 ]
[Embedding Output] Token 159 Last 3: [0 0 0 ]
[Embedding Output] Token 160 First 3: [0 0 0 ]
[Embedding Output] Token 160 Last 3: [0 0 0 ]
[Embedding Output] Token 161 First 3: [0 0 0 ]
[Embedding Output] Token 161 Last 3: [0 0 0 ]
[Embedding Output] Token 162 First 3: [0 0 0 ]
[Embedding Output] Token 162 Last 3: [0 0 0 ]
[Embedding Output] Token 163 First 3: [0 0 0 ]
[Embedding Output] Token 163 Last 3: [0 0 0 ]
[Embedding Output] Token 164 First 3: [0 0 0 ]
[Embedding Output] Token 164 Last 3: [0 0 0 ]
[Embedding Output] Token 165 First 3: [0 0 0 ]
[Embedding Output] Token 165 Last 3: [0 0 0 ]
[Embedding Output] Token 166 First 3: [0 0 0 ]
[Embedding Output] Token 166 Last 3: [0 0 0 ]
[Embedding Output] Token 167 First 3: [0 0 0 ]
[Embedding Output] Token 167 Last 3: [0 0 0 ]
[Embedding Output] Token 168 First 3: [0 0 0 ]
[Embedding Output] Token 168 Last 3: [0 0 0 ]
[Embedding Output] Token 169 First 3: [0 0 0 ]
[Embedding Output] Token 169 Last 3: [0 0 0 ]
[Embedding Output] Token 170 First 3: [0 0 0 ]
[Embedding Output] Token 170 Last 3: [0 0 0 ]
[Embedding Output] Token 171 First 3: [0 0 0 ]
[Embedding Output] Token 171 Last 3: [0 0 0 ]
[Embedding Output] Token 172 First 3: [0 0 0 ]
[Embedding Output] Token 172 Last 3: [0 0 0 ]
[Embedding Output] Token 173 First 3: [0 0 0 ]
[Embedding Output] Token 173 Last 3: [0 0 0 ]
[Embedding Output] Token 174 First 3: [0 0 0 ]
[Embedding Output] Token 174 Last 3: [0 0 0 ]
[Embedding Output] Token 175 First 3: [0 0 0 ]
[Embedding Output] Token 175 Last 3: [0 0 0 ]
[Embedding Output] Token 176 First 3: [0 0 0 ]
[Embedding Output] Token 176 Last 3: [0 0 0 ]
[Embedding Output] Token 177 First 3: [0 0 0 ]
[Embedding Output] Token 177 Last 3: [0 0 0 ]
[Embedding Output] Token 178 First 3: [0 0 0 ]
[Embedding Output] Token 178 Last 3: [0 0 0 ]
[Embedding Output] Token 179 First 3: [0 0 0 ]
[Embedding Output] Token 179 Last 3: [0 0 0 ]
[Embedding Output] Token 180 First 3: [0 0 0 ]
[Embedding Output] Token 180 Last 3: [0 0 0 ]
[Embedding Output] Token 181 First 3: [0 0 0 ]
[Embedding Output] Token 181 Last 3: [0 0 0 ]
[Embedding Output] Token 182 First 3: [0 0 0 ]
[Embedding Output] Token 182 Last 3: [0 0 0 ]
[Embedding Output] Token 183 First 3: [0 0 0 ]
[Embedding Output] Token 183 Last 3: [0 0 0 ]
[Embedding Output] Token 184 First 3: [0 0 0 ]
[Embedding Output] Token 184 Last 3: [0 0 0 ]
[Embedding Output] Token 185 First 3: [0 0 0 ]
[Embedding Output] Token 185 Last 3: [0 0 0 ]
[Embedding Output] Token 186 First 3: [0 0 0 ]
[Embedding Output] Token 186 Last 3: [0 0 0 ]
[Embedding Output] Token 187 First 3: [0 0 0 ]
[Embedding Output] Token 187 Last 3: [0 0 0 ]
[Embedding Output] Token 188 First 3: [0 0 0 ]
[Embedding Output] Token 188 Last 3: [0 0 0 ]
[Embedding Output] Token 189 First 3: [0 0 0 ]
[Embedding Output] Token 189 Last 3: [0 0 0 ]
[Embedding Output] Token 190 First 3: [0 0 0 ]
[Embedding Output] Token 190 Last 3: [0 0 0 ]
[Embedding Output] Token 191 First 3: [0 0 0 ]
[Embedding Output] Token 191 Last 3: [0 0 0 ]
[Embedding Output] Token 192 First 3: [0 0 0 ]
[Embedding Output] Token 192 Last 3: [0 0 0 ]
[Embedding Output] Token 193 First 3: [0 0 0 ]
[Embedding Output] Token 193 Last 3: [0 0 0 ]
[Embedding Output] Token 194 First 3: [0 0 0 ]
[Embedding Output] Token 194 Last 3: [0 0 0 ]
[Embedding Output] Token 195 First 3: [0 0 0 ]
[Embedding Output] Token 195 Last 3: [0 0 0 ]
[Embedding Output] Token 196 First 3: [0 0 0 ]
[Embedding Output] Token 196 Last 3: [0 0 0 ]
[Embedding Output] Token 197 First 3: [0 0 0 ]
[Embedding Output] Token 197 Last 3: [0 0 0 ]
[Embedding Output] Token 198 First 3: [0 0 0 ]
[Embedding Output] Token 198 Last 3: [0 0 0 ]
[Embedding Output] Token 199 First 3: [0 0 0 ]
[Embedding Output] Token 199 Last 3: [0 0 0 ]
[Embedding Output] Token 200 First 3: [0 0 0 ]
[Embedding Output] Token 200 Last 3: [0 0 0 ]
[Embedding Output] Token 201 First 3: [0 0 0 ]
[Embedding Output] Token 201 Last 3: [0 0 0 ]
[Embedding Output] Token 202 First 3: [0 0 0 ]
[Embedding Output] Token 202 Last 3: [0 0 0 ]
[Embedding Output] Token 203 First 3: [0 0 0 ]
[Embedding Output] Token 203 Last 3: [0 0 0 ]
[Embedding Output] Token 204 First 3: [0 0 0 ]
[Embedding Output] Token 204 Last 3: [0 0 0 ]
[Embedding Output] Token 205 First 3: [0 0 0 ]
[Embedding Output] Token 205 Last 3: [0 0 0 ]
[Embedding Output] Token 206 First 3: [0 0 0 ]
[Embedding Output] Token 206 Last 3: [0 0 0 ]
[Embedding Output] Token 207 First 3: [0 0 0 ]
[Embedding Output] Token 207 Last 3: [0 0 0 ]
[Embedding Output] Token 208 First 3: [0 0 0 ]
[Embedding Output] Token 208 Last 3: [0 0 0 ]
[Embedding Output] Token 209 First 3: [0 0 0 ]
[Embedding Output] Token 209 Last 3: [0 0 0 ]
[Embedding Output] Token 210 First 3: [0 0 0 ]
[Embedding Output] Token 210 Last 3: [0 0 0 ]
[Embedding Output] Token 211 First 3: [0 0 0 ]
[Embedding Output] Token 211 Last 3: [0 0 0 ]
[Embedding Output] Token 212 First 3: [0 0 0 ]
[Embedding Output] Token 212 Last 3: [0 0 0 ]
[Embedding Output] Token 213 First 3: [0 0 0 ]
[Embedding Output] Token 213 Last 3: [0 0 0 ]
[Embedding Output] Token 214 First 3: [0 0 0 ]
[Embedding Output] Token 214 Last 3: [0 0 0 ]
[Embedding Output] Token 215 First 3: [0 0 0 ]
[Embedding Output] Token 215 Last 3: [0 0 0 ]
[Embedding Output] Token 216 First 3: [0 0 0 ]
[Embedding Output] Token 216 Last 3: [0 0 0 ]
[Embedding Output] Token 217 First 3: [0 0 0 ]
[Embedding Output] Token 217 Last 3: [0 0 0 ]
[Embedding Output] Token 218 First 3: [0 0 0 ]
[Embedding Output] Token 218 Last 3: [0 0 0 ]
[Embedding Output] Token 219 First 3: [0 0 0 ]
[Embedding Output] Token 219 Last 3: [0 0 0 ]
[Embedding Output] Token 220 First 3: [0 0 0 ]
[Embedding Output] Token 220 Last 3: [0 0 0 ]
[Embedding Output] Token 221 First 3: [0 0 0 ]
[Embedding Output] Token 221 Last 3: [0 0 0 ]
[Embedding Output] Token 222 First 3: [0 0 0 ]
[Embedding Output] Token 222 Last 3: [0 0 0 ]
[Embedding Output] Token 223 First 3: [0 0 0 ]
[Embedding Output] Token 223 Last 3: [0 0 0 ]
[Embedding Output] Token 224 First 3: [0 0 0 ]
[Embedding Output] Token 224 Last 3: [0 0 0 ]
[Embedding Output] Token 225 First 3: [0 0 0 ]
[Embedding Output] Token 225 Last 3: [0 0 0 ]
[Embedding Output] Token 226 First 3: [0 0 0 ]
[Embedding Output] Token 226 Last 3: [0 0 0 ]
[Embedding Output] Token 227 First 3: [0 0 0 ]
[Embedding Output] Token 227 Last 3: [0 0 0 ]
[Embedding Output] Token 228 First 3: [0 0 0 ]
[Embedding Output] Token 228 Last 3: [0 0 0 ]
[Embedding Output] Token 229 First 3: [0 0 0 ]
[Embedding Output] Token 229 Last 3: [0 0 0 ]
[Embedding Output] Token 230 First 3: [0 0 0 ]
[Embedding Output] Token 230 Last 3: [0 0 0 ]
[Embedding Output] Token 231 First 3: [0 0 0 ]
[Embedding Output] Token 231 Last 3: [0 0 0 ]
[Embedding Output] Token 232 First 3: [0 0 0 ]
[Embedding Output] Token 232 Last 3: [0 0 0 ]
[Embedding Output] Token 233 First 3: [0 0 0 ]
[Embedding Output] Token 233 Last 3: [0 0 0 ]
[Embedding Output] Token 234 First 3: [0 0 0 ]
[Embedding Output] Token 234 Last 3: [0 0 0 ]
[Embedding Output] Token 235 First 3: [0 0 0 ]
[Embedding Output] Token 235 Last 3: [0 0 0 ]
[Embedding Output] Token 236 First 3: [0 0 0 ]
[Embedding Output] Token 236 Last 3: [0 0 0 ]
[Embedding Output] Token 237 First 3: [0 0 0 ]
[Embedding Output] Token 237 Last 3: [0 0 0 ]
[Embedding Output] Token 238 First 3: [0 0 0 ]
[Embedding Output] Token 238 Last 3: [0 0 0 ]
[Embedding Output] Token 239 First 3: [0 0 0 ]
[Embedding Output] Token 239 Last 3: [0 0 0 ]
[Embedding Output] Token 240 First 3: [0 0 0 ]
[Embedding Output] Token 240 Last 3: [0 0 0 ]
[Embedding Output] Token 241 First 3: [0 0 0 ]
[Embedding Output] Token 241 Last 3: [0 0 0 ]
[Embedding Output] Token 242 First 3: [0 0 0 ]
[Embedding Output] Token 242 Last 3: [0 0 0 ]
[Embedding Output] Token 243 First 3: [0 0 0 ]
[Embedding Output] Token 243 Last 3: [0 0 0 ]
[Embedding Output] Token 244 First 3: [0 0 0 ]
[Embedding Output] Token 244 Last 3: [0 0 0 ]
[Embedding Output] Token 245 First 3: [0 0 0 ]
[Embedding Output] Token 245 Last 3: [0 0 0 ]
[Embedding Output] Token 246 First 3: [0 0 0 ]
[Embedding Output] Token 246 Last 3: [0 0 0 ]
[Embedding Output] Token 247 First 3: [0 0 0 ]
[Embedding Output] Token 247 Last 3: [0 0 0 ]
[Embedding Output] Token 248 First 3: [0 0 0 ]
[Embedding Output] Token 248 Last 3: [0 0 0 ]
[Embedding Output] Token 249 First 3: [0 0 0 ]
[Embedding Output] Token 249 Last 3: [0 0 0 ]
[Embedding Output] Token 250 First 3: [0 0 0 ]
[Embedding Output] Token 250 Last 3: [0 0 0 ]
[Embedding Output] Token 251 First 3: [0 0 0 ]
[Embedding Output] Token 251 Last 3: [0 0 0 ]
[Embedding Output] Token 252 First 3: [0 0 0 ]
[Embedding Output] Token 252 Last 3: [0 0 0 ]
[Embedding Output] Token 253 First 3: [0 0 0 ]
[Embedding Output] Token 253 Last 3: [0 0 0 ]
[Embedding Output] Token 254 First 3: [0 0 0 ]
[Embedding Output] Token 254 Last 3: [0 0 0 ]
[Embedding Output] Token 255 First 3: [0 0 0 ]
[Embedding Output] Token 255 Last 3: [0 0 0 ]
[Embedding Output] Token 256 First 3: [0 0 0 ]
[Embedding Output] Token 256 Last 3: [0 0 0 ]
[Embedding Output] Token 257 First 3: [0 0 0 ]
[Embedding Output] Token 257 Last 3: [0 0 0 ]
[Embedding Output] Token 258 First 3: [0 0 0 ]
[Embedding Output] Token 258 Last 3: [0 0 0 ]
[Embedding Output] Token 259 First 3: [0 0 0 ]
[Embedding Output] Token 259 Last 3: [0 0 0 ]
[Embedding Output] Token 260 First 3: [0 0 0 ]
[Embedding Output] Token 260 Last 3: [0 0 0 ]
[Embedding Output] Token 261 First 3: [0 0 0 ]
[Embedding Output] Token 261 Last 3: [0 0 0 ]
[Embedding Output] Token 262 First 3: [0 0 0 ]
[Embedding Output] Token 262 Last 3: [0 0 0 ]
[Embedding Output] Token 263 First 3: [0 0 0 ]
[Embedding Output] Token 263 Last 3: [0 0 0 ]
[Embedding Output] Token 264 First 3: [0 0 0 ]
[Embedding Output] Token 264 Last 3: [0 0 0 ]
[Embedding Output] Token 265 First 3: [0 0 0 ]
[Embedding Output] Token 265 Last 3: [0 0 0 ]
[Embedding Output] Token 266 First 3: [0 0 0 ]
[Embedding Output] Token 266 Last 3: [0 0 0 ]
[Embedding Output] Token 267 First 3: [0 0 0 ]
[Embedding Output] Token 267 Last 3: [0 0 0 ]
[Embedding Output] Token 268 First 3: [0 0 0 ]
[Embedding Output] Token 268 Last 3: [0 0 0 ]
[Embedding Output] Token 269 First 3: [0 0 0 ]
[Embedding Output] Token 269 Last 3: [0 0 0 ]
[Embedding Output] Token 270 First 3: [0 0 0 ]
[Embedding Output] Token 270 Last 3: [0 0 0 ]
[Embedding Output] Token 271 First 3: [0 0 0 ]
[Embedding Output] Token 271 Last 3: [0 0 0 ]
[Embedding Output] Token 272 First 3: [0 0 0 ]
[Embedding Output] Token 272 Last 3: [0 0 0 ]
[Embedding Output] Token 273 First 3: [0 0 0 ]
[Embedding Output] Token 273 Last 3: [0 0 0 ]
[Embedding Output] Token 274 First 3: [0 0 0 ]
[Embedding Output] Token 274 Last 3: [0 0 0 ]
[Embedding Output] Token 275 First 3: [0 0 0 ]
[Embedding Output] Token 275 Last 3: [0 0 0 ]
[Embedding Output] Token 276 First 3: [0 0 0 ]
[Embedding Output] Token 276 Last 3: [0 0 0 ]
[Embedding Output] Token 277 First 3: [0 0 0 ]
[Embedding Output] Token 277 Last 3: [0 0 0 ]
[Embedding Output] Token 278 First 3: [0 0 0 ]
[Embedding Output] Token 278 Last 3: [0 0 0 ]
[Embedding Output] Token 279 First 3: [0 0 0 ]
[Embedding Output] Token 279 Last 3: [0 0 0 ]
[Embedding Output] Token 280 First 3: [0 0 0 ]
[Embedding Output] Token 280 Last 3: [0 0 0 ]
[Embedding Output] Token 281 First 3: [0 0 0 ]
[Embedding Output] Token 281 Last 3: [0 0 0 ]
[Embedding Output] Token 282 First 3: [0 0 0 ]
[Embedding Output] Token 282 Last 3: [0 0 0 ]
[Embedding Output] Token 283 First 3: [0 0 0 ]
[Embedding Output] Token 283 Last 3: [0 0 0 ]
[Embedding Output] Token 284 First 3: [0 0 0 ]
[Embedding Output] Token 284 Last 3: [0 0 0 ]
[Embedding Output] Token 285 First 3: [0 0 0 ]
[Embedding Output] Token 285 Last 3: [0 0 0 ]
[Embedding Output] Token 286 First 3: [0 0 0 ]
[Embedding Output] Token 286 Last 3: [0 0 0 ]
[Embedding Output] Token 287 First 3: [0 0 0 ]
[Embedding Output] Token 287 Last 3: [0 0 0 ]
[Embedding Output] Token 288 First 3: [0 0 0 ]
[Embedding Output] Token 288 Last 3: [0 0 0 ]
[Embedding Output] Token 289 First 3: [0 0 0 ]
[Embedding Output] Token 289 Last 3: [0 0 0 ]
[Embedding Output] Token 290 First 3: [0 0 0 ]
[Embedding Output] Token 290 Last 3: [0 0 0 ]
[Embedding Output] Token 291 First 3: [0 0 0 ]
[Embedding Output] Token 291 Last 3: [0 0 0 ]
[Embedding Output] Token 292 First 3: [0 0 0 ]
[Embedding Output] Token 292 Last 3: [0 0 0 ]
[Embedding Output] Token 293 First 3: [0 0 0 ]
[Embedding Output] Token 293 Last 3: [0 0 0 ]
[Embedding Output] Token 294 First 3: [0 0 0 ]
[Embedding Output] Token 294 Last 3: [0 0 0 ]
[Embedding Output] Token 295 First 3: [0 0 0 ]
[Embedding Output] Token 295 Last 3: [0 0 0 ]
[Embedding Output] Token 296 First 3: [0 0 0 ]
[Embedding Output] Token 296 Last 3: [0 0 0 ]
[Embedding Output] Token 297 First 3: [0 0 0 ]
[Embedding Output] Token 297 Last 3: [0 0 0 ]
[Embedding Output] Token 298 First 3: [0 0 0 ]
[Embedding Output] Token 298 Last 3: [0 0 0 ]
[Embedding Output] Token 299 First 3: [0 0 0 ]
[Embedding Output] Token 299 Last 3: [0 0 0 ]
[Embedding Output] Token 300 First 3: [0 0 0 ]
[Embedding Output] Token 300 Last 3: [0 0 0 ]
[Embedding Output] Token 301 First 3: [0 0 0 ]
[Embedding Output] Token 301 Last 3: [0 0 0 ]
[Embedding Output] Token 302 First 3: [0 0 0 ]
[Embedding Output] Token 302 Last 3: [0 0 0 ]
[Embedding Output] Token 303 First 3: [0 0 0 ]
[Embedding Output] Token 303 Last 3: [0 0 0 ]
[Embedding Output] Token 304 First 3: [0 0 0 ]
[Embedding Output] Token 304 Last 3: [0 0 0 ]
[Embedding Output] Token 305 First 3: [0 0 0 ]
[Embedding Output] Token 305 Last 3: [0 0 0 ]
[Embedding Output] Token 306 First 3: [0 0 0 ]
[Embedding Output] Token 306 Last 3: [0 0 0 ]
[Embedding Output] Token 307 First 3: [0 0 0 ]
[Embedding Output] Token 307 Last 3: [0 0 0 ]
[Embedding Output] Token 308 First 3: [0 0 0 ]
[Embedding Output] Token 308 Last 3: [0 0 0 ]
[Embedding Output] Token 309 First 3: [0 0 0 ]
[Embedding Output] Token 309 Last 3: [0 0 0 ]
[Embedding Output] Token 310 First 3: [0 0 0 ]
[Embedding Output] Token 310 Last 3: [0 0 0 ]
[Embedding Output] Token 311 First 3: [0 0 0 ]
[Embedding Output] Token 311 Last 3: [0 0 0 ]
[Embedding Output] Token 312 First 3: [0 0 0 ]
[Embedding Output] Token 312 Last 3: [0 0 0 ]
[Embedding Output] Token 313 First 3: [0 0 0 ]
[Embedding Output] Token 313 Last 3: [0 0 0 ]
[Embedding Output] Token 314 First 3: [0 0 0 ]
[Embedding Output] Token 314 Last 3: [0 0 0 ]
[Embedding Output] Token 315 First 3: [0 0 0 ]
[Embedding Output] Token 315 Last 3: [0 0 0 ]
[Embedding Output] Token 316 First 3: [0 0 0 ]
[Embedding Output] Token 316 Last 3: [0 0 0 ]
[Embedding Output] Token 317 First 3: [0 0 0 ]
[Embedding Output] Token 317 Last 3: [0 0 0 ]
[Embedding Output] Token 318 First 3: [0 0 0 ]
[Embedding Output] Token 318 Last 3: [0 0 0 ]
[Embedding Output] Token 319 First 3: [0 0 0 ]
[Embedding Output] Token 319 Last 3: [0 0 0 ]
[Embedding Output] Token 320 First 3: [0 0 0 ]
[Embedding Output] Token 320 Last 3: [0 0 0 ]
[Embedding Output] Token 321 First 3: [0 0 0 ]
[Embedding Output] Token 321 Last 3: [0 0 0 ]
[Embedding Output] Token 322 First 3: [0 0 0 ]
[Embedding Output] Token 322 Last 3: [0 0 0 ]
[Embedding Output] Token 323 First 3: [0 0 0 ]
[Embedding Output] Token 323 Last 3: [0 0 0 ]
[Embedding Output] Token 324 First 3: [0 0 0 ]
[Embedding Output] Token 324 Last 3: [0 0 0 ]
[Embedding Output] Token 325 First 3: [0 0 0 ]
[Embedding Output] Token 325 Last 3: [0 0 0 ]
[Embedding Output] Token 326 First 3: [0 0 0 ]
[Embedding Output] Token 326 Last 3: [0 0 0 ]
[Embedding Output] Token 327 First 3: [0 0 0 ]
[Embedding Output] Token 327 Last 3: [0 0 0 ]
[Embedding Output] Token 328 First 3: [0 0 0 ]
[Embedding Output] Token 328 Last 3: [0 0 0 ]
[Embedding Output] Token 329 First 3: [0 0 0 ]
[Embedding Output] Token 329 Last 3: [0 0 0 ]
[Embedding Output] Token 330 First 3: [0 0 0 ]
[Embedding Output] Token 330 Last 3: [0 0 0 ]
[Embedding Output] Token 331 First 3: [0 0 0 ]
[Embedding Output] Token 331 Last 3: [0 0 0 ]
[Embedding Output] Token 332 First 3: [0 0 0 ]
[Embedding Output] Token 332 Last 3: [0 0 0 ]
[Embedding Output] Token 333 First 3: [0 0 0 ]
[Embedding Output] Token 333 Last 3: [0 0 0 ]
[Embedding Output] Token 334 First 3: [0 0 0 ]
[Embedding Output] Token 334 Last 3: [0 0 0 ]
[Embedding Output] Token 335 First 3: [0 0 0 ]
[Embedding Output] Token 335 Last 3: [0 0 0 ]
[Embedding Output] Token 336 First 3: [0 0 0 ]
[Embedding Output] Token 336 Last 3: [0 0 0 ]
[Embedding Output] Token 337 First 3: [0 0 0 ]
[Embedding Output] Token 337 Last 3: [0 0 0 ]
[Embedding Output] Token 338 First 3: [0 0 0 ]
[Embedding Output] Token 338 Last 3: [0 0 0 ]
[Embedding Output] Token 339 First 3: [0 0 0 ]
[Embedding Output] Token 339 Last 3: [0 0 0 ]
[Embedding Output] Token 340 First 3: [0 0 0 ]
[Embedding Output] Token 340 Last 3: [0 0 0 ]
[Embedding Output] Token 341 First 3: [0 0 0 ]
[Embedding Output] Token 341 Last 3: [0 0 0 ]
[Embedding Output] Token 342 First 3: [0 0 0 ]
[Embedding Output] Token 342 Last 3: [0 0 0 ]
[Embedding Output] Token 343 First 3: [0 0 0 ]
[Embedding Output] Token 343 Last 3: [0 0 0 ]
[Embedding Output] Token 344 First 3: [0 0 0 ]
[Embedding Output] Token 344 Last 3: [0 0 0 ]
[Embedding Output] Token 345 First 3: [0 0 0 ]
[Embedding Output] Token 345 Last 3: [0 0 0 ]
[Embedding Output] Token 346 First 3: [0 0 0 ]
[Embedding Output] Token 346 Last 3: [0 0 0 ]
[Embedding Output] Token 347 First 3: [0 0 0 ]
[Embedding Output] Token 347 Last 3: [0 0 0 ]
[Embedding Output] Token 348 First 3: [0 0 0 ]
[Embedding Output] Token 348 Last 3: [0 0 0 ]
[Embedding Output] Token 349 First 3: [0 0 0 ]
[Embedding Output] Token 349 Last 3: [0 0 0 ]
[Embedding Output] Token 350 First 3: [0 0 0 ]
[Embedding Output] Token 350 Last 3: [0 0 0 ]
[Embedding Output] Token 351 First 3: [0 0 0 ]
[Embedding Output] Token 351 Last 3: [0 0 0 ]
[Embedding Output] Token 352 First 3: [0 0 0 ]
[Embedding Output] Token 352 Last 3: [0 0 0 ]
[Embedding Output] Token 353 First 3: [0 0 0 ]
[Embedding Output] Token 353 Last 3: [0 0 0 ]
[Embedding Output] Token 354 First 3: [0 0 0 ]
[Embedding Output] Token 354 Last 3: [0 0 0 ]
[Embedding Output] Token 355 First 3: [0 0 0 ]
[Embedding Output] Token 355 Last 3: [0 0 0 ]
[Embedding Output] Token 356 First 3: [0 0 0 ]
[Embedding Output] Token 356 Last 3: [0 0 0 ]
[Embedding Output] Token 357 First 3: [0 0 0 ]
[Embedding Output] Token 357 Last 3: [0 0 0 ]
[Embedding Output] Token 358 First 3: [0 0 0 ]
[Embedding Output] Token 358 Last 3: [0 0 0 ]
[Embedding Output] Token 359 First 3: [0 0 0 ]
[Embedding Output] Token 359 Last 3: [0 0 0 ]
[Embedding Output] Token 360 First 3: [0 0 0 ]
[Embedding Output] Token 360 Last 3: [0 0 0 ]
[Embedding Output] Token 361 First 3: [0 0 0 ]
[Embedding Output] Token 361 Last 3: [0 0 0 ]
[Embedding Output] Token 362 First 3: [0 0 0 ]
[Embedding Output] Token 362 Last 3: [0 0 0 ]
[Embedding Output] Token 363 First 3: [0 0 0 ]
[Embedding Output] Token 363 Last 3: [0 0 0 ]
[Embedding Output] Token 364 First 3: [0 0 0 ]
[Embedding Output] Token 364 Last 3: [0 0 0 ]
[Embedding Output] Token 365 First 3: [0 0 0 ]
[Embedding Output] Token 365 Last 3: [0 0 0 ]
[Embedding Output] Token 366 First 3: [0 0 0 ]
[Embedding Output] Token 366 Last 3: [0 0 0 ]
[Embedding Output] Token 367 First 3: [0 0 0 ]
[Embedding Output] Token 367 Last 3: [0 0 0 ]
[Embedding Output] Token 368 First 3: [0 0 0 ]
[Embedding Output] Token 368 Last 3: [0 0 0 ]
[Embedding Output] Token 369 First 3: [0 0 0 ]
[Embedding Output] Token 369 Last 3: [0 0 0 ]
[Embedding Output] Token 370 First 3: [0 0 0 ]
[Embedding Output] Token 370 Last 3: [0 0 0 ]
[Embedding Output] Token 371 First 3: [0 0 0 ]
[Embedding Output] Token 371 Last 3: [0 0 0 ]
[Embedding Output] Token 372 First 3: [0 0 0 ]
[Embedding Output] Token 372 Last 3: [0 0 0 ]
[Embedding Output] Token 373 First 3: [0 0 0 ]
[Embedding Output] Token 373 Last 3: [0 0 0 ]
[Embedding Output] Token 374 First 3: [0 0 0 ]
[Embedding Output] Token 374 Last 3: [0 0 0 ]
[Embedding Output] Token 375 First 3: [0 0 0 ]
[Embedding Output] Token 375 Last 3: [0 0 0 ]
[Embedding Output] Token 376 First 3: [0 0 0 ]
[Embedding Output] Token 376 Last 3: [0 0 0 ]
[Embedding Output] Token 377 First 3: [0 0 0 ]
[Embedding Output] Token 377 Last 3: [0 0 0 ]
[Embedding Output] Token 378 First 3: [0 0 0 ]
[Embedding Output] Token 378 Last 3: [0 0 0 ]
[Embedding Output] Token 379 First 3: [0 0 0 ]
[Embedding Output] Token 379 Last 3: [0 0 0 ]
[Embedding Output] Token 380 First 3: [0 0 0 ]
[Embedding Output] Token 380 Last 3: [0 0 0 ]
[Embedding Output] Token 381 First 3: [0 0 0 ]
[Embedding Output] Token 381 Last 3: [0 0 0 ]
[Embedding Output] Token 382 First 3: [0 0 0 ]
[Embedding Output] Token 382 Last 3: [0 0 0 ]
[Embedding Output] Token 383 First 3: [0 0 0 ]
[Embedding Output] Token 383 Last 3: [0 0 0 ]
[Embedding Output] Token 384 First 3: [0 0 0 ]
[Embedding Output] Token 384 Last 3: [0 0 0 ]
[Embedding Output] Token 385 First 3: [0 0 0 ]
[Embedding Output] Token 385 Last 3: [0 0 0 ]
[Embedding Output] Token 386 First 3: [0 0 0 ]
[Embedding Output] Token 386 Last 3: [0 0 0 ]
[Embedding Output] Token 387 First 3: [0 0 0 ]
[Embedding Output] Token 387 Last 3: [0 0 0 ]
[Embedding Output] Token 388 First 3: [0 0 0 ]
[Embedding Output] Token 388 Last 3: [0 0 0 ]
[Embedding Output] Token 389 First 3: [0 0 0 ]
[Embedding Output] Token 389 Last 3: [0 0 0 ]
[Embedding Output] Token 390 First 3: [0 0 0 ]
[Embedding Output] Token 390 Last 3: [0 0 0 ]
[Embedding Output] Token 391 First 3: [0 0 0 ]
[Embedding Output] Token 391 Last 3: [0 0 0 ]
[Embedding Output] Token 392 First 3: [0 0 0 ]
[Embedding Output] Token 392 Last 3: [0 0 0 ]
[Embedding Output] Token 393 First 3: [0 0 0 ]
[Embedding Output] Token 393 Last 3: [0 0 0 ]
[Embedding Output] Token 394 First 3: [0 0 0 ]
[Embedding Output] Token 394 Last 3: [0 0 0 ]
[Embedding Output] Token 395 First 3: [0 0 0 ]
[Embedding Output] Token 395 Last 3: [0 0 0 ]
[Embedding Output] Token 396 First 3: [0 0 0 ]
[Embedding Output] Token 396 Last 3: [0 0 0 ]
[Embedding Output] Token 397 First 3: [0 0 0 ]
[Embedding Output] Token 397 Last 3: [0 0 0 ]
[Embedding Output] Token 398 First 3: [0 0 0 ]
[Embedding Output] Token 398 Last 3: [0 0 0 ]
[Embedding Output] Token 399 First 3: [0 0 0 ]
[Embedding Output] Token 399 Last 3: [0 0 0 ]
[Embedding Output] Token 400 First 3: [0 0 0 ]
[Embedding Output] Token 400 Last 3: [0 0 0 ]
[Embedding Output] Token 401 First 3: [0 0 0 ]
[Embedding Output] Token 401 Last 3: [0 0 0 ]
[Embedding Output] Token 402 First 3: [0 0 0 ]
[Embedding Output] Token 402 Last 3: [0 0 0 ]
[Embedding Output] Token 403 First 3: [0 0 0 ]
[Embedding Output] Token 403 Last 3: [0 0 0 ]
[Embedding Output] Token 404 First 3: [0 0 0 ]
[Embedding Output] Token 404 Last 3: [0 0 0 ]
[Embedding Output] Token 405 First 3: [0 0 0 ]
[Embedding Output] Token 405 Last 3: [0 0 0 ]
[Embedding Output] Token 406 First 3: [0 0 0 ]
[Embedding Output] Token 406 Last 3: [0 0 0 ]
[Embedding Output] Token 407 First 3: [0 0 0 ]
[Embedding Output] Token 407 Last 3: [0 0 0 ]
[Embedding Output] Token 408 First 3: [0 0 0 ]
[Embedding Output] Token 408 Last 3: [0 0 0 ]
[Embedding Output] Token 409 First 3: [0 0 0 ]
[Embedding Output] Token 409 Last 3: [0 0 0 ]
[Embedding Output] Token 410 First 3: [0 0 0 ]
[Embedding Output] Token 410 Last 3: [0 0 0 ]
[Embedding Output] Token 411 First 3: [0 0 0 ]
[Embedding Output] Token 411 Last 3: [0 0 0 ]
[Embedding Output] Token 412 First 3: [0 0 0 ]
[Embedding Output] Token 412 Last 3: [0 0 0 ]
[Embedding Output] Token 413 First 3: [0 0 0 ]
[Embedding Output] Token 413 Last 3: [0 0 0 ]
[Embedding Output] Token 414 First 3: [0 0 0 ]
[Embedding Output] Token 414 Last 3: [0 0 0 ]
[Embedding Output] Token 415 First 3: [0 0 0 ]
[Embedding Output] Token 415 Last 3: [0 0 0 ]
[Embedding Output] Token 416 First 3: [0 0 0 ]
[Embedding Output] Token 416 Last 3: [0 0 0 ]
[Embedding Output] Token 417 First 3: [0 0 0 ]
[Embedding Output] Token 417 Last 3: [0 0 0 ]
[Embedding Output] Token 418 First 3: [0 0 0 ]
[Embedding Output] Token 418 Last 3: [0 0 0 ]
[Embedding Output] Token 419 First 3: [0 0 0 ]
[Embedding Output] Token 419 Last 3: [0 0 0 ]
[Embedding Output] Token 420 First 3: [0 0 0 ]
[Embedding Output] Token 420 Last 3: [0 0 0 ]
[Embedding Output] Token 421 First 3: [0 0 0 ]
[Embedding Output] Token 421 Last 3: [0 0 0 ]
[Embedding Output] Token 422 First 3: [0 0 0 ]
[Embedding Output] Token 422 Last 3: [0 0 0 ]
[Embedding Output] Token 423 First 3: [0 0 0 ]
[Embedding Output] Token 423 Last 3: [0 0 0 ]
[Embedding Output] Token 424 First 3: [0 0 0 ]
[Embedding Output] Token 424 Last 3: [0 0 0 ]
[Embedding Output] Token 425 First 3: [0 0 0 ]
[Embedding Output] Token 425 Last 3: [0 0 0 ]
[Embedding Output] Token 426 First 3: [0 0 0 ]
[Embedding Output] Token 426 Last 3: [0 0 0 ]
[Embedding Output] Token 427 First 3: [0 0 0 ]
[Embedding Output] Token 427 Last 3: [0 0 0 ]
[Embedding Output] Token 428 First 3: [0 0 0 ]
[Embedding Output] Token 428 Last 3: [0 0 0 ]
[Embedding Output] Token 429 First 3: [0 0 0 ]
[Embedding Output] Token 429 Last 3: [0 0 0 ]
[Embedding Output] Token 430 First 3: [0 0 0 ]
[Embedding Output] Token 430 Last 3: [0 0 0 ]
[Embedding Output] Token 431 First 3: [0 0 0 ]
[Embedding Output] Token 431 Last 3: [0 0 0 ]
[Embedding Output] Token 432 First 3: [0 0 0 ]
[Embedding Output] Token 432 Last 3: [0 0 0 ]
[Embedding Output] Token 433 First 3: [0 0 0 ]
[Embedding Output] Token 433 Last 3: [0 0 0 ]
[Embedding Output] Token 434 First 3: [0 0 0 ]
[Embedding Output] Token 434 Last 3: [0 0 0 ]
[Embedding Output] Token 435 First 3: [0 0 0 ]
[Embedding Output] Token 435 Last 3: [0 0 0 ]
[Embedding Output] Token 436 First 3: [0 0 0 ]
[Embedding Output] Token 436 Last 3: [0 0 0 ]
[Embedding Output] Token 437 First 3: [0 0 0 ]
[Embedding Output] Token 437 Last 3: [0 0 0 ]
[Embedding Output] Token 438 First 3: [0 0 0 ]
[Embedding Output] Token 438 Last 3: [0 0 0 ]
[Embedding Output] Token 439 First 3: [0 0 0 ]
[Embedding Output] Token 439 Last 3: [0 0 0 ]
[Embedding Output] Token 440 First 3: [0 0 0 ]
[Embedding Output] Token 440 Last 3: [0 0 0 ]
[Embedding Output] Token 441 First 3: [0 0 0 ]
[Embedding Output] Token 441 Last 3: [0 0 0 ]
[Embedding Output] Token 442 First 3: [0 0 0 ]
[Embedding Output] Token 442 Last 3: [0 0 0 ]
[Embedding Output] Token 443 First 3: [0 0 0 ]
[Embedding Output] Token 443 Last 3: [0 0 0 ]
[Embedding Output] Token 444 First 3: [0 0 0 ]
[Embedding Output] Token 444 Last 3: [0 0 0 ]
[Embedding Output] Token 445 First 3: [0 0 0 ]
[Embedding Output] Token 445 Last 3: [0 0 0 ]
[Embedding Output] Token 446 First 3: [0 0 0 ]
[Embedding Output] Token 446 Last 3: [0 0 0 ]
[Embedding Output] Token 447 First 3: [0 0 0 ]
[Embedding Output] Token 447 Last 3: [0 0 0 ]
[Embedding Output] Token 448 First 3: [0 0 0 ]
[Embedding Output] Token 448 Last 3: [0 0 0 ]
[Embedding Output] Token 449 First 3: [0 0 0 ]
[Embedding Output] Token 449 Last 3: [0 0 0 ]
[Embedding Output] Token 450 First 3: [0 0 0 ]
[Embedding Output] Token 450 Last 3: [0 0 0 ]
[Embedding Output] Token 451 First 3: [0 0 0 ]
[Embedding Output] Token 451 Last 3: [0 0 0 ]
[Embedding Output] Token 452 First 3: [0 0 0 ]
[Embedding Output] Token 452 Last 3: [0 0 0 ]
[Embedding Output] Token 453 First 3: [0 0 0 ]
[Embedding Output] Token 453 Last 3: [0 0 0 ]
[Embedding Output] Token 454 First 3: [0 0 0 ]
[Embedding Output] Token 454 Last 3: [0 0 0 ]
[Embedding Output] Token 455 First 3: [0 0 0 ]
[Embedding Output] Token 455 Last 3: [0 0 0 ]
[Embedding Output] Token 456 First 3: [0 0 0 ]
[Embedding Output] Token 456 Last 3: [0 0 0 ]
[Embedding Output] Token 457 First 3: [0 0 0 ]
[Embedding Output] Token 457 Last 3: [0 0 0 ]
[Embedding Output] Token 458 First 3: [0 0 0 ]
[Embedding Output] Token 458 Last 3: [0 0 0 ]
[Embedding Output] Token 459 First 3: [0 0 0 ]
[Embedding Output] Token 459 Last 3: [0 0 0 ]
[Embedding Output] Token 460 First 3: [0 0 0 ]
[Embedding Output] Token 460 Last 3: [0 0 0 ]
[Embedding Output] Token 461 First 3: [0 0 0 ]
[Embedding Output] Token 461 Last 3: [0 0 0 ]
[Embedding Output] Token 462 First 3: [0 0 0 ]
[Embedding Output] Token 462 Last 3: [0 0 0 ]
[Embedding Output] Token 463 First 3: [0 0 0 ]
[Embedding Output] Token 463 Last 3: [0 0 0 ]
[Embedding Output] Token 464 First 3: [0 0 0 ]
[Embedding Output] Token 464 Last 3: [0 0 0 ]
[Embedding Output] Token 465 First 3: [0 0 0 ]
[Embedding Output] Token 465 Last 3: [0 0 0 ]
[Embedding Output] Token 466 First 3: [0 0 0 ]
[Embedding Output] Token 466 Last 3: [0 0 0 ]
[Embedding Output] Token 467 First 3: [0 0 0 ]
[Embedding Output] Token 467 Last 3: [0 0 0 ]
[Embedding Output] Token 468 First 3: [0 0 0 ]
[Embedding Output] Token 468 Last 3: [0 0 0 ]
[Embedding Output] Token 469 First 3: [0 0 0 ]
[Embedding Output] Token 469 Last 3: [0 0 0 ]
[Embedding Output] Token 470 First 3: [0 0 0 ]
[Embedding Output] Token 470 Last 3: [0 0 0 ]
[Embedding Output] Token 471 First 3: [0 0 0 ]
[Embedding Output] Token 471 Last 3: [0 0 0 ]
[Embedding Output] Token 472 First 3: [0 0 0 ]
[Embedding Output] Token 472 Last 3: [0 0 0 ]
[Embedding Output] Token 473 First 3: [0 0 0 ]
[Embedding Output] Token 473 Last 3: [0 0 0 ]
[Embedding Output] Token 474 First 3: [0 0 0 ]
[Embedding Output] Token 474 Last 3: [0 0 0 ]
[Embedding Output] Token 475 First 3: [0 0 0 ]
[Embedding Output] Token 475 Last 3: [0 0 0 ]
[Embedding Output] Token 476 First 3: [0 0 0 ]
[Embedding Output] Token 476 Last 3: [0 0 0 ]
[Embedding Output] Token 477 First 3: [0 0 0 ]
[Embedding Output] Token 477 Last 3: [0 0 0 ]
[Embedding Output] Token 478 First 3: [0 0 0 ]
[Embedding Output] Token 478 Last 3: [0 0 0 ]
[Embedding Output] Token 479 First 3: [0 0 0 ]
[Embedding Output] Token 479 Last 3: [0 0 0 ]
[Embedding Output] Token 480 First 3: [0 0 0 ]
[Embedding Output] Token 480 Last 3: [0 0 0 ]
[Embedding Output] Token 481 First 3: [0 0 0 ]
[Embedding Output] Token 481 Last 3: [0 0 0 ]
[Embedding Output] Token 482 First 3: [0 0 0 ]
[Embedding Output] Token 482 Last 3: [0 0 0 ]
[Embedding Output] Token 483 First 3: [0 0 0 ]
[Embedding Output] Token 483 Last 3: [0 0 0 ]
[Embedding Output] Token 484 First 3: [0 0 0 ]
[Embedding Output] Token 484 Last 3: [0 0 0 ]
[Embedding Output] Token 485 First 3: [0 0 0 ]
[Embedding Output] Token 485 Last 3: [0 0 0 ]
[Embedding Output] Token 486 First 3: [0 0 0 ]
[Embedding Output] Token 486 Last 3: [0 0 0 ]
[Embedding Output] Token 487 First 3: [0 0 0 ]
[Embedding Output] Token 487 Last 3: [0 0 0 ]
[Embedding Output] Token 488 First 3: [0 0 0 ]
[Embedding Output] Token 488 Last 3: [0 0 0 ]
[Embedding Output] Token 489 First 3: [0 0 0 ]
[Embedding Output] Token 489 Last 3: [0 0 0 ]
[Embedding Output] Token 490 First 3: [0 0 0 ]
[Embedding Output] Token 490 Last 3: [0 0 0 ]
[Embedding Output] Token 491 First 3: [0 0 0 ]
[Embedding Output] Token 491 Last 3: [0 0 0 ]
[Embedding Output] Token 492 First 3: [0 0 0 ]
[Embedding Output] Token 492 Last 3: [0 0 0 ]
[Embedding Output] Token 493 First 3: [0 0 0 ]
[Embedding Output] Token 493 Last 3: [0 0 0 ]
[Embedding Output] Token 494 First 3: [0 0 0 ]
[Embedding Output] Token 494 Last 3: [0 0 0 ]
[Embedding Output] Token 495 First 3: [0 0 0 ]
[Embedding Output] Token 495 Last 3: [0 0 0 ]
[Embedding Output] Token 496 First 3: [0 0 0 ]
[Embedding Output] Token 496 Last 3: [0 0 0 ]
[Embedding Output] Token 497 First 3: [0 0 0 ]
[Embedding Output] Token 497 Last 3: [0 0 0 ]
[Embedding Output] Token 498 First 3: [0 0 0 ]
[Embedding Output] Token 498 Last 3: [0 0 0 ]
[Embedding Output] Token 499 First 3: [0 0 0 ]
[Embedding Output] Token 499 Last 3: [0 0 0 ]
[Embedding Output] Token 500 First 3: [0 0 0 ]
[Embedding Output] Token 500 Last 3: [0 0 0 ]
[Embedding Output] Token 501 First 3: [0 0 0 ]
[Embedding Output] Token 501 Last 3: [0 0 0 ]
[Embedding Output] Token 502 First 3: [0 0 0 ]
[Embedding Output] Token 502 Last 3: [0 0 0 ]
[Embedding Output] Token 503 First 3: [0 0 0 ]
[Embedding Output] Token 503 Last 3: [0 0 0 ]
[Embedding Output] Token 504 First 3: [0 0 0 ]
[Embedding Output] Token 504 Last 3: [0 0 0 ]
[Embedding Output] Token 505 First 3: [0 0 0 ]
[Embedding Output] Token 505 Last 3: [0 0 0 ]
[Embedding Output] Token 506 First 3: [0 0 0 ]
[Embedding Output] Token 506 Last 3: [0 0 0 ]
[Embedding Output] Token 507 First 3: [0 0 0 ]
[Embedding Output] Token 507 Last 3: [0 0 0 ]
[Embedding Output] Token 508 First 3: [0 0 0 ]
[Embedding Output] Token 508 Last 3: [0 0 0 ]
[Embedding Output] Token 509 First 3: [0 0 0 ]
[Embedding Output] Token 509 Last 3: [0 0 0 ]
[Embedding Output] Token 510 First 3: [0 0 0 ]
[Embedding Output] Token 510 Last 3: [0 0 0 ]
[Embedding Output] Token 511 First 3: [0 0 0 ]
[Embedding Output] Token 511 Last 3: [0 0 0 ]
[Embedding Output] Token 512 First 3: [0 0 0 ]
[Embedding Output] Token 512 Last 3: [0 0 0 ]
[Embedding Output] Token 513 First 3: [0 0 0 ]
[Embedding Output] Token 513 Last 3: [0 0 0 ]
[Embedding Output] Token 514 First 3: [0 0 0 ]
[Embedding Output] Token 514 Last 3: [0 0 0 ]
[Embedding Output] Token 515 First 3: [0 0 0 ]
[Embedding Output] Token 515 Last 3: [0 0 0 ]
[Embedding Output] Token 516 First 3: [0 0 0 ]
[Embedding Output] Token 516 Last 3: [0 0 0 ]
[Embedding Output] Token 517 First 3: [0 0 0 ]
[Embedding Output] Token 517 Last 3: [0 0 0 ]
[Embedding Output] Token 518 First 3: [0 0 0 ]
[Embedding Output] Token 518 Last 3: [0 0 0 ]
[Embedding Output] Token 519 First 3: [0 0 0 ]
[Embedding Output] Token 519 Last 3: [0 0 0 ]
[Embedding Output] Token 520 First 3: [0 0 0 ]
[Embedding Output] Token 520 Last 3: [0 0 0 ]
[Embedding Output] Token 521 First 3: [0 0 0 ]
[Embedding Output] Token 521 Last 3: [0 0 0 ]
[Embedding Output] Token 522 First 3: [0 0 0 ]
[Embedding Output] Token 522 Last 3: [0 0 0 ]
[Embedding Output] Token 523 First 3: [0 0 0 ]
[Embedding Output] Token 523 Last 3: [0 0 0 ]
[Embedding Output] Token 524 First 3: [0 0 0 ]
[Embedding Output] Token 524 Last 3: [0 0 0 ]
[Embedding Output] Token 525 First 3: [0 0 0 ]
[Embedding Output] Token 525 Last 3: [0 0 0 ]
[Embedding Output] Token 526 First 3: [0 0 0 ]
[Embedding Output] Token 526 Last 3: [0 0 0 ]
[Embedding Output] Token 527 First 3: [0 0 0 ]
[Embedding Output] Token 527 Last 3: [0 0 0 ]
[Embedding Output] Token 528 First 3: [0 0 0 ]
[Embedding Output] Token 528 Last 3: [0 0 0 ]
[Embedding Output] Token 529 First 3: [0 0 0 ]
[Embedding Output] Token 529 Last 3: [0 0 0 ]
[Embedding Output] Token 530 First 3: [0 0 0 ]
[Embedding Output] Token 530 Last 3: [0 0 0 ]
[Embedding Output] Token 531 First 3: [0 0 0 ]
[Embedding Output] Token 531 Last 3: [0 0 0 ]
[Embedding Output] Token 532 First 3: [0 0 0 ]
[Embedding Output] Token 532 Last 3: [0 0 0 ]
[Embedding Output] Token 533 First 3: [0 0 0 ]
[Embedding Output] Token 533 Last 3: [0 0 0 ]
[Embedding Output] Token 534 First 3: [0 0 0 ]
[Embedding Output] Token 534 Last 3: [0 0 0 ]
[Embedding Output] Token 535 First 3: [0 0 0 ]
[Embedding Output] Token 535 Last 3: [0 0 0 ]
[Embedding Output] Token 536 First 3: [0 0 0 ]
[Embedding Output] Token 536 Last 3: [0 0 0 ]
[Embedding Output] Token 537 First 3: [0 0 0 ]
[Embedding Output] Token 537 Last 3: [0 0 0 ]
[Embedding Output] Token 538 First 3: [0 0 0 ]
[Embedding Output] Token 538 Last 3: [0 0 0 ]
[Embedding Output] Token 539 First 3: [0 0 0 ]
[Embedding Output] Token 539 Last 3: [0 0 0 ]
[Embedding Output] Token 540 First 3: [0 0 0 ]
[Embedding Output] Token 540 Last 3: [0 0 0 ]
[Embedding Output] Token 541 First 3: [0 0 0 ]
[Embedding Output] Token 541 Last 3: [0 0 0 ]
[Embedding Output] Token 542 First 3: [0 0 0 ]
[Embedding Output] Token 542 Last 3: [0 0 0 ]
[Embedding Output] Token 543 First 3: [0 0 0 ]
[Embedding Output] Token 543 Last 3: [0 0 0 ]
[Embedding Output] Token 544 First 3: [0 0 0 ]
[Embedding Output] Token 544 Last 3: [0 0 0 ]
[Embedding Output] Token 545 First 3: [0 0 0 ]
[Embedding Output] Token 545 Last 3: [0 0 0 ]
[Embedding Output] Token 546 First 3: [0 0 0 ]
[Embedding Output] Token 546 Last 3: [0 0 0 ]
[Embedding Output] Token 547 First 3: [0 0 0 ]
[Embedding Output] Token 547 Last 3: [0 0 0 ]
[Embedding Output] Token 548 First 3: [0 0 0 ]
[Embedding Output] Token 548 Last 3: [0 0 0 ]
[Embedding Output] Token 549 First 3: [0 0 0 ]
[Embedding Output] Token 549 Last 3: [0 0 0 ]
[Embedding Output] Token 550 First 3: [0 0 0 ]
[Embedding Output] Token 550 Last 3: [0 0 0 ]
[Embedding Output] Token 551 First 3: [0 0 0 ]
[Embedding Output] Token 551 Last 3: [0 0 0 ]
[Embedding Output] Token 552 First 3: [0 0 0 ]
[Embedding Output] Token 552 Last 3: [0 0 0 ]
[Embedding Output] Token 553 First 3: [0 0 0 ]
[Embedding Output] Token 553 Last 3: [0 0 0 ]
[Embedding Output] Token 554 First 3: [0 0 0 ]
[Embedding Output] Token 554 Last 3: [0 0 0 ]
[Embedding Output] Token 555 First 3: [0 0 0 ]
[Embedding Output] Token 555 Last 3: [0 0 0 ]
[Embedding Output] Token 556 First 3: [0 0 0 ]
[Embedding Output] Token 556 Last 3: [0 0 0 ]
[Embedding Output] Token 557 First 3: [0 0 0 ]
[Embedding Output] Token 557 Last 3: [0 0 0 ]
[Embedding Output] Token 558 First 3: [0 0 0 ]
[Embedding Output] Token 558 Last 3: [0 0 0 ]
[Embedding Output] Token 559 First 3: [0 0 0 ]
[Embedding Output] Token 559 Last 3: [0 0 0 ]
[Embedding Output] Token 560 First 3: [0 0 0 ]
[Embedding Output] Token 560 Last 3: [0 0 0 ]
[Embedding Output] Token 561 First 3: [0 0 0 ]
[Embedding Output] Token 561 Last 3: [0 0 0 ]
[Embedding Output] Token 562 First 3: [0 0 0 ]
[Embedding Output] Token 562 Last 3: [0 0 0 ]
[Embedding Output] Token 563 First 3: [0 0 0 ]
[Embedding Output] Token 563 Last 3: [0 0 0 ]
[Embedding Output] Token 564 First 3: [0 0 0 ]
[Embedding Output] Token 564 Last 3: [0 0 0 ]
[Embedding Output] Token 565 First 3: [0 0 0 ]
[Embedding Output] Token 565 Last 3: [0 0 0 ]
[Embedding Output] Token 566 First 3: [0 0 0 ]
[Embedding Output] Token 566 Last 3: [0 0 0 ]
[Embedding Output] Token 567 First 3: [0 0 0 ]
[Embedding Output] Token 567 Last 3: [0 0 0 ]
[Embedding Output] Token 568 First 3: [0 0 0 ]
[Embedding Output] Token 568 Last 3: [0 0 0 ]
[Embedding Output] Token 569 First 3: [0 0 0 ]
[Embedding Output] Token 569 Last 3: [0 0 0 ]
[Embedding Output] Token 570 First 3: [0 0 0 ]
[Embedding Output] Token 570 Last 3: [0 0 0 ]
[Embedding Output] Token 571 First 3: [0 0 0 ]
[Embedding Output] Token 571 Last 3: [0 0 0 ]
[Embedding Output] Token 572 First 3: [0 0 0 ]
[Embedding Output] Token 572 Last 3: [0 0 0 ]
[Embedding Output] Token 573 First 3: [0 0 0 ]
[Embedding Output] Token 573 Last 3: [0 0 0 ]
[Embedding Output] Token 574 First 3: [0 0 0 ]
[Embedding Output] Token 574 Last 3: [0 0 0 ]
[Embedding Output] Token 575 First 3: [0 0 0 ]
[Embedding Output] Token 575 Last 3: [0 0 0 ]
[Embedding Output] Token 576 First 3: [0 0 0 ]
[Embedding Output] Token 576 Last 3: [0 0 0 ]
[Embedding Output] Token 577 First 3: [0 0 0 ]
[Embedding Output] Token 577 Last 3: [0 0 0 ]
[Embedding Output] Token 578 First 3: [0 0 0 ]
[Embedding Output] Token 578 Last 3: [0 0 0 ]
[Embedding Output] Token 579 First 3: [0 0 0 ]
[Embedding Output] Token 579 Last 3: [0 0 0 ]
[Embedding Output] Token 580 First 3: [0 0 0 ]
[Embedding Output] Token 580 Last 3: [0 0 0 ]
[Embedding Output] Token 581 First 3: [0 0 0 ]
[Embedding Output] Token 581 Last 3: [0 0 0 ]
[Embedding Output] Token 582 First 3: [0 0 0 ]
[Embedding Output] Token 582 Last 3: [0 0 0 ]
[Embedding Output] Token 583 First 3: [0 0 0 ]
[Embedding Output] Token 583 Last 3: [0 0 0 ]
[Embedding Output] Token 584 First 3: [0 0 0 ]
[Embedding Output] Token 584 Last 3: [0 0 0 ]
[Embedding Output] Token 585 First 3: [0 0 0 ]
[Embedding Output] Token 585 Last 3: [0 0 0 ]
[Embedding Output] Token 586 First 3: [0 0 0 ]
[Embedding Output] Token 586 Last 3: [0 0 0 ]
[Embedding Output] Token 587 First 3: [0 0 0 ]
[Embedding Output] Token 587 Last 3: [0 0 0 ]
[Embedding Output] Token 588 First 3: [0 0 0 ]
[Embedding Output] Token 588 Last 3: [0 0 0 ]
[Embedding Output] Token 589 First 3: [0 0 0 ]
[Embedding Output] Token 589 Last 3: [0 0 0 ]
[Embedding Output] Token 590 First 3: [0 0 0 ]
[Embedding Output] Token 590 Last 3: [0 0 0 ]
[Embedding Output] Token 591 First 3: [0 0 0 ]
[Embedding Output] Token 591 Last 3: [0 0 0 ]
[Embedding Output] Token 592 First 3: [0 0 0 ]
[Embedding Output] Token 592 Last 3: [0 0 0 ]
[Embedding Output] Token 593 First 3: [0 0 0 ]
[Embedding Output] Token 593 Last 3: [0 0 0 ]
[Embedding Output] Token 594 First 3: [0 0 0 ]
[Embedding Output] Token 594 Last 3: [0 0 0 ]
[Embedding Output] Token 595 First 3: [0 0 0 ]
[Embedding Output] Token 595 Last 3: [0 0 0 ]
[Embedding Output] Token 596 First 3: [0 0 0 ]
[Embedding Output] Token 596 Last 3: [0 0 0 ]
[Embedding Output] Token 597 First 3: [0 0 0 ]
[Embedding Output] Token 597 Last 3: [0 0 0 ]
[Embedding Output] Token 598 First 3: [0 0 0 ]
[Embedding Output] Token 598 Last 3: [0 0 0 ]
[Embedding Output] Token 599 First 3: [0 0 0 ]
[Embedding Output] Token 599 Last 3: [0 0 0 ]
[Embedding Output] Token 600 First 3: [0 0 0 ]
[Embedding Output] Token 600 Last 3: [0 0 0 ]
[Embedding Output] Token 601 First 3: [0 0 0 ]
[Embedding Output] Token 601 Last 3: [0 0 0 ]
[Embedding Output] Token 602 First 3: [0 0 0 ]
[Embedding Output] Token 602 Last 3: [0 0 0 ]
[Embedding Output] Token 603 First 3: [0 0 0 ]
[Embedding Output] Token 603 Last 3: [0 0 0 ]
[Embedding Output] Token 604 First 3: [0 0 0 ]
[Embedding Output] Token 604 Last 3: [0 0 0 ]
[Embedding Output] Token 605 First 3: [0 0 0 ]
[Embedding Output] Token 605 Last 3: [0 0 0 ]
[Embedding Output] Token 606 First 3: [0 0 0 ]
[Embedding Output] Token 606 Last 3: [0 0 0 ]
[Embedding Output] Token 607 First 3: [0 0 0 ]
[Embedding Output] Token 607 Last 3: [0 0 0 ]
[Embedding Output] Token 608 First 3: [0 0 0 ]
[Embedding Output] Token 608 Last 3: [0 0 0 ]
[Embedding Output] Token 609 First 3: [0 0 0 ]
[Embedding Output] Token 609 Last 3: [0 0 0 ]
[Embedding Output] Token 610 First 3: [0 0 0 ]
[Embedding Output] Token 610 Last 3: [0 0 0 ]
[Embedding Output] Token 611 First 3: [0 0 0 ]
[Embedding Output] Token 611 Last 3: [0 0 0 ]
[Embedding Output] Token 612 First 3: [0 0 0 ]
[Embedding Output] Token 612 Last 3: [0 0 0 ]
[Embedding Output] Token 613 First 3: [0 0 0 ]
[Embedding Output] Token 613 Last 3: [0 0 0 ]
[Embedding Output] Token 614 First 3: [0 0 0 ]
[Embedding Output] Token 614 Last 3: [0 0 0 ]
[Embedding Output] Token 615 First 3: [0 0 0 ]
[Embedding Output] Token 615 Last 3: [0 0 0 ]
[Embedding Output] Token 616 First 3: [0 0 0 ]
[Embedding Output] Token 616 Last 3: [0 0 0 ]
[Embedding Output] Token 617 First 3: [0 0 0 ]
[Embedding Output] Token 617 Last 3: [0 0 0 ]
[Embedding Output] Token 618 First 3: [0 0 0 ]
[Embedding Output] Token 618 Last 3: [0 0 0 ]
[Embedding Output] Token 619 First 3: [0 0 0 ]
[Embedding Output] Token 619 Last 3: [0 0 0 ]
[Embedding Output] Token 620 First 3: [0 0 0 ]
[Embedding Output] Token 620 Last 3: [0 0 0 ]
[Embedding Output] Token 621 First 3: [0 0 0 ]
[Embedding Output] Token 621 Last 3: [0 0 0 ]
[Embedding Output] Token 622 First 3: [0 0 0 ]
[Embedding Output] Token 622 Last 3: [0 0 0 ]
[Embedding Output] Token 623 First 3: [0 0 0 ]
[Embedding Output] Token 623 Last 3: [0 0 0 ]
[Embedding Output] Token 624 First 3: [0 0 0 ]
[Embedding Output] Token 624 Last 3: [0 0 0 ]
[Embedding Output] Token 625 First 3: [0 0 0 ]
[Embedding Output] Token 625 Last 3: [0 0 0 ]
[Embedding Output] Token 626 First 3: [0 0 0 ]
[Embedding Output] Token 626 Last 3: [0 0 0 ]
[Embedding Output] Token 627 First 3: [0 0 0 ]
[Embedding Output] Token 627 Last 3: [0 0 0 ]
[Embedding Output] Token 628 First 3: [0 0 0 ]
[Embedding Output] Token 628 Last 3: [0 0 0 ]
[Embedding Output] Token 629 First 3: [0 0 0 ]
[Embedding Output] Token 629 Last 3: [0 0 0 ]
[Embedding Output] Token 630 First 3: [0 0 0 ]
[Embedding Output] Token 630 Last 3: [0 0 0 ]
[Embedding Output] Token 631 First 3: [0 0 0 ]
[Embedding Output] Token 631 Last 3: [0 0 0 ]
[Embedding Output] Token 632 First 3: [0 0 0 ]
[Embedding Output] Token 632 Last 3: [0 0 0 ]
[Embedding Output] Token 633 First 3: [0 0 0 ]
[Embedding Output] Token 633 Last 3: [0 0 0 ]
[Embedding Output] Token 634 First 3: [0 0 0 ]
[Embedding Output] Token 634 Last 3: [0 0 0 ]
[Embedding Output] Token 635 First 3: [0 0 0 ]
[Embedding Output] Token 635 Last 3: [0 0 0 ]
[Embedding Output] Token 636 First 3: [0 0 0 ]
[Embedding Output] Token 636 Last 3: [0 0 0 ]
[Embedding Output] Token 637 First 3: [0 0 0 ]
[Embedding Output] Token 637 Last 3: [0 0 0 ]
[Embedding Output] Token 638 First 3: [0 0 0 ]
[Embedding Output] Token 638 Last 3: [0 0 0 ]
[Embedding Output] Token 639 First 3: [0 0 0 ]
[Embedding Output] Token 639 Last 3: [0 0 0 ]
[Embedding Output] Token 640 First 3: [0 0 0 ]
[Embedding Output] Token 640 Last 3: [0 0 0 ]
[Embedding Output] Token 641 First 3: [0 0 0 ]
[Embedding Output] Token 641 Last 3: [0 0 0 ]
[Embedding Output] Token 642 First 3: [0 0 0 ]
[Embedding Output] Token 642 Last 3: [0 0 0 ]
[Embedding Output] Token 643 First 3: [0 0 0 ]
[Embedding Output] Token 643 Last 3: [0 0 0 ]
[Embedding Output] Token 644 First 3: [0 0 0 ]
[Embedding Output] Token 644 Last 3: [0 0 0 ]
[Embedding Output] Token 645 First 3: [0 0 0 ]
[Embedding Output] Token 645 Last 3: [0 0 0 ]
[Embedding Output] Token 646 First 3: [0 0 0 ]
[Embedding Output] Token 646 Last 3: [0 0 0 ]
[Embedding Output] Token 647 First 3: [0 0 0 ]
[Embedding Output] Token 647 Last 3: [0 0 0 ]
[Embedding Output] Token 648 First 3: [0 0 0 ]
[Embedding Output] Token 648 Last 3: [0 0 0 ]
[Embedding Output] Token 649 First 3: [0 0 0 ]
[Embedding Output] Token 649 Last 3: [0 0 0 ]
[Embedding Output] Token 650 First 3: [0 0 0 ]
[Embedding Output] Token 650 Last 3: [0 0 0 ]
[Embedding Output] Token 651 First 3: [0 0 0 ]
[Embedding Output] Token 651 Last 3: [0 0 0 ]
[Embedding Output] Token 652 First 3: [0 0 0 ]
[Embedding Output] Token 652 Last 3: [0 0 0 ]
[Embedding Output] Token 653 First 3: [0 0 0 ]
[Embedding Output] Token 653 Last 3: [0 0 0 ]
[Embedding Output] Token 654 First 3: [0 0 0 ]
[Embedding Output] Token 654 Last 3: [0 0 0 ]
[Embedding Output] Token 655 First 3: [0 0 0 ]
[Embedding Output] Token 655 Last 3: [0 0 0 ]
[Embedding Output] Token 656 First 3: [0 0 0 ]
[Embedding Output] Token 656 Last 3: [0 0 0 ]
[Embedding Output] Token 657 First 3: [0 0 0 ]
[Embedding Output] Token 657 Last 3: [0 0 0 ]
[Embedding Output] Token 658 First 3: [0 0 0 ]
[Embedding Output] Token 658 Last 3: [0 0 0 ]
[Embedding Output] Token 659 First 3: [0 0 0 ]
[Embedding Output] Token 659 Last 3: [0 0 0 ]
[Embedding Output] Token 660 First 3: [0 0 0 ]
[Embedding Output] Token 660 Last 3: [0 0 0 ]
[Embedding Output] Token 661 First 3: [0 0 0 ]
[Embedding Output] Token 661 Last 3: [0 0 0 ]
[Embedding Output] Token 662 First 3: [0 0 0 ]
[Embedding Output] Token 662 Last 3: [0 0 0 ]
[Embedding Output] Token 663 First 3: [0 0 0 ]
[Embedding Output] Token 663 Last 3: [0 0 0 ]
[Embedding Output] Token 664 First 3: [0 0 0 ]
[Embedding Output] Token 664 Last 3: [0 0 0 ]
[Embedding Output] Token 665 First 3: [0 0 0 ]
[Embedding Output] Token 665 Last 3: [0 0 0 ]
[Embedding Output] Token 666 First 3: [0 0 0 ]
[Embedding Output] Token 666 Last 3: [0 0 0 ]
[Embedding Output] Token 667 First 3: [0 0 0 ]
[Embedding Output] Token 667 Last 3: [0 0 0 ]
[Embedding Output] Token 668 First 3: [0 0 0 ]
[Embedding Output] Token 668 Last 3: [0 0 0 ]
[Embedding Output] Token 669 First 3: [0 0 0 ]
[Embedding Output] Token 669 Last 3: [0 0 0 ]
[Embedding Output] Token 670 First 3: [0 0 0 ]
[Embedding Output] Token 670 Last 3: [0 0 0 ]
[Embedding Output] Token 671 First 3: [0 0 0 ]
[Embedding Output] Token 671 Last 3: [0 0 0 ]
[Embedding Output] Token 672 First 3: [0 0 0 ]
[Embedding Output] Token 672 Last 3: [0 0 0 ]
[Embedding Output] Token 673 First 3: [0 0 0 ]
[Embedding Output] Token 673 Last 3: [0 0 0 ]
[Embedding Output] Token 674 First 3: [0 0 0 ]
[Embedding Output] Token 674 Last 3: [0 0 0 ]
[Embedding Output] Token 675 First 3: [0 0 0 ]
[Embedding Output] Token 675 Last 3: [0 0 0 ]
[Embedding Output] Token 676 First 3: [0 0 0 ]
[Embedding Output] Token 676 Last 3: [0 0 0 ]
[Embedding Output] Token 677 First 3: [0 0 0 ]
[Embedding Output] Token 677 Last 3: [0 0 0 ]
[Embedding Output] Token 678 First 3: [0 0 0 ]
[Embedding Output] Token 678 Last 3: [0 0 0 ]
[Embedding Output] Token 679 First 3: [0 0 0 ]
[Embedding Output] Token 679 Last 3: [0 0 0 ]
[Embedding Output] Token 680 First 3: [0 0 0 ]
[Embedding Output] Token 680 Last 3: [0 0 0 ]
[Embedding Output] Token 681 First 3: [0 0 0 ]
[Embedding Output] Token 681 Last 3: [0 0 0 ]
[Embedding Output] Token 682 First 3: [0 0 0 ]
[Embedding Output] Token 682 Last 3: [0 0 0 ]
[Embedding Output] Token 683 First 3: [0 0 0 ]
[Embedding Output] Token 683 Last 3: [0 0 0 ]
[Embedding Output] Token 684 First 3: [0 0 0 ]
[Embedding Output] Token 684 Last 3: [0 0 0 ]
[Embedding Output] Token 685 First 3: [0 0 0 ]
[Embedding Output] Token 685 Last 3: [0 0 0 ]
[Embedding Output] Token 686 First 3: [0 0 0 ]
[Embedding Output] Token 686 Last 3: [0 0 0 ]
[Embedding Output] Token 687 First 3: [0 0 0 ]
[Embedding Output] Token 687 Last 3: [0 0 0 ]
[Embedding Output] Token 688 First 3: [0 0 0 ]
[Embedding Output] Token 688 Last 3: [0 0 0 ]
[Embedding Output] Token 689 First 3: [0 0 0 ]
[Embedding Output] Token 689 Last 3: [0 0 0 ]
[Embedding Output] Token 690 First 3: [0 0 0 ]
[Embedding Output] Token 690 Last 3: [0 0 0 ]
[Embedding Output] Token 691 First 3: [0 0 0 ]
[Embedding Output] Token 691 Last 3: [0 0 0 ]
[Embedding Output] Token 692 First 3: [0 0 0 ]
[Embedding Output] Token 692 Last 3: [0 0 0 ]
[Embedding Output] Token 693 First 3: [0 0 0 ]
[Embedding Output] Token 693 Last 3: [0 0 0 ]
[Embedding Output] Token 694 First 3: [0 0 0 ]
[Embedding Output] Token 694 Last 3: [0 0 0 ]
[Embedding Output] Token 695 First 3: [0 0 0 ]
[Embedding Output] Token 695 Last 3: [0 0 0 ]
[Embedding Output] Token 696 First 3: [0 0 0 ]
[Embedding Output] Token 696 Last 3: [0 0 0 ]
[Embedding Output] Token 697 First 3: [0 0 0 ]
[Embedding Output] Token 697 Last 3: [0 0 0 ]
[Embedding Output] Token 698 First 3: [0 0 0 ]
[Embedding Output] Token 698 Last 3: [0 0 0 ]
[Embedding Output] Token 699 First 3: [0 0 0 ]
[Embedding Output] Token 699 Last 3: [0 0 0 ]
[Embedding Output] Token 700 First 3: [0 0 0 ]
[Embedding Output] Token 700 Last 3: [0 0 0 ]
[Embedding Output] Token 701 First 3: [0 0 0 ]
[Embedding Output] Token 701 Last 3: [0 0 0 ]
[Embedding Output] Token 702 First 3: [0 0 0 ]
[Embedding Output] Token 702 Last 3: [0 0 0 ]
[Embedding Output] Token 703 First 3: [0 0 0 ]
[Embedding Output] Token 703 Last 3: [0 0 0 ]
[Embedding Output] Token 704 First 3: [0 0 0 ]
[Embedding Output] Token 704 Last 3: [0 0 0 ]
[Embedding Output] Token 705 First 3: [0 0 0 ]
[Embedding Output] Token 705 Last 3: [0 0 0 ]
[Embedding Output] Token 706 First 3: [0 0 0 ]
[Embedding Output] Token 706 Last 3: [0 0 0 ]
[Embedding Output] Token 707 First 3: [0 0 0 ]
[Embedding Output] Token 707 Last 3: [0 0 0 ]
[Embedding Output] Token 708 First 3: [0 0 0 ]
[Embedding Output] Token 708 Last 3: [0 0 0 ]
[Embedding Output] Token 709 First 3: [0 0 0 ]
[Embedding Output] Token 709 Last 3: [0 0 0 ]
[Embedding Output] Token 710 First 3: [0 0 0 ]
[Embedding Output] Token 710 Last 3: [0 0 0 ]
[Embedding Output] Token 711 First 3: [0 0 0 ]
[Embedding Output] Token 711 Last 3: [0 0 0 ]
[Embedding Output] Token 712 First 3: [0 0 0 ]
[Embedding Output] Token 712 Last 3: [0 0 0 ]
[Embedding Output] Token 713 First 3: [0 0 0 ]
[Embedding Output] Token 713 Last 3: [0 0 0 ]
[Embedding Output] Token 714 First 3: [0 0 0 ]
[Embedding Output] Token 714 Last 3: [0 0 0 ]
[Embedding Output] Token 715 First 3: [0 0 0 ]
[Embedding Output] Token 715 Last 3: [0 0 0 ]
[Embedding Output] Token 716 First 3: [0 0 0 ]
[Embedding Output] Token 716 Last 3: [0 0 0 ]
[Embedding Output] Token 717 First 3: [0 0 0 ]
[Embedding Output] Token 717 Last 3: [0 0 0 ]
[Embedding Output] Token 718 First 3: [0 0 0 ]
[Embedding Output] Token 718 Last 3: [0 0 0 ]
[Embedding Output] Token 719 First 3: [0 0 0 ]
[Embedding Output] Token 719 Last 3: [0 0 0 ]
[Embedding Output] Token 720 First 3: [0 0 0 ]
[Embedding Output] Token 720 Last 3: [0 0 0 ]
[Embedding Output] Token 721 First 3: [0 0 0 ]
[Embedding Output] Token 721 Last 3: [0 0 0 ]
[Embedding Output] Token 722 First 3: [0 0 0 ]
[Embedding Output] Token 722 Last 3: [0 0 0 ]
[Embedding Output] Token 723 First 3: [0 0 0 ]
[Embedding Output] Token 723 Last 3: [0 0 0 ]
[Embedding Output] Token 724 First 3: [0 0 0 ]
[Embedding Output] Token 724 Last 3: [0 0 0 ]
[Embedding Output] Token 725 First 3: [0 0 0 ]
[Embedding Output] Token 725 Last 3: [0 0 0 ]
[Embedding Output] Token 726 First 3: [0 0 0 ]
[Embedding Output] Token 726 Last 3: [0 0 0 ]
[Embedding Output] Token 727 First 3: [0 0 0 ]
[Embedding Output] Token 727 Last 3: [0 0 0 ]
[Embedding Output] Token 728 First 3: [0 0 0 ]
[Embedding Output] Token 728 Last 3: [0 0 0 ]
[Embedding Output] Token 729 First 3: [0 0 0 ]
[Embedding Output] Token 729 Last 3: [0 0 0 ]
[Embedding Output] Token 730 First 3: [0 0 0 ]
[Embedding Output] Token 730 Last 3: [0 0 0 ]
[Embedding Output] Token 731 First 3: [0 0 0 ]
[Embedding Output] Token 731 Last 3: [0 0 0 ]
[Embedding Output] Token 732 First 3: [0 0 0 ]
[Embedding Output] Token 732 Last 3: [0 0 0 ]
[Embedding Output] Token 733 First 3: [0 0 0 ]
[Embedding Output] Token 733 Last 3: [0 0 0 ]
[Embedding Output] Token 734 First 3: [0 0 0 ]
[Embedding Output] Token 734 Last 3: [0 0 0 ]
[Embedding Output] Token 735 First 3: [0 0 0 ]
[Embedding Output] Token 735 Last 3: [0 0 0 ]
[Embedding Output] Token 736 First 3: [0 0 0 ]
[Embedding Output] Token 736 Last 3: [0 0 0 ]
[Embedding Output] Token 737 First 3: [0 0 0 ]
[Embedding Output] Token 737 Last 3: [0 0 0 ]
[Embedding Output] Token 738 First 3: [0 0 0 ]
[Embedding Output] Token 738 Last 3: [0 0 0 ]
[Embedding Output] Token 739 First 3: [0 0 0 ]
[Embedding Output] Token 739 Last 3: [0 0 0 ]
[Embedding Output] Token 740 First 3: [0 0 0 ]
[Embedding Output] Token 740 Last 3: [0 0 0 ]
[Embedding Output] Token 741 First 3: [0 0 0 ]
[Embedding Output] Token 741 Last 3: [0 0 0 ]
[Embedding Output] Token 742 First 3: [0 0 0 ]
[Embedding Output] Token 742 Last 3: [0 0 0 ]
[Embedding Output] Token 743 First 3: [0 0 0 ]
[Embedding Output] Token 743 Last 3: [0 0 0 ]
[Embedding Output] Token 744 First 3: [0 0 0 ]
[Embedding Output] Token 744 Last 3: [0 0 0 ]
[Embedding Output] Token 745 First 3: [0 0 0 ]
[Embedding Output] Token 745 Last 3: [0 0 0 ]
[Embedding Output] Token 746 First 3: [0 0 0 ]
[Embedding Output] Token 746 Last 3: [0 0 0 ]
[Embedding Output] Token 747 First 3: [0 0 0 ]
[Embedding Output] Token 747 Last 3: [0 0 0 ]
[Embedding Output] Token 748 First 3: [0 0 0 ]
[Embedding Output] Token 748 Last 3: [0 0 0 ]
[Embedding Output] Token 749 First 3: [0 0 0 ]
[Embedding Output] Token 749 Last 3: [0 0 0 ]
[Embedding Output] Token 750 First 3: [0 0 0 ]
[Embedding Output] Token 750 Last 3: [0 0 0 ]
[Embedding Output] Token 751 First 3: [0 0 0 ]
[Embedding Output] Token 751 Last 3: [0 0 0 ]
[Embedding Output] Token 752 First 3: [0 0 0 ]
[Embedding Output] Token 752 Last 3: [0 0 0 ]
[Embedding Output] Token 753 First 3: [0 0 0 ]
[Embedding Output] Token 753 Last 3: [0 0 0 ]
[Embedding Output] Token 754 First 3: [0 0 0 ]
[Embedding Output] Token 754 Last 3: [0 0 0 ]
[Embedding Output] Token 755 First 3: [0 0 0 ]
[Embedding Output] Token 755 Last 3: [0 0 0 ]
[Embedding Output] Token 756 First 3: [0 0 0 ]
[Embedding Output] Token 756 Last 3: [0 0 0 ]
[Embedding Output] Token 757 First 3: [0 0 0 ]
[Embedding Output] Token 757 Last 3: [0 0 0 ]
[Embedding Output] Token 758 First 3: [0 0 0 ]
[Embedding Output] Token 758 Last 3: [0 0 0 ]
[Embedding Output] Token 759 First 3: [0 0 0 ]
[Embedding Output] Token 759 Last 3: [0 0 0 ]
[Embedding Output] Token 760 First 3: [0 0 0 ]
[Embedding Output] Token 760 Last 3: [0 0 0 ]
[Embedding Output] Token 761 First 3: [0 0 0 ]
[Embedding Output] Token 761 Last 3: [0 0 0 ]
[Embedding Output] Token 762 First 3: [0 0 0 ]
[Embedding Output] Token 762 Last 3: [0 0 0 ]
[Embedding Output] Token 763 First 3: [0 0 0 ]
[Embedding Output] Token 763 Last 3: [0 0 0 ]
[Embedding Output] Token 764 First 3: [0 0 0 ]
[Embedding Output] Token 764 Last 3: [0 0 0 ]
[Embedding Output] Token 765 First 3: [0 0 0 ]
[Embedding Output] Token 765 Last 3: [0 0 0 ]
[Embedding Output] Token 766 First 3: [0 0 0 ]
[Embedding Output] Token 766 Last 3: [0 0 0 ]
[Embedding Output] Token 767 First 3: [0 0 0 ]
[Embedding Output] Token 767 Last 3: [0 0 0 ]
[Embedding Output] Token 768 First 3: [0 0 0 ]
[Embedding Output] Token 768 Last 3: [0 0 0 ]
[Embedding Output] Token 769 First 3: [0 0 0 ]
[Embedding Output] Token 769 Last 3: [0 0 0 ]
[Embedding Output] Token 770 First 3: [0 0 0 ]
[Embedding Output] Token 770 Last 3: [0 0 0 ]
[Embedding Output] Token 771 First 3: [0 0 0 ]
[Embedding Output] Token 771 Last 3: [0 0 0 ]
[Embedding Output] Token 772 First 3: [0 0 0 ]
[Embedding Output] Token 772 Last 3: [0 0 0 ]
[Embedding Output] Token 773 First 3: [0 0 0 ]
[Embedding Output] Token 773 Last 3: [0 0 0 ]
[Embedding Output] Token 774 First 3: [0 0 0 ]
[Embedding Output] Token 774 Last 3: [0 0 0 ]
[Embedding Output] Token 775 First 3: [0 0 0 ]
[Embedding Output] Token 775 Last 3: [0 0 0 ]
[Embedding Output] Token 776 First 3: [0 0 0 ]
[Embedding Output] Token 776 Last 3: [0 0 0 ]
[Embedding Output] Token 777 First 3: [0 0 0 ]
[Embedding Output] Token 777 Last 3: [0 0 0 ]
[Embedding Output] Token 778 First 3: [0 0 0 ]
[Embedding Output] Token 778 Last 3: [0 0 0 ]
[Embedding Output] Token 779 First 3: [0 0 0 ]
[Embedding Output] Token 779 Last 3: [0 0 0 ]
[Embedding Output] Token 780 First 3: [0 0 0 ]
[Embedding Output] Token 780 Last 3: [0 0 0 ]
[Embedding Output] Token 781 First 3: [0 0 0 ]
[Embedding Output] Token 781 Last 3: [0 0 0 ]
[Embedding Output] Token 782 First 3: [0 0 0 ]
[Embedding Output] Token 782 Last 3: [0 0 0 ]
[Embedding Output] Token 783 First 3: [0 0 0 ]
[Embedding Output] Token 783 Last 3: [0 0 0 ]
[Embedding Output] Token 784 First 3: [0 0 0 ]
[Embedding Output] Token 784 Last 3: [0 0 0 ]
[Embedding Output] Token 785 First 3: [0 0 0 ]
[Embedding Output] Token 785 Last 3: [0 0 0 ]
[Embedding Output] Token 786 First 3: [0 0 0 ]
[Embedding Output] Token 786 Last 3: [0 0 0 ]
[Embedding Output] Token 787 First 3: [0 0 0 ]
[Embedding Output] Token 787 Last 3: [0 0 0 ]
[Embedding Output] Token 788 First 3: [0 0 0 ]
[Embedding Output] Token 788 Last 3: [0 0 0 ]
[Embedding Output] Token 789 First 3: [0 0 0 ]
[Embedding Output] Token 789 Last 3: [0 0 0 ]
[Embedding Output] Token 790 First 3: [0 0 0 ]
[Embedding Output] Token 790 Last 3: [0 0 0 ]
[Embedding Output] Token 791 First 3: [0 0 0 ]
[Embedding Output] Token 791 Last 3: [0 0 0 ]
[Embedding Output] Token 792 First 3: [0 0 0 ]
[Embedding Output] Token 792 Last 3: [0 0 0 ]
[Embedding Output] Token 793 First 3: [0 0 0 ]
[Embedding Output] Token 793 Last 3: [0 0 0 ]
[Embedding Output] Token 794 First 3: [0 0 0 ]
[Embedding Output] Token 794 Last 3: [0 0 0 ]
[Embedding Output] Token 795 First 3: [0 0 0 ]
[Embedding Output] Token 795 Last 3: [0 0 0 ]
[Embedding Output] Token 796 First 3: [0 0 0 ]
[Embedding Output] Token 796 Last 3: [0 0 0 ]
[Embedding Output] Token 797 First 3: [0 0 0 ]
[Embedding Output] Token 797 Last 3: [0 0 0 ]
[Embedding Output] Token 798 First 3: [0 0 0 ]
[Embedding Output] Token 798 Last 3: [0 0 0 ]
[Embedding Output] Token 799 First 3: [0 0 0 ]
[Embedding Output] Token 799 Last 3: [0 0 0 ]
[Embedding Output] Token 800 First 3: [0 0 0 ]
[Embedding Output] Token 800 Last 3: [0 0 0 ]
[Embedding Output] Token 801 First 3: [0 0 0 ]
[Embedding Output] Token 801 Last 3: [0 0 0 ]
[Embedding Output] Token 802 First 3: [0 0 0 ]
[Embedding Output] Token 802 Last 3: [0 0 0 ]
[Embedding Output] Token 803 First 3: [0 0 0 ]
[Embedding Output] Token 803 Last 3: [0 0 0 ]
[Embedding Output] Token 804 First 3: [0 0 0 ]
[Embedding Output] Token 804 Last 3: [0 0 0 ]
[Embedding Output] Token 805 First 3: [0 0 0 ]
[Embedding Output] Token 805 Last 3: [0 0 0 ]
[Embedding Output] Token 806 First 3: [0 0 0 ]
[Embedding Output] Token 806 Last 3: [0 0 0 ]
[Embedding Output] Token 807 First 3: [0 0 0 ]
[Embedding Output] Token 807 Last 3: [0 0 0 ]
[Embedding Output] Token 808 First 3: [0 0 0 ]
[Embedding Output] Token 808 Last 3: [0 0 0 ]
[Embedding Output] Token 809 First 3: [0 0 0 ]
[Embedding Output] Token 809 Last 3: [0 0 0 ]
[Embedding Output] Token 810 First 3: [0 0 0 ]
[Embedding Output] Token 810 Last 3: [0 0 0 ]
[Embedding Output] Token 811 First 3: [0 0 0 ]
[Embedding Output] Token 811 Last 3: [0 0 0 ]
[Embedding Output] Token 812 First 3: [0 0 0 ]
[Embedding Output] Token 812 Last 3: [0 0 0 ]
[Embedding Output] Token 813 First 3: [0 0 0 ]
[Embedding Output] Token 813 Last 3: [0 0 0 ]
[Embedding Output] Token 814 First 3: [0 0 0 ]
[Embedding Output] Token 814 Last 3: [0 0 0 ]
[Embedding Output] Token 815 First 3: [0 0 0 ]
[Embedding Output] Token 815 Last 3: [0 0 0 ]
[Embedding Output] Token 816 First 3: [0 0 0 ]
[Embedding Output] Token 816 Last 3: [0 0 0 ]
[Embedding Output] Token 817 First 3: [0 0 0 ]
[Embedding Output] Token 817 Last 3: [0 0 0 ]
[Embedding Output] Token 818 First 3: [0 0 0 ]
[Embedding Output] Token 818 Last 3: [0 0 0 ]
[Embedding Output] Token 819 First 3: [0 0 0 ]
[Embedding Output] Token 819 Last 3: [0 0 0 ]
[Embedding Output] Token 820 First 3: [0 0 0 ]
[Embedding Output] Token 820 Last 3: [0 0 0 ]
[Embedding Output] Token 821 First 3: [0 0 0 ]
[Embedding Output] Token 821 Last 3: [0 0 0 ]
[Embedding Output] Token 822 First 3: [0 0 0 ]
[Embedding Output] Token 822 Last 3: [0 0 0 ]
[Embedding Output] Token 823 First 3: [0 0 0 ]
[Embedding Output] Token 823 Last 3: [0 0 0 ]
[Embedding Output] Token 824 First 3: [0 0 0 ]
[Embedding Output] Token 824 Last 3: [0 0 0 ]
[Embedding Output] Token 825 First 3: [0 0 0 ]
[Embedding Output] Token 825 Last 3: [0 0 0 ]
[Embedding Output] Token 826 First 3: [0 0 0 ]
[Embedding Output] Token 826 Last 3: [0 0 0 ]
[Embedding Output] Token 827 First 3: [0 0 0 ]
[Embedding Output] Token 827 Last 3: [0 0 0 ]
[Embedding Output] Token 828 First 3: [0 0 0 ]
[Embedding Output] Token 828 Last 3: [0 0 0 ]
[Embedding Output] Token 829 First 3: [0 0 0 ]
[Embedding Output] Token 829 Last 3: [0 0 0 ]
[Embedding Output] Token 830 First 3: [0 0 0 ]
[Embedding Output] Token 830 Last 3: [0 0 0 ]
[Embedding Output] Token 831 First 3: [0 0 0 ]
[Embedding Output] Token 831 Last 3: [0 0 0 ]
[Embedding Output] Token 832 First 3: [0 0 0 ]
[Embedding Output] Token 832 Last 3: [0 0 0 ]
[Embedding Output] Token 833 First 3: [0 0 0 ]
[Embedding Output] Token 833 Last 3: [0 0 0 ]
[Embedding Output] Token 834 First 3: [0 0 0 ]
[Embedding Output] Token 834 Last 3: [0 0 0 ]
[Embedding Output] Token 835 First 3: [0 0 0 ]
[Embedding Output] Token 835 Last 3: [0 0 0 ]
[Embedding Output] Token 836 First 3: [0 0 0 ]
[Embedding Output] Token 836 Last 3: [0 0 0 ]
[Embedding Output] Token 837 First 3: [0 0 0 ]
[Embedding Output] Token 837 Last 3: [0 0 0 ]
[Embedding Output] Token 838 First 3: [0 0 0 ]
[Embedding Output] Token 838 Last 3: [0 0 0 ]
[Embedding Output] Token 839 First 3: [0 0 0 ]
[Embedding Output] Token 839 Last 3: [0 0 0 ]
[Embedding Output] Token 840 First 3: [0 0 0 ]
[Embedding Output] Token 840 Last 3: [0 0 0 ]
[Embedding Output] Token 841 First 3: [0 0 0 ]
[Embedding Output] Token 841 Last 3: [0 0 0 ]
[Embedding Output] Token 842 First 3: [0 0 0 ]
[Embedding Output] Token 842 Last 3: [0 0 0 ]
[Embedding Output] Token 843 First 3: [0 0 0 ]
[Embedding Output] Token 843 Last 3: [0 0 0 ]
[Embedding Output] Token 844 First 3: [0 0 0 ]
[Embedding Output] Token 844 Last 3: [0 0 0 ]
[Embedding Output] Token 845 First 3: [0 0 0 ]
[Embedding Output] Token 845 Last 3: [0 0 0 ]
[Embedding Output] Token 846 First 3: [0 0 0 ]
[Embedding Output] Token 846 Last 3: [0 0 0 ]
[Embedding Output] Token 847 First 3: [0 0 0 ]
[Embedding Output] Token 847 Last 3: [0 0 0 ]
[Embedding Output] Token 848 First 3: [0 0 0 ]
[Embedding Output] Token 848 Last 3: [0 0 0 ]
[Embedding Output] Token 849 First 3: [0 0 0 ]
[Embedding Output] Token 849 Last 3: [0 0 0 ]
[Embedding Output] Token 850 First 3: [0 0 0 ]
[Embedding Output] Token 850 Last 3: [0 0 0 ]
[Embedding Output] Token 851 First 3: [0 0 0 ]
[Embedding Output] Token 851 Last 3: [0 0 0 ]
[Embedding Output] Token 852 First 3: [0 0 0 ]
[Embedding Output] Token 852 Last 3: [0 0 0 ]
[Embedding Output] Token 853 First 3: [0 0 0 ]
[Embedding Output] Token 853 Last 3: [0 0 0 ]
[Embedding Output] Token 854 First 3: [0 0 0 ]
[Embedding Output] Token 854 Last 3: [0 0 0 ]
[Embedding Output] Token 855 First 3: [0 0 0 ]
[Embedding Output] Token 855 Last 3: [0 0 0 ]
[Embedding Output] Token 856 First 3: [0 0 0 ]
[Embedding Output] Token 856 Last 3: [0 0 0 ]
[Embedding Output] Token 857 First 3: [0 0 0 ]
[Embedding Output] Token 857 Last 3: [0 0 0 ]
[Embedding Output] Token 858 First 3: [0 0 0 ]
[Embedding Output] Token 858 Last 3: [0 0 0 ]
[Embedding Output] Token 859 First 3: [0 0 0 ]
[Embedding Output] Token 859 Last 3: [0 0 0 ]
[Embedding Output] Token 860 First 3: [0 0 0 ]
[Embedding Output] Token 860 Last 3: [0 0 0 ]
[Embedding Output] Token 861 First 3: [0 0 0 ]
[Embedding Output] Token 861 Last 3: [0 0 0 ]
[Embedding Output] Token 862 First 3: [0 0 0 ]
[Embedding Output] Token 862 Last 3: [0 0 0 ]
[Embedding Output] Token 863 First 3: [0 0 0 ]
[Embedding Output] Token 863 Last 3: [0 0 0 ]
[Embedding Output] Token 864 First 3: [0 0 0 ]
[Embedding Output] Token 864 Last 3: [0 0 0 ]
[Embedding Output] Token 865 First 3: [0 0 0 ]
[Embedding Output] Token 865 Last 3: [0 0 0 ]
[Embedding Output] Token 866 First 3: [0 0 0 ]
[Embedding Output] Token 866 Last 3: [0 0 0 ]
[Embedding Output] Token 867 First 3: [0 0 0 ]
[Embedding Output] Token 867 Last 3: [0 0 0 ]
[Embedding Output] Token 868 First 3: [0 0 0 ]
[Embedding Output] Token 868 Last 3: [0 0 0 ]
[Embedding Output] Token 869 First 3: [0 0 0 ]
[Embedding Output] Token 869 Last 3: [0 0 0 ]
[Embedding Output] Token 870 First 3: [0 0 0 ]
[Embedding Output] Token 870 Last 3: [0 0 0 ]
[Embedding Output] Token 871 First 3: [0 0 0 ]
[Embedding Output] Token 871 Last 3: [0 0 0 ]
[Embedding Output] Token 872 First 3: [0 0 0 ]
[Embedding Output] Token 872 Last 3: [0 0 0 ]
[Embedding Output] Token 873 First 3: [0 0 0 ]
[Embedding Output] Token 873 Last 3: [0 0 0 ]
[Embedding Output] Token 874 First 3: [0 0 0 ]
[Embedding Output] Token 874 Last 3: [0 0 0 ]
[Embedding Output] Token 875 First 3: [0 0 0 ]
[Embedding Output] Token 875 Last 3: [0 0 0 ]
[Embedding Output] Token 876 First 3: [0 0 0 ]
[Embedding Output] Token 876 Last 3: [0 0 0 ]
[Embedding Output] Token 877 First 3: [0 0 0 ]
[Embedding Output] Token 877 Last 3: [0 0 0 ]
[Embedding Output] Token 878 First 3: [0 0 0 ]
[Embedding Output] Token 878 Last 3: [0 0 0 ]
[Embedding Output] Token 879 First 3: [0 0 0 ]
[Embedding Output] Token 879 Last 3: [0 0 0 ]
[Embedding Output] Token 880 First 3: [0 0 0 ]
[Embedding Output] Token 880 Last 3: [0 0 0 ]
[Embedding Output] Token 881 First 3: [0 0 0 ]
[Embedding Output] Token 881 Last 3: [0 0 0 ]
[Embedding Output] Token 882 First 3: [0 0 0 ]
[Embedding Output] Token 882 Last 3: [0 0 0 ]
[Embedding Output] Token 883 First 3: [0 0 0 ]
[Embedding Output] Token 883 Last 3: [0 0 0 ]
[Embedding Output] Token 884 First 3: [0 0 0 ]
[Embedding Output] Token 884 Last 3: [0 0 0 ]
[Embedding Output] Token 885 First 3: [0 0 0 ]
[Embedding Output] Token 885 Last 3: [0 0 0 ]
[Embedding Output] Token 886 First 3: [0 0 0 ]
[Embedding Output] Token 886 Last 3: [0 0 0 ]
[Embedding Output] Token 887 First 3: [0 0 0 ]
[Embedding Output] Token 887 Last 3: [0 0 0 ]
[Embedding Output] Token 888 First 3: [0 0 0 ]
[Embedding Output] Token 888 Last 3: [0 0 0 ]
[Embedding Output] Token 889 First 3: [0 0 0 ]
[Embedding Output] Token 889 Last 3: [0 0 0 ]
[Embedding Output] Token 890 First 3: [0 0 0 ]
[Embedding Output] Token 890 Last 3: [0 0 0 ]
[Embedding Output] Token 891 First 3: [0 0 0 ]
[Embedding Output] Token 891 Last 3: [0 0 0 ]
[Embedding Output] Token 892 First 3: [0 0 0 ]
[Embedding Output] Token 892 Last 3: [0 0 0 ]
[Embedding Output] Token 893 First 3: [0 0 0 ]
[Embedding Output] Token 893 Last 3: [0 0 0 ]
[Embedding Output] Token 894 First 3: [0 0 0 ]
[Embedding Output] Token 894 Last 3: [0 0 0 ]
[Embedding Output] Token 895 First 3: [0 0 0 ]
[Embedding Output] Token 895 Last 3: [0 0 0 ]
[Embedding Output] Token 896 First 3: [0 0 0 ]
[Embedding Output] Token 896 Last 3: [0 0 0 ]
[Embedding Output] Token 897 First 3: [0 0 0 ]
[Embedding Output] Token 897 Last 3: [0 0 0 ]
[Embedding Output] Token 898 First 3: [0 0 0 ]
[Embedding Output] Token 898 Last 3: [0 0 0 ]
[Embedding Output] Token 899 First 3: [0 0 0 ]
[Embedding Output] Token 899 Last 3: [0 0 0 ]
[Embedding Output] Token 900 First 3: [0 0 0 ]
[Embedding Output] Token 900 Last 3: [0 0 0 ]
[Embedding Output] Token 901 First 3: [0 0 0 ]
[Embedding Output] Token 901 Last 3: [0 0 0 ]
[Embedding Output] Token 902 First 3: [0 0 0 ]
[Embedding Output] Token 902 Last 3: [0 0 0 ]
[Embedding Output] Token 903 First 3: [0 0 0 ]
[Embedding Output] Token 903 Last 3: [0 0 0 ]
[Embedding Output] Token 904 First 3: [0 0 0 ]
[Embedding Output] Token 904 Last 3: [0 0 0 ]
[Embedding Output] Token 905 First 3: [0 0 0 ]
[Embedding Output] Token 905 Last 3: [0 0 0 ]
[Embedding Output] Token 906 First 3: [0 0 0 ]
[Embedding Output] Token 906 Last 3: [0 0 0 ]
[Embedding Output] Token 907 First 3: [0 0 0 ]
[Embedding Output] Token 907 Last 3: [0 0 0 ]
[Embedding Output] Token 908 First 3: [0 0 0 ]
[Embedding Output] Token 908 Last 3: [0 0 0 ]
[Embedding Output] Token 909 First 3: [0 0 0 ]
[Embedding Output] Token 909 Last 3: [0 0 0 ]
[Embedding Output] Token 910 First 3: [0 0 0 ]
[Embedding Output] Token 910 Last 3: [0 0 0 ]
[Embedding Output] Token 911 First 3: [0 0 0 ]
[Embedding Output] Token 911 Last 3: [0 0 0 ]
[Embedding Output] Token 912 First 3: [0 0 0 ]
[Embedding Output] Token 912 Last 3: [0 0 0 ]
[Embedding Output] Token 913 First 3: [0 0 0 ]
[Embedding Output] Token 913 Last 3: [0 0 0 ]
[Embedding Output] Token 914 First 3: [0 0 0 ]
[Embedding Output] Token 914 Last 3: [0 0 0 ]
[Embedding Output] Token 915 First 3: [0 0 0 ]
[Embedding Output] Token 915 Last 3: [0 0 0 ]
[Embedding Output] Token 916 First 3: [0 0 0 ]
[Embedding Output] Token 916 Last 3: [0 0 0 ]
[Embedding Output] Token 917 First 3: [0 0 0 ]
[Embedding Output] Token 917 Last 3: [0 0 0 ]
[Embedding Output] Token 918 First 3: [0 0 0 ]
[Embedding Output] Token 918 Last 3: [0 0 0 ]
[Embedding Output] Token 919 First 3: [0 0 0 ]
[Embedding Output] Token 919 Last 3: [0 0 0 ]
[Embedding Output] Token 920 First 3: [0 0 0 ]
[Embedding Output] Token 920 Last 3: [0 0 0 ]
[Embedding Output] Token 921 First 3: [0 0 0 ]
[Embedding Output] Token 921 Last 3: [0 0 0 ]
[Embedding Output] Token 922 First 3: [0 0 0 ]
[Embedding Output] Token 922 Last 3: [0 0 0 ]
[Embedding Output] Token 923 First 3: [0 0 0 ]
[Embedding Output] Token 923 Last 3: [0 0 0 ]
[Embedding Output] Token 924 First 3: [0 0 0 ]
[Embedding Output] Token 924 Last 3: [0 0 0 ]
[Embedding Output] Token 925 First 3: [0 0 0 ]
[Embedding Output] Token 925 Last 3: [0 0 0 ]
[Embedding Output] Token 926 First 3: [0 0 0 ]
[Embedding Output] Token 926 Last 3: [0 0 0 ]
[Embedding Output] Token 927 First 3: [0 0 0 ]
[Embedding Output] Token 927 Last 3: [0 0 0 ]
[Embedding Output] Token 928 First 3: [0 0 0 ]
[Embedding Output] Token 928 Last 3: [0 0 0 ]
[Embedding Output] Token 929 First 3: [0 0 0 ]
[Embedding Output] Token 929 Last 3: [0 0 0 ]
[Embedding Output] Token 930 First 3: [0 0 0 ]
[Embedding Output] Token 930 Last 3: [0 0 0 ]
[Embedding Output] Token 931 First 3: [0 0 0 ]
[Embedding Output] Token 931 Last 3: [0 0 0 ]
[Embedding Output] Token 932 First 3: [0 0 0 ]
[Embedding Output] Token 932 Last 3: [0 0 0 ]
[Embedding Output] Token 933 First 3: [0 0 0 ]
[Embedding Output] Token 933 Last 3: [0 0 0 ]
[Embedding Output] Token 934 First 3: [0 0 0 ]
[Embedding Output] Token 934 Last 3: [0 0 0 ]
[Embedding Output] Token 935 First 3: [0 0 0 ]
[Embedding Output] Token 935 Last 3: [0 0 0 ]
[Embedding Output] Token 936 First 3: [0 0 0 ]
[Embedding Output] Token 936 Last 3: [0 0 0 ]
[Embedding Output] Token 937 First 3: [0 0 0 ]
[Embedding Output] Token 937 Last 3: [0 0 0 ]
[Embedding Output] Token 938 First 3: [0 0 0 ]
[Embedding Output] Token 938 Last 3: [0 0 0 ]
[Embedding Output] Token 939 First 3: [0 0 0 ]
[Embedding Output] Token 939 Last 3: [0 0 0 ]
[Embedding Output] Token 940 First 3: [0 0 0 ]
[Embedding Output] Token 940 Last 3: [0 0 0 ]
[Embedding Output] Token 941 First 3: [0 0 0 ]
[Embedding Output] Token 941 Last 3: [0 0 0 ]
[Embedding Output] Token 942 First 3: [0 0 0 ]
[Embedding Output] Token 942 Last 3: [0 0 0 ]
[Embedding Output] Token 943 First 3: [0 0 0 ]
[Embedding Output] Token 943 Last 3: [0 0 0 ]
[Embedding Output] Token 944 First 3: [0 0 0 ]
[Embedding Output] Token 944 Last 3: [0 0 0 ]
[Embedding Output] Token 945 First 3: [0 0 0 ]
[Embedding Output] Token 945 Last 3: [0 0 0 ]
[Embedding Output] Token 946 First 3: [0 0 0 ]
[Embedding Output] Token 946 Last 3: [0 0 0 ]
[Embedding Output] Token 947 First 3: [0 0 0 ]
[Embedding Output] Token 947 Last 3: [0 0 0 ]
[Embedding Output] Token 948 First 3: [0 0 0 ]
[Embedding Output] Token 948 Last 3: [0 0 0 ]
[Embedding Output] Token 949 First 3: [0 0 0 ]
[Embedding Output] Token 949 Last 3: [0 0 0 ]
[Embedding Output] Token 950 First 3: [0 0 0 ]
[Embedding Output] Token 950 Last 3: [0 0 0 ]
[Embedding Output] Token 951 First 3: [0 0 0 ]
[Embedding Output] Token 951 Last 3: [0 0 0 ]
[Embedding Output] Token 952 First 3: [0 0 0 ]
[Embedding Output] Token 952 Last 3: [0 0 0 ]
[Embedding Output] Token 953 First 3: [0 0 0 ]
[Embedding Output] Token 953 Last 3: [0 0 0 ]
[Embedding Output] Token 954 First 3: [0 0 0 ]
[Embedding Output] Token 954 Last 3: [0 0 0 ]
[Embedding Output] Token 955 First 3: [0 0 0 ]
[Embedding Output] Token 955 Last 3: [0 0 0 ]
[Embedding Output] Token 956 First 3: [0 0 0 ]
[Embedding Output] Token 956 Last 3: [0 0 0 ]
[Embedding Output] Token 957 First 3: [0 0 0 ]
[Embedding Output] Token 957 Last 3: [0 0 0 ]
[Embedding Output] Token 958 First 3: [0 0 0 ]
[Embedding Output] Token 958 Last 3: [0 0 0 ]
[Embedding Output] Token 959 First 3: [0 0 0 ]
[Embedding Output] Token 959 Last 3: [0 0 0 ]
[Embedding Output] Token 960 First 3: [0 0 0 ]
[Embedding Output] Token 960 Last 3: [0 0 0 ]
[Embedding Output] Token 961 First 3: [0 0 0 ]
[Embedding Output] Token 961 Last 3: [0 0 0 ]
[Embedding Output] Token 962 First 3: [0 0 0 ]
[Embedding Output] Token 962 Last 3: [0 0 0 ]
[Embedding Output] Token 963 First 3: [0 0 0 ]
[Embedding Output] Token 963 Last 3: [0 0 0 ]
[Embedding Output] Token 964 First 3: [0 0 0 ]
[Embedding Output] Token 964 Last 3: [0 0 0 ]
[Embedding Output] Token 965 First 3: [0 0 0 ]
[Embedding Output] Token 965 Last 3: [0 0 0 ]
[Embedding Output] Token 966 First 3: [0 0 0 ]
[Embedding Output] Token 966 Last 3: [0 0 0 ]
[Embedding Output] Token 967 First 3: [0 0 0 ]
[Embedding Output] Token 967 Last 3: [0 0 0 ]
[Embedding Output] Token 968 First 3: [0 0 0 ]
[Embedding Output] Token 968 Last 3: [0 0 0 ]
[Embedding Output] Token 969 First 3: [0 0 0 ]
[Embedding Output] Token 969 Last 3: [0 0 0 ]
[Embedding Output] Token 970 First 3: [0 0 0 ]
[Embedding Output] Token 970 Last 3: [0 0 0 ]
[Embedding Output] Token 971 First 3: [0 0 0 ]
[Embedding Output] Token 971 Last 3: [0 0 0 ]
[Embedding Output] Token 972 First 3: [0 0 0 ]
[Embedding Output] Token 972 Last 3: [0 0 0 ]
[Embedding Output] Token 973 First 3: [0 0 0 ]
[Embedding Output] Token 973 Last 3: [0 0 0 ]
[Embedding Output] Token 974 First 3: [0 0 0 ]
[Embedding Output] Token 974 Last 3: [0 0 0 ]
[Embedding Output] Token 975 First 3: [0 0 0 ]
[Embedding Output] Token 975 Last 3: [0 0 0 ]
[Embedding Output] Token 976 First 3: [0 0 0 ]
[Embedding Output] Token 976 Last 3: [0 0 0 ]
[Embedding Output] Token 977 First 3: [0 0 0 ]
[Embedding Output] Token 977 Last 3: [0 0 0 ]
[Embedding Output] Token 978 First 3: [0 0 0 ]
[Embedding Output] Token 978 Last 3: [0 0 0 ]
[Embedding Output] Token 979 First 3: [0 0 0 ]
[Embedding Output] Token 979 Last 3: [0 0 0 ]
[Embedding Output] Token 980 First 3: [0 0 0 ]
[Embedding Output] Token 980 Last 3: [0 0 0 ]
[Embedding Output] Token 981 First 3: [0 0 0 ]
[Embedding Output] Token 981 Last 3: [0 0 0 ]
[Embedding Output] Token 982 First 3: [0 0 0 ]
[Embedding Output] Token 982 Last 3: [0 0 0 ]
[Embedding Output] Token 983 First 3: [0 0 0 ]
[Embedding Output] Token 983 Last 3: [0 0 0 ]
[Embedding Output] Token 984 First 3: [0 0 0 ]
[Embedding Output] Token 984 Last 3: [0 0 0 ]
[Embedding Output] Token 985 First 3: [0 0 0 ]
[Embedding Output] Token 985 Last 3: [0 0 0 ]
[Embedding Output] Token 986 First 3: [0 0 0 ]
[Embedding Output] Token 986 Last 3: [0 0 0 ]
[Embedding Output] Token 987 First 3: [0 0 0 ]
[Embedding Output] Token 987 Last 3: [0 0 0 ]
[Embedding Output] Token 988 First 3: [0 0 0 ]
[Embedding Output] Token 988 Last 3: [0 0 0 ]
[Embedding Output] Token 989 First 3: [0 0 0 ]
[Embedding Output] Token 989 Last 3: [0 0 0 ]
[Embedding Output] Token 990 First 3: [0 0 0 ]
[Embedding Output] Token 990 Last 3: [0 0 0 ]
[Embedding Output] Token 991 First 3: [0 0 0 ]
[Embedding Output] Token 991 Last 3: [0 0 0 ]
[Embedding Output] Token 992 First 3: [0 0 0 ]
[Embedding Output] Token 992 Last 3: [0 0 0 ]
[Embedding Output] Token 993 First 3: [0 0 0 ]
[Embedding Output] Token 993 Last 3: [0 0 0 ]
[Embedding Output] Token 994 First 3: [0 0 0 ]
[Embedding Output] Token 994 Last 3: [0 0 0 ]
[Embedding Output] Token 995 First 3: [0 0 0 ]
[Embedding Output] Token 995 Last 3: [0 0 0 ]
[Embedding Output] Token 996 First 3: [0 0 0 ]
[Embedding Output] Token 996 Last 3: [0 0 0 ]
[Embedding Output] Token 997 First 3: [0 0 0 ]
[Embedding Output] Token 997 Last 3: [0 0 0 ]
[Embedding Output] Token 998 First 3: [0 0 0 ]
[Embedding Output] Token 998 Last 3: [0 0 0 ]
[Embedding Output] Token 999 First 3: [0 0 0 ]
[Embedding Output] Token 999 Last 3: [0 0 0 ]
[Embedding Output] Token 1000 First 3: [0 0 0 ]
[Embedding Output] Token 1000 Last 3: [0 0 0 ]
[Embedding Output] Token 1001 First 3: [0 0 0 ]
[Embedding Output] Token 1001 Last 3: [0 0 0 ]
[Embedding Output] Token 1002 First 3: [0 0 0 ]
[Embedding Output] Token 1002 Last 3: [0 0 0 ]
[Embedding Output] Token 1003 First 3: [0 0 0 ]
[Embedding Output] Token 1003 Last 3: [0 0 0 ]
[Embedding Output] Token 1004 First 3: [0 0 0 ]
[Embedding Output] Token 1004 Last 3: [0 0 0 ]
[Embedding Output] Token 1005 First 3: [0 0 0 ]
[Embedding Output] Token 1005 Last 3: [0 0 0 ]
[Embedding Output] Token 1006 First 3: [0 0 0 ]
[Embedding Output] Token 1006 Last 3: [0 0 0 ]
[Embedding Output] Token 1007 First 3: [0 0 0 ]
[Embedding Output] Token 1007 Last 3: [0 0 0 ]
[Embedding Output] Token 1008 First 3: [0 0 0 ]
[Embedding Output] Token 1008 Last 3: [0 0 0 ]
[Embedding Output] Token 1009 First 3: [0 0 0 ]
[Embedding Output] Token 1009 Last 3: [0 0 0 ]
[Embedding Output] Token 1010 First 3: [0 0 0 ]
[Embedding Output] Token 1010 Last 3: [0 0 0 ]
[Embedding Output] Token 1011 First 3: [0 0 0 ]
[Embedding Output] Token 1011 Last 3: [0 0 0 ]
[Embedding Output] Token 1012 First 3: [0 0 0 ]
[Embedding Output] Token 1012 Last 3: [0 0 0 ]
[Embedding Output] Token 1013 First 3: [0 0 0 ]
[Embedding Output] Token 1013 Last 3: [0 0 0 ]
[Embedding Output] Token 1014 First 3: [0 0 0 ]
[Embedding Output] Token 1014 Last 3: [0 0 0 ]
[Embedding Output] Token 1015 First 3: [0 0 0 ]
[Embedding Output] Token 1015 Last 3: [0 0 0 ]
[Embedding Output] Token 1016 First 3: [0 0 0 ]
[Embedding Output] Token 1016 Last 3: [0 0 0 ]
[Embedding Output] Token 1017 First 3: [0 0 0 ]
[Embedding Output] Token 1017 Last 3: [0 0 0 ]
[Embedding Output] Token 1018 First 3: [0 0 0 ]
[Embedding Output] Token 1018 Last 3: [0 0 0 ]
[Embedding Output] Token 1019 First 3: [0 0 0 ]
[Embedding Output] Token 1019 Last 3: [0 0 0 ]
[Embedding Output] Token 1020 First 3: [0 0 0 ]
[Embedding Output] Token 1020 Last 3: [0 0 0 ]
[Embedding Output] Token 1021 First 3: [0 0 0 ]
[Embedding Output] Token 1021 Last 3: [0 0 0 ]
[Embedding Output] Token 1022 First 3: [0 0 0 ]
[Embedding Output] Token 1022 Last 3: [0 0 0 ]
[Embedding Output] Token 1023 First 3: [0 0 0 ]
[Embedding Output] Token 1023 Last 3: [0 0 0 ]
--------------------
[layer0_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer0_attention_norm] Token 0 First 3: [-30.3181 -13.2179 3.80432 ]
[layer0_attention_norm] Token 0 Last 3: [13.9876 -5.23635 0.610878 ]
[layer0_attention_norm] Token 1 First 3: [-4.12448 -0.88387 -17.242 ]
[layer0_attention_norm] Token 1 Last 3: [35.8563 -2.58134 -4.49671 ]
[layer0_attention_norm] Token 2 First 3: [-3.11235 10.3766 -20.9186 ]
[layer0_attention_norm] Token 2 Last 3: [87.9295 21.2873 -8.23404 ]
--------------------
[layer0_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer0_attention_norm input] Token 0 First 3: [-2.4773 -1.26365 0.382693 ]
[layer0_attention_norm input] Token 0 Last 3: [0.5694 -0.491872 0.0579476 ]
[layer0_attention_norm input] Token 1 First 3: [-0.472067 -0.118361 -2.42951 ]
[layer0_attention_norm input] Token 1 Last 3: [2.04454 -0.339647 -0.597494 ]
[layer0_attention_norm input] Token 2 First 3: [-0.406472 1.58556 -3.36334 ]
[layer0_attention_norm input] Token 2 Last 3: [5.72101 3.19602 -1.24842 ]
--------------------
[layer0_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer0_attention_norm_weight] First 3: [14.7718 12.6254 11.9987 ]
[layer0_attention_norm_weight] Last 3: [29.6508 12.8495 12.7241 ]
--------------------
[layer0_wv input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer0_wv input] First 3: [-30.3181 -13.2179 3.80432 ]
[layer0_wv input] Last 3: [87.9295 21.2873 -8.23404 ]
--------------------
[layer0_wv_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer0_wv_weight] First 3: [0.0165389 0.0116573 -0.00459494 ]
[layer0_wv_weight] Last 3: [0.00102983 0.0020934 0.0152904 ]
--------------------
[layer0_wv] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer0_wv] First 3: [-0.19048 0.631655 -2.82159 ]
[layer0_wv] Last 3: [-1.57503 3.49855 -0.0855975 ]
--------------------
[layer0_wk input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer0_wk input] First 3: [-30.3181 -13.2179 3.80432 ]
[layer0_wk input] Last 3: [87.9295 21.2873 -8.23404 ]
--------------------
[layer0_wk_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer0_wk_weight] First 3: [0.0174795 0.0108076 -0.00312103 ]
[layer0_wk_weight] Last 3: [-0.00118842 0.00795038 0.0118715 ]
--------------------
[layer0_wk] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer0_wk] First 3: [0.948256 0.578421 0.154344 ]
[layer0_wk] Last 3: [3.12829 -1.72823 2.64325 ]
--------------------
[layer0_k_norm] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer0_k_norm] Token 0 First 3: [0.245936 0.110714 0.0181235 ]
[layer0_k_norm] Token 0 Last 3: [0.0900705 0.0585414 0.941353 ]
[layer0_k_norm] Token 1 First 3: [3.17306 3.51001 1.30624 ]
[layer0_k_norm] Token 1 Last 3: [0.475 0.37048 3.01383 ]
[layer0_k_norm] Token 2 First 3: [0.709117 -0.751565 -0.557563 ]
[layer0_k_norm] Token 2 Last 3: [0.869461 -0.323674 0.775551 ]
--------------------
[layer0_wq input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer0_wq input] First 3: [-30.3181 -13.2179 3.80432 ]
[layer0_wq input] Last 3: [87.9295 21.2873 -8.23404 ]
--------------------
[layer0_wq_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer0_wq_weight] First 3: [0.00445754 -0.00865422 -0.00513644 ]
[layer0_wq_weight] Last 3: [0.00634343 0.00257639 -0.00143187 ]
--------------------
[layer0_wq] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer0_wq] First 3: [0.388783 3.35154 -3.1459 ]
[layer0_wq] Last 3: [1.2293 -0.234323 -0.279046 ]
--------------------
[layer0_q_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer0_q_norm] Token 0 First 3: [0.151945 1.65307 0.456503 ]
[layer0_q_norm] Token 0 Last 3: [-0.557824 2.3688 -3.15082 ]
[layer0_q_norm] Token 1 First 3: [6.34423 -3.96002 -0.275963 ]
[layer0_q_norm] Token 1 Last 3: [-0.1791 0.355813 0.961775 ]
[layer0_q_norm] Token 2 First 3: [-0.688631 1.56714 0.747509 ]
[layer0_q_norm] Token 2 Last 3: [0.382217 -0.105344 -0.101745 ]
--------------------
[layer0_attention_query_rope] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer0_attention_query_rope] Token 0 First 3: [0.151945 1.65307 0.456503 ]
[layer0_attention_query_rope] Token 0 Last 3: [-0.557824 2.3688 -3.15082 ]
[layer0_attention_query_rope] Token 1 First 3: [3.68009 -1.83581 -3.84633 ]
[layer0_attention_query_rope] Token 1 Last 3: [-0.179162 0.355799 0.961787 ]
[layer0_attention_query_rope] Token 2 First 3: [0.657882 0.241131 0.567496 ]
[layer0_attention_query_rope] Token 2 Last 3: [0.382764 -0.105545 -0.101746 ]
--------------------
[layer0_attention_key_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer0_attention_value_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer0_attention_attn_probs] Shape: Shape: 1:1:6:3 [ FP32 : NCHW ]

[layer0_attention_attn_probs] Token 0 First 3: [1 1 1 ]
[layer0_attention_attn_probs] Token 0 Last 3: [1 1 1 ]
[layer0_attention_attn_probs] Token 1 First 3: [0.163689 0.824105 0.511573 ]
[layer0_attention_attn_probs] Token 1 Last 3: [0.163689 0.824105 0.511573 ]
[layer0_attention_attn_probs] Token 2 First 3: [0.836311 0.175895 0.488427 ]
[layer0_attention_attn_probs] Token 2 Last 3: [0.836311 0.175895 0.488427 ]
[layer0_attention_attn_probs] Token 3 First 3: [0.000376829 0.368573 0.542895 ]
[layer0_attention_attn_probs] Token 3 Last 3: [0.000376829 0.368573 0.542895 ]
[layer0_attention_attn_probs] Token 4 First 3: [5.89013e-05 0.238226 0.125805 ]
[layer0_attention_attn_probs] Token 4 Last 3: [5.89013e-05 0.238226 0.125805 ]
[layer0_attention_attn_probs] Token 5 First 3: [0.999564 0.393202 0.3313 ]
[layer0_attention_attn_probs] Token 5 Last 3: [0.999564 0.393202 0.3313 ]
--------------------
[layer0_attention_attention_out_core] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer0_attention_attention_out_core] Token 0 First 3: [-0.19043 0.631836 -2.82227 ]
[layer0_attention_attention_out_core] Token 0 Last 3: [-0.175659 2.94141 -0.35498 ]
[layer0_attention_attention_out_core] Token 1 First 3: [-0.667797 -2.88573 -2.43514 ]
[layer0_attention_attention_out_core] Token 1 Last 3: [0.537843 0.0690338 -0.816458 ]
[layer0_attention_attention_out_core] Token 2 First 3: [-0.368608 -2.6004 2.74174 ]
[layer0_attention_attention_out_core] Token 2 Last 3: [-0.455548 2.38598 -0.384589 ]
--------------------
[layer0_attention_out input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer0_attention_out input] First 3: [-0.19043 0.631836 -2.82227 ]
[layer0_attention_out input] Last 3: [-0.455548 2.38598 -0.384589 ]
--------------------
[layer0_attention_out_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer0_attention_out_weight] First 3: [-0.00612173 0.00328428 0.0102345 ]
[layer0_attention_out_weight] Last 3: [0.00251389 -0.00388765 0.00994184 ]
--------------------
[layer0_attention_out] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer0_attention_out] First 3: [-0.377939 -0.144592 0.362909 ]
[layer0_attention_out] Last 3: [-0.167601 -0.018045 -0.367687 ]
--------------------
[layer0_post_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer0_post_attention_norm] Token 0 First 3: [-3.07966 -0.0590973 0.0679645 ]
[layer0_post_attention_norm] Token 0 Last 3: [-0.394175 0.150722 -0.339533 ]
[layer0_post_attention_norm] Token 1 First 3: [-3.44536 -0.160003 0.0348984 ]
[layer0_post_attention_norm] Token 1 Last 3: [-0.294658 0.162375 -0.215914 ]
[layer0_post_attention_norm] Token 2 First 3: [-2.07137 -0.271543 -0.0167996 ]
[layer0_post_attention_norm] Token 2 Last 3: [-0.12877 -0.00707876 -0.106095 ]
--------------------
[layer0_post_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer0_post_attention_norm input] Token 0 First 3: [-0.377939 -0.144592 0.362909 ]
[layer0_post_attention_norm input] Token 0 Last 3: [-0.419422 0.314107 -0.961983 ]
[layer0_post_attention_norm input] Token 1 First 3: [-0.545299 -0.504874 0.240327 ]
[layer0_post_attention_norm input] Token 1 Last 3: [-0.404353 0.436415 -0.788946 ]
[layer0_post_attention_norm input] Token 2 First 3: [-0.31094 -0.812668 -0.109727 ]
[layer0_post_attention_norm input] Token 2 Last 3: [-0.167601 -0.018045 -0.367687 ]
--------------------
[layer0_post_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer0_post_attention_norm_weight] First 3: [4.04478 0.20288 0.0929605 ]
[layer0_post_attention_norm_weight] Last 3: [0.4665 0.238184 0.175198 ]
--------------------
[layer0pre_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer0pre_ffn_norm] Token 0 First 3: [-13.5812 -3.74729 0.886334 ]
[layer0pre_ffn_norm] Token 0 Last 3: [1.55247 -0.910002 -0.785401 ]
[layer0pre_ffn_norm] Token 1 First 3: [-9.4146 -0.775449 -4.63111 ]
[layer0pre_ffn_norm] Token 1 Last 3: [15.2453 -0.464983 -2.23095 ]
[layer0pre_ffn_norm] Token 2 First 3: [-4.95048 3.04309 -5.43448 ]
[layer0pre_ffn_norm] Token 2 Last 3: [40.5028 6.95369 -3.08842 ]
--------------------
[layer0_ffn_up input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer0_ffn_up input] First 3: [-13.5812 -3.74729 0.886334 ]
[layer0_ffn_up input] Last 3: [40.5028 6.95369 -3.08842 ]
--------------------
[layer0_ffn_up_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer0_ffn_up_weight] First 3: [-0.00203255 0.000215912 -0.00081092 ]
[layer0_ffn_up_weight] Last 3: [0.00715594 -0.00521602 0.0078931 ]
--------------------
[layer0_ffn_up] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer0_ffn_up] First 3: [-0.621672 -0.348892 0.293506 ]
[layer0_ffn_up] Last 3: [-2.52559 -0.63387 -0.0188567 ]
--------------------
[layer0_ffn_gate input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer0_ffn_gate input] First 3: [-13.5812 -3.74729 0.886334 ]
[layer0_ffn_gate input] Last 3: [40.5028 6.95369 -3.08842 ]
--------------------
[layer0_ffn_gate_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer0_ffn_gate_weight] First 3: [0.00226581 -0.000461018 -0.00511899 ]
[layer0_ffn_gate_weight] Last 3: [-0.000371245 -0.002286 -0.00535489 ]
--------------------
[layer0_ffn_gate] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer0_ffn_gate] First 3: [-0.207709 0.630713 -1.6627 ]
[layer0_ffn_gate] Last 3: [-1.04466 -1.66914 0.674484 ]
--------------------
[layer0_ffn_down input] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer0_ffn_down input] First 3: [0.0539403 -0.161919 -0.0235606 ]
[layer0_ffn_down input] Last 3: [0.391141 0.0503989 -0.00953795 ]
--------------------
[layer0_ffn_down_weight] Shape: Shape: 1:1:1152:768 [ FP32 : NCHW ]

[layer0_ffn_down_weight] First 3: [0.00221752 -0.007489 -0.00444963 ]
[layer0_ffn_down_weight] Last 3: [0.00149807 -0.000525553 0.00106583 ]
--------------------
[layer0_ffn_down] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer0_ffn_down] First 3: [-0.0458288 -0.0172904 -0.0557403 ]
[layer0_ffn_down] Last 3: [-0.348187 -0.471405 0.208739 ]
--------------------
[layer0post_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer0post_ffn_norm] Token 0 First 3: [-5.01865 -0.0989289 -0.216539 ]
[layer0post_ffn_norm] Token 0 Last 3: [0.212006 -0.294243 -0.348938 ]
[layer0post_ffn_norm] Token 1 First 3: [9.17795 -0.656329 0.441245 ]
[layer0post_ffn_norm] Token 1 Last 3: [-4.38839 0.0310904 0.452665 ]
[layer0post_ffn_norm] Token 2 First 3: [2.64026 0.0717323 0.107575 ]
[layer0post_ffn_norm] Token 2 Last 3: [-1.32129 -0.763434 0.269517 ]
--------------------
[layer1_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer1_attention_norm] Token 0 First 3: [-0.899478 -1.13312 0.202061 ]
[layer1_attention_norm] Token 0 Last 3: [0.222492 -0.396285 -0.454271 ]
[layer1_attention_norm] Token 1 First 3: [3.99071 -6.64477 -15.0372 ]
[layer1_attention_norm] Token 1 Last 3: [-13.522 -0.813196 -2.31819 ]
[layer1_attention_norm] Token 2 First 3: [0.0229783 1.83724 -4.6983 ]
[layer1_attention_norm] Token 2 Last 3: [4.08201 2.51636 -1.30031 ]
--------------------
[layer1_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer1_attention_norm input] Token 0 First 3: [-10.5756 -1.42168 0.234118 ]
[layer1_attention_norm input] Token 0 Last 3: [0.387231 -0.635393 -0.630523 ]
[layer1_attention_norm input] Token 1 First 3: [5.26052 -0.934693 -1.95336 ]
[layer1_attention_norm input] Token 1 Last 3: [-2.63851 -0.146182 -0.360743 ]
[layer1_attention_norm input] Token 2 First 3: [0.162416 1.38575 -3.27257 ]
[layer1_attention_norm input] Token 2 Last 3: [4.27095 2.4255 -1.08499 ]
--------------------
[layer1_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer1_attention_norm_weight] First 3: [2.85977 26.7992 29.0198 ]
[layer1_attention_norm_weight] Last 3: [19.3193 20.9707 24.2248 ]
--------------------
[layer1_wv input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer1_wv input] First 3: [-0.899478 -1.13312 0.202061 ]
[layer1_wv input] Last 3: [4.08201 2.51636 -1.30031 ]
--------------------
[layer1_wv_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer1_wv_weight] First 3: [-0.00325681 -0.00202511 -0.00400134 ]
[layer1_wv_weight] Last 3: [-0.00277166 -0.00622062 -0.00241642 ]
--------------------
[layer1_wv] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer1_wv] First 3: [-0.0063296 -0.135226 -0.0591071 ]
[layer1_wv] Last 3: [-0.263826 0.258577 -0.0128979 ]
--------------------
[layer1_wk input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer1_wk input] First 3: [-0.899478 -1.13312 0.202061 ]
[layer1_wk input] Last 3: [4.08201 2.51636 -1.30031 ]
--------------------
[layer1_wk_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer1_wk_weight] First 3: [0.0064349 0.00544602 3.88792e-05 ]
[layer1_wk_weight] Last 3: [-0.00149681 -0.00389845 -0.0027174 ]
--------------------
[layer1_wk] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer1_wk] First 3: [-0.0295157 0.00367934 -0.0229913 ]
[layer1_wk] Last 3: [0.521465 0.595265 -0.166522 ]
--------------------
[layer1_k_norm] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer1_k_norm] Token 0 First 3: [-0.0287304 0.00658148 -0.0449085 ]
[layer1_k_norm] Token 0 Last 3: [0.755728 1.03759 0.662851 ]
[layer1_k_norm] Token 1 First 3: [0.467476 3.11879 1.77561 ]
[layer1_k_norm] Token 1 Last 3: [0.36393 0.224412 0.174456 ]
[layer1_k_norm] Token 2 First 3: [-0.00180959 0.147309 -0.0464006 ]
[layer1_k_norm] Token 2 Last 3: [0.802465 0.956223 -0.281959 ]
--------------------
[layer1_wq input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer1_wq input] First 3: [-0.899478 -1.13312 0.202061 ]
[layer1_wq input] Last 3: [4.08201 2.51636 -1.30031 ]
--------------------
[layer1_wq_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer1_wq_weight] First 3: [0.00488685 0.0256994 0.00155882 ]
[layer1_wq_weight] Last 3: [0.0184449 -0.00501002 -0.000701042 ]
--------------------
[layer1_wq] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer1_wq] First 3: [0.612491 0.523138 -0.31128 ]
[layer1_wq] Last 3: [0.274674 0.328352 0.132327 ]
--------------------
[layer1_q_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer1_q_norm] Token 0 First 3: [4.00939 2.04342 -1.52968 ]
[layer1_q_norm] Token 0 Last 3: [0.17496 0.993939 0.42176 ]
[layer1_q_norm] Token 1 First 3: [11.635 1.97979 1.71894 ]
[layer1_q_norm] Token 1 Last 3: [0.0302316 0.0296558 -1.42182 ]
[layer1_q_norm] Token 2 First 3: [5.65996 -0.566159 0.639675 ]
[layer1_q_norm] Token 2 Last 3: [1.02234 1.12934 0.450324 ]
--------------------
[layer1_attention_query_rope] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer1_attention_query_rope] Token 0 First 3: [4.00939 2.04342 -1.52968 ]
[layer1_attention_query_rope] Token 0 Last 3: [0.17496 0.993939 0.42176 ]
[layer1_attention_query_rope] Token 1 First 3: [0.570376 4.96952 0.222024 ]
[layer1_attention_query_rope] Token 1 Last 3: [0.0301679 0.0296159 -1.42173 ]
[layer1_attention_query_rope] Token 2 First 3: [-1.31204 0.896558 2.16332 ]
[layer1_attention_query_rope] Token 2 Last 3: [1.02234 1.12899 0.450427 ]
--------------------
[layer1_attention_key_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer1_attention_value_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer1_attention_attn_probs] Shape: Shape: 1:1:6:3 [ FP32 : NCHW ]

[layer1_attention_attn_probs] Token 0 First 3: [1 1 1 ]
[layer1_attention_attn_probs] Token 0 Last 3: [1 1 1 ]
[layer1_attention_attn_probs] Token 1 First 3: [0.989151 0.998084 0.985625 ]
[layer1_attention_attn_probs] Token 1 Last 3: [0.989151 0.998084 0.985625 ]
[layer1_attention_attn_probs] Token 2 First 3: [0.0108491 0.00191626 0.0143746 ]
[layer1_attention_attn_probs] Token 2 Last 3: [0.0108491 0.00191626 0.0143746 ]
[layer1_attention_attn_probs] Token 3 First 3: [0.232683 0.00910469 0.747543 ]
[layer1_attention_attn_probs] Token 3 Last 3: [0.232683 0.00910469 0.747543 ]
[layer1_attention_attn_probs] Token 4 First 3: [0.000712609 0.000202637 0.00495689 ]
[layer1_attention_attn_probs] Token 4 Last 3: [0.000712609 0.000202637 0.00495689 ]
[layer1_attention_attn_probs] Token 5 First 3: [0.766604 0.990693 0.2475 ]
[layer1_attention_attn_probs] Token 5 Last 3: [0.766604 0.990693 0.2475 ]
--------------------
[layer1_attention_attention_out_core] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer1_attention_attention_out_core] Token 0 First 3: [-0.00632858 -0.135254 -0.0591125 ]
[layer1_attention_attention_out_core] Token 0 Last 3: [0.00314903 0.523438 0.0760498 ]
[layer1_attention_attention_out_core] Token 1 First 3: [0.0350388 -0.110944 -0.131025 ]
[layer1_attention_attention_out_core] Token 1 Last 3: [0.0202859 0.428262 0.0779256 ]
[layer1_attention_attention_out_core] Token 2 First 3: [-0.0381803 0.123593 -0.108544 ]
[layer1_attention_attention_out_core] Token 2 Last 3: [-0.0570401 0.425057 0.0546813 ]
--------------------
[layer1_attention_out input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer1_attention_out input] First 3: [-0.00632858 -0.135254 -0.0591125 ]
[layer1_attention_out input] Last 3: [-0.0570401 0.425057 0.0546813 ]
--------------------
[layer1_attention_out_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer1_attention_out_weight] First 3: [0.00987351 -0.0133647 -0.00462248 ]
[layer1_attention_out_weight] Last 3: [-0.00847637 0.000278182 0.0123266 ]
--------------------
[layer1_attention_out] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer1_attention_out] First 3: [-0.0384358 -0.0650442 0.221274 ]
[layer1_attention_out] Last 3: [-0.1819 -0.0574712 -0.515106 ]
--------------------
[layer1_post_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer1_post_attention_norm] Token 0 First 3: [-1.27355 -0.153629 0.361346 ]
[layer1_post_attention_norm] Token 0 Last 3: [-0.351095 0.0562208 -0.64593 ]
[layer1_post_attention_norm] Token 1 First 3: [-1.44713 -0.129166 0.340788 ]
[layer1_post_attention_norm] Token 1 Last 3: [-0.280031 0.0421061 -0.667626 ]
[layer1_post_attention_norm] Token 2 First 3: [4.82357 -0.0813195 0.39376 ]
[layer1_post_attention_norm] Token 2 Last 3: [-0.571276 -0.0720144 -0.513419 ]
--------------------
[layer1_post_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer1_post_attention_norm input] Token 0 First 3: [-0.0384358 -0.0650442 0.221274 ]
[layer1_post_attention_norm input] Token 0 Last 3: [-0.0599716 0.0240693 -0.347653 ]
[layer1_post_attention_norm input] Token 1 First 3: [-0.0441112 -0.055234 0.210772 ]
[layer1_post_attention_norm input] Token 1 Last 3: [-0.0483113 0.0182068 -0.362924 ]
[layer1_post_attention_norm input] Token 2 First 3: [0.271364 -0.0641793 0.449473 ]
[layer1_post_attention_norm input] Token 2 Last 3: [-0.1819 -0.0574712 -0.515106 ]
--------------------
[layer1_post_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer1_post_attention_norm_weight] First 3: [10.917 0.778192 0.538041 ]
[layer1_post_attention_norm_weight] Last 3: [1.92886 0.769585 0.612156 ]
--------------------
[layer1pre_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer1pre_ffn_norm] Token 0 First 3: [-0.746648 -0.679473 0.167489 ]
[layer1pre_ffn_norm] Token 0 Last 3: [0.0184241 -0.235578 -0.562828 ]
[layer1pre_ffn_norm] Token 1 First 3: [0.959917 -1.8331 -1.81194 ]
[layer1pre_ffn_norm] Token 1 Last 3: [-5.94441 -0.16911 -1.8114 ]
[layer1pre_ffn_norm] Token 2 First 3: [0.496136 0.888487 -1.27869 ]
[layer1pre_ffn_norm] Token 2 Last 3: [2.97875 1.51168 -1.11296 ]
--------------------
[layer1_ffn_up input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer1_ffn_up input] First 3: [-0.746648 -0.679473 0.167489 ]
[layer1_ffn_up input] Last 3: [2.97875 1.51168 -1.11296 ]
--------------------
[layer1_ffn_up_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer1_ffn_up_weight] First 3: [0.00523371 -0.00287404 -0.00059496 ]
[layer1_ffn_up_weight] Last 3: [-0.0045437 -0.000659866 0.00541853 ]
--------------------
[layer1_ffn_up] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer1_ffn_up] First 3: [0.133422 -0.00451071 0.0341789 ]
[layer1_ffn_up] Last 3: [-0.0262224 0.0847411 0.216429 ]
--------------------
[layer1_ffn_gate input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer1_ffn_gate input] First 3: [-0.746648 -0.679473 0.167489 ]
[layer1_ffn_gate input] Last 3: [2.97875 1.51168 -1.11296 ]
--------------------
[layer1_ffn_gate_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer1_ffn_gate_weight] First 3: [0.0196996 -0.00483528 -0.00266094 ]
[layer1_ffn_gate_weight] Last 3: [0.00150826 -4.19137e-06 0.010422 ]
--------------------
[layer1_ffn_gate] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer1_ffn_gate] First 3: [-0.0712787 0.0136189 -0.0483603 ]
[layer1_ffn_gate] Last 3: [-0.261925 -0.113203 -0.149251 ]
--------------------
[layer1_ffn_down input] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer1_ffn_down input] First 3: [-0.00448486 -3.10491e-05 -0.000794575 ]
[layer1_ffn_down input] Last 3: [0.00272462 -0.00436416 -0.014235 ]
--------------------
[layer1_ffn_down_weight] Shape: Shape: 1:1:1152:768 [ FP32 : NCHW ]

[layer1_ffn_down_weight] First 3: [0.00928759 -0.00320032 0.00617766 ]
[layer1_ffn_down_weight] Last 3: [-0.00354683 -0.00263309 0.000859006 ]
--------------------
[layer1_ffn_down] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer1_ffn_down] First 3: [0.00291088 0.000308612 0.00063147 ]
[layer1_ffn_down] Last 3: [-0.0196913 -0.00380683 0.00289712 ]
--------------------
[layer1post_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer1post_ffn_norm] Token 0 First 3: [11.8742 0.0945449 0.114661 ]
[layer1post_ffn_norm] Token 0 Last 3: [0.300722 0.497895 -0.153928 ]
[layer1post_ffn_norm] Token 1 First 3: [-1.67224 -0.652849 0.278548 ]
[layer1post_ffn_norm] Token 1 Last 3: [-1.00619 -0.216315 0.676744 ]
[layer1post_ffn_norm] Token 2 First 3: [7.60457 -0.289488 0.783813 ]
[layer1post_ffn_norm] Token 2 Last 3: [-1.76255 -0.310997 0.187092 ]
--------------------
[layer2_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer2_attention_norm] Token 0 First 3: [0.00271945 -1.13288 0.627903 ]
[layer2_attention_norm] Token 0 Last 3: [0.184938 -0.0584858 -1.20118 ]
[layer2_attention_norm] Token 1 First 3: [0.690569 -3.89973 -3.50238 ]
[layer2_attention_norm] Token 1 Last 3: [-6.39779 -0.684546 -0.876757 ]
[layer2_attention_norm] Token 2 First 3: [1.92581 1.09343 -2.6085 ]
[layer2_attention_norm] Token 2 Last 3: [1.49757 2.06963 -1.66891 ]
--------------------
[layer2_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer2_attention_norm input] Token 0 First 3: [0.0250359 -1.48076 0.710125 ]
[layer2_attention_norm input] Token 0 Last 3: [0.336857 -0.0812772 -1.43038 ]
[layer2_attention_norm input] Token 1 First 3: [2.14115 -1.71671 -1.33402 ]
[layer2_attention_norm input] Token 1 Last 3: [-3.92473 -0.320391 -0.351625 ]
[layer2_attention_norm input] Token 2 First 3: [12.5906 1.01495 -2.09499 ]
[layer2_attention_norm input] Token 2 Last 3: [1.93712 2.04249 -1.41132 ]
--------------------
[layer2_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer2_attention_norm_weight] First 3: [4.02748 28.367 32.7849 ]
[layer2_attention_norm_weight] Last 3: [20.3561 26.6807 31.1367 ]
--------------------
[layer2_wv input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer2_wv input] First 3: [0.00271945 -1.13288 0.627903 ]
[layer2_wv input] Last 3: [1.49757 2.06963 -1.66891 ]
--------------------
[layer2_wv_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer2_wv_weight] First 3: [-0.00530868 -0.00125764 -0.00236829 ]
[layer2_wv_weight] Last 3: [-0.00375193 -0.00973588 -0.00529384 ]
--------------------
[layer2_wv] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer2_wv] First 3: [0.0992869 -0.0166889 -0.204965 ]
[layer2_wv] Last 3: [0.340418 -0.256636 2.02861 ]
--------------------
[layer2_wk input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer2_wk input] First 3: [0.00271945 -1.13288 0.627903 ]
[layer2_wk input] Last 3: [1.49757 2.06963 -1.66891 ]
--------------------
[layer2_wk_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer2_wk_weight] First 3: [0.00721993 0.00940384 -0.00149919 ]
[layer2_wk_weight] Last 3: [0.00381705 0.0192006 0.00587659 ]
--------------------
[layer2_wk] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer2_wk] First 3: [-1.05517 -1.94871 -0.491197 ]
[layer2_wk] Last 3: [3.24272 0.802859 0.207341 ]
--------------------
[layer2_k_norm] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer2_k_norm] Token 0 First 3: [-2.88022 -3.15708 -0.859219 ]
[layer2_k_norm] Token 0 Last 3: [3.8606 1.34253 -1.09326 ]
[layer2_k_norm] Token 1 First 3: [-4.5141 -4.69729 -0.550679 ]
[layer2_k_norm] Token 1 Last 3: [3.91312 1.39053 -0.40868 ]
[layer2_k_norm] Token 2 First 3: [-0.377032 -3.08096 -0.268531 ]
[layer2_k_norm] Token 2 Last 3: [3.7533 1.02747 0.29697 ]
--------------------
[layer2_wq input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer2_wq input] First 3: [0.00271945 -1.13288 0.627903 ]
[layer2_wq input] Last 3: [1.49757 2.06963 -1.66891 ]
--------------------
[layer2_wq_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer2_wq_weight] First 3: [0.00587441 -0.00147753 -0.00971008 ]
[layer2_wq_weight] Last 3: [0.000502993 -0.00523375 -0.00758126 ]
--------------------
[layer2_wq] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer2_wq] First 3: [-0.363358 0.0178312 -0.0214596 ]
[layer2_wq] Last 3: [0.0312391 0.281304 -1.09708 ]
--------------------
[layer2_q_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer2_q_norm] Token 0 First 3: [-1.96739 0.0202579 -0.142951 ]
[layer2_q_norm] Token 0 Last 3: [0.219025 0.41927 -0.770445 ]
[layer2_q_norm] Token 1 First 3: [-3.39883 -0.16339 -2.99464 ]
[layer2_q_norm] Token 1 Last 3: [-0.256579 1.37459 0.591495 ]
[layer2_q_norm] Token 2 First 3: [-0.121364 0.0538398 -1.34545 ]
[layer2_q_norm] Token 2 Last 3: [0.0567464 0.461858 -1.80647 ]
--------------------
[layer2_attention_query_rope] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer2_attention_query_rope] Token 0 First 3: [-1.96739 0.0202579 -0.142951 ]
[layer2_attention_query_rope] Token 0 Last 3: [0.219025 0.41927 -0.770445 ]
[layer2_attention_query_rope] Token 1 First 3: [-2.31699 -1.62089 -2.80565 ]
[layer2_attention_query_rope] Token 1 Last 3: [-0.256407 1.37455 0.591389 ]
[layer2_attention_query_rope] Token 2 First 3: [0.0465699 0.0937419 -0.483462 ]
[layer2_attention_query_rope] Token 2 Last 3: [0.0568761 0.461662 -1.80627 ]
--------------------
[layer2_attention_key_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer2_attention_value_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer2_attention_attn_probs] Shape: Shape: 1:1:6:3 [ FP32 : NCHW ]

[layer2_attention_attn_probs] Token 0 First 3: [1 1 1 ]
[layer2_attention_attn_probs] Token 0 Last 3: [1 1 1 ]
[layer2_attention_attn_probs] Token 1 First 3: [0.686263 0.575175 0.997587 ]
[layer2_attention_attn_probs] Token 1 Last 3: [0.686263 0.575175 0.997587 ]
[layer2_attention_attn_probs] Token 2 First 3: [0.313737 0.424825 0.00241291 ]
[layer2_attention_attn_probs] Token 2 Last 3: [0.313737 0.424825 0.00241291 ]
[layer2_attention_attn_probs] Token 3 First 3: [0.471274 0.487333 0.215057 ]
[layer2_attention_attn_probs] Token 3 Last 3: [0.471274 0.487333 0.215057 ]
[layer2_attention_attn_probs] Token 4 First 3: [0.00139105 0.0116258 0.667492 ]
[layer2_attention_attn_probs] Token 4 Last 3: [0.00139105 0.0116258 0.667492 ]
[layer2_attention_attn_probs] Token 5 First 3: [0.527334 0.501041 0.117451 ]
[layer2_attention_attn_probs] Token 5 Last 3: [0.527334 0.501041 0.117451 ]
--------------------
[layer2_attention_attention_out_core] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer2_attention_attention_out_core] Token 0 First 3: [0.0993042 -0.0166931 -0.204956 ]
[layer2_attention_attention_out_core] Token 0 Last 3: [-0.0334473 0.163574 0.0176392 ]
[layer2_attention_attention_out_core] Token 1 First 3: [-0.159954 -0.161507 -0.168037 ]
[layer2_attention_attention_out_core] Token 1 Last 3: [-0.0340193 0.162617 0.0100091 ]
[layer2_attention_attention_out_core] Token 2 First 3: [0.221395 -0.145129 0.0935713 ]
[layer2_attention_attention_out_core] Token 2 Last 3: [-0.147783 -0.150669 -1.85681 ]
--------------------
[layer2_attention_out input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer2_attention_out input] First 3: [0.0993042 -0.0166931 -0.204956 ]
[layer2_attention_out input] Last 3: [-0.147783 -0.150669 -1.85681 ]
--------------------
[layer2_attention_out_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer2_attention_out_weight] First 3: [-0.00604033 -0.00260302 0.00849657 ]
[layer2_attention_out_weight] Last 3: [0.0038065 -0.00293189 0.000120734 ]
--------------------
[layer2_attention_out] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer2_attention_out] First 3: [0.0282374 0.0367191 -0.00804938 ]
[layer2_attention_out] Last 3: [-0.200783 -0.0472085 0.198416 ]
--------------------
[layer2_post_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer2_post_attention_norm] Token 0 First 3: [2.60119 0.150434 -0.0180676 ]
[layer2_post_attention_norm] Token 0 Last 3: [-0.823786 -0.0331858 0.285256 ]
[layer2_post_attention_norm] Token 1 First 3: [-8.14332 0.232978 0.00974852 ]
[layer2_post_attention_norm] Token 1 Last 3: [-0.156596 0.0235453 -0.106016 ]
[layer2_post_attention_norm] Token 2 First 3: [-0.350314 -0.124557 0.0656869 ]
[layer2_post_attention_norm] Token 2 Last 3: [-1.28588 -0.106131 0.53865 ]
--------------------
[layer2_post_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer2_post_attention_norm input] Token 0 First 3: [0.0282374 0.0367191 -0.00804938 ]
[layer2_post_attention_norm input] Token 0 Last 3: [-0.0662412 -0.00760183 0.0541119 ]
[layer2_post_attention_norm input] Token 1 First 3: [-0.102177 0.0657296 0.00501997 ]
[layer2_post_attention_norm input] Token 1 Last 3: [-0.0145544 0.00623404 -0.023245 ]
[layer2_post_attention_norm input] Token 2 First 3: [-0.00738452 -0.0590375 0.0568269 ]
[layer2_post_attention_norm input] Token 2 Last 3: [-0.200783 -0.0472085 0.198416 ]
--------------------
[layer2_post_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer2_post_attention_norm_weight] First 3: [13.1733 0.585872 0.320986 ]
[layer2_post_attention_norm_weight] Last 3: [1.77842 0.624284 0.753859 ]
--------------------
[layer2pre_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer2pre_ffn_norm] Token 0 First 3: [0.19425 -1.38649 0.539736 ]
[layer2pre_ffn_norm] Token 0 Last 3: [-0.370867 -0.101119 -1.21146 ]
[layer2pre_ffn_norm] Token 1 First 3: [-1.3336 -4.64518 -3.10245 ]
[layer2pre_ffn_norm] Token 1 Last 3: [-9.33776 -0.787747 -1.45434 ]
[layer2pre_ffn_norm] Token 2 First 3: [1.22868 1.25939 -2.14786 ]
[layer2pre_ffn_norm] Token 2 Last 3: [0.673157 2.32153 -1.25292 ]
--------------------
[layer2_ffn_up input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer2_ffn_up input] First 3: [0.19425 -1.38649 0.539736 ]
[layer2_ffn_up input] Last 3: [0.673157 2.32153 -1.25292 ]
--------------------
[layer2_ffn_up_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer2_ffn_up_weight] First 3: [0.00658236 0.00283664 0.00570155 ]
[layer2_ffn_up_weight] Last 3: [-0.00788294 0.00791449 -0.00234142 ]
--------------------
[layer2_ffn_up] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer2_ffn_up] First 3: [-0.0102392 -0.323754 0.0259309 ]
[layer2_ffn_up] Last 3: [0.159078 -1.15643 0.24483 ]
--------------------
[layer2_ffn_gate input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer2_ffn_gate input] First 3: [0.19425 -1.38649 0.539736 ]
[layer2_ffn_gate input] Last 3: [0.673157 2.32153 -1.25292 ]
--------------------
[layer2_ffn_gate_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer2_ffn_gate_weight] First 3: [0.00665672 -1.98802e-05 0.00429954 ]
[layer2_ffn_gate_weight] Last 3: [0.00385995 -0.00557108 -0.00777056 ]
--------------------
[layer2_ffn_gate] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer2_ffn_gate] First 3: [-0.0326961 -0.0499359 0.0626014 ]
[layer2_ffn_gate] Last 3: [-0.864534 0.275457 -0.498667 ]
--------------------
[layer2_ffn_down input] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer2_ffn_down input] First 3: [0.000163024 0.00776154 0.00085217 ]
[layer2_ffn_down input] Last 3: [-0.0266488 -0.193839 -0.0377304 ]
--------------------
[layer2_ffn_down_weight] Shape: Shape: 1:1:1152:768 [ FP32 : NCHW ]

[layer2_ffn_down_weight] First 3: [0.000513212 -0.0143816 -0.00106516 ]
[layer2_ffn_down_weight] Last 3: [-0.000699075 0.00128977 0.000309561 ]
--------------------
[layer2_ffn_down] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer2_ffn_down] First 3: [0.0120043 -0.00662938 -0.00667734 ]
[layer2_ffn_down] Last 3: [-0.0204947 0.0144741 0.0067176 ]
--------------------
[layer2post_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer2post_ffn_norm] Token 0 First 3: [12.6723 -0.605145 -0.324073 ]
[layer2post_ffn_norm] Token 0 Last 3: [1.05515 0.764887 0.36129 ]
[layer2post_ffn_norm] Token 1 First 3: [7.84091 -0.805658 -0.185736 ]
[layer2post_ffn_norm] Token 1 Last 3: [-2.35443 -0.485194 -0.813429 ]
[layer2post_ffn_norm] Token 2 First 3: [8.10087 -0.796232 0.0810777 ]
[layer2post_ffn_norm] Token 2 Last 3: [-2.15021 1.13654 0.522529 ]
--------------------
[layer3_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer3_attention_norm] Token 0 First 3: [2.55053 -2.32165 0.433469 ]
[layer3_attention_norm] Token 0 Last 3: [0.418125 0.703195 -1.0826 ]
[layer3_attention_norm] Token 1 First 3: [0.880749 -7.89008 -5.11048 ]
[layer3_attention_norm] Token 1 Last 3: [-13.6064 -2.42918 -5.04389 ]
[layer3_attention_norm] Token 2 First 3: [3.44356 0.114688 -2.33035 ]
[layer3_attention_norm] Token 2 Last 3: [-1.12005 3.3735 -0.491068 ]
--------------------
[layer3_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer3_attention_norm input] Token 0 First 3: [15.2985 -1.93547 0.367984 ]
[layer3_attention_norm input] Token 0 Last 3: [0.568217 0.650424 -0.783836 ]
[layer3_attention_norm input] Token 1 First 3: [1.83874 -2.28939 -1.51001 ]
[layer3_attention_norm input] Token 1 Last 3: [-6.43576 -0.782039 -1.27107 ]
[layer3_attention_norm input] Token 2 First 3: [20.3411 0.0941572 -1.94823 ]
[layer3_attention_norm input] Token 2 Last 3: [-1.49897 3.0729 -0.350142 ]
--------------------
[layer3_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer3_attention_norm_weight] First 3: [4.9243 35.4303 34.7932 ]
[layer3_attention_norm_weight] Last 3: [21.7349 31.9333 40.7952 ]
--------------------
[layer3_wv input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer3_wv input] First 3: [2.55053 -2.32165 0.433469 ]
[layer3_wv input] Last 3: [-1.12005 3.3735 -0.491068 ]
--------------------
[layer3_wv_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer3_wv_weight] First 3: [0.0106198 -0.0193535 0.00431076 ]
[layer3_wv_weight] Last 3: [-0.00516676 -0.00436494 -0.00465336 ]
--------------------
[layer3_wv] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer3_wv] First 3: [0.607595 -0.420162 0.434005 ]
[layer3_wv] Last 3: [0.32963 0.173332 0.559271 ]
--------------------
[layer3_wk input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer3_wk input] First 3: [2.55053 -2.32165 0.433469 ]
[layer3_wk input] Last 3: [-1.12005 3.3735 -0.491068 ]
--------------------
[layer3_wk_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer3_wk_weight] First 3: [-0.0127108 -0.0188006 -0.0221706 ]
[layer3_wk_weight] Last 3: [-0.00581932 0.0030838 -0.00215899 ]
--------------------
[layer3_wk] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer3_wk] First 3: [1.34397 -0.322244 0.0588182 ]
[layer3_wk] Last 3: [0.694798 -0.448417 1.13869 ]
--------------------
[layer3_k_norm] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer3_k_norm] Token 0 First 3: [1.2438 -0.428558 0.0775338 ]
[layer3_k_norm] Token 0 Last 3: [0.871329 -1.01481 0.70445 ]
[layer3_k_norm] Token 1 First 3: [1.19492 3.02153 1.55019 ]
[layer3_k_norm] Token 1 Last 3: [0.556073 0.174152 1.03913 ]
[layer3_k_norm] Token 2 First 3: [1.57769 -0.399679 0.49109 ]
[layer3_k_norm] Token 2 Last 3: [0.658531 -0.477688 0.981841 ]
--------------------
[layer3_wq input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer3_wq input] First 3: [2.55053 -2.32165 0.433469 ]
[layer3_wq input] Last 3: [-1.12005 3.3735 -0.491068 ]
--------------------
[layer3_wq_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer3_wq_weight] First 3: [-0.0247345 -0.0160562 -0.00605495 ]
[layer3_wq_weight] Last 3: [0.0112121 0.00192617 -0.00124826 ]
--------------------
[layer3_wq] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer3_wq] First 3: [5.27844 0.96973 -0.0238123 ]
[layer3_wq] Last 3: [-0.329485 -0.922796 0.474187 ]
--------------------
[layer3_q_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer3_q_norm] Token 0 First 3: [14.8816 1.88106 -0.0485566 ]
[layer3_q_norm] Token 0 Last 3: [-0.675705 0.421711 2.0073 ]
[layer3_q_norm] Token 1 First 3: [14.3057 0.274732 0.601831 ]
[layer3_q_norm] Token 1 Last 3: [0.120127 0.552163 0.27044 ]
[layer3_q_norm] Token 2 First 3: [13.6555 0.237394 -0.853603 ]
[layer3_q_norm] Token 2 Last 3: [-0.421696 -0.766041 0.561917 ]
--------------------
[layer3_attention_query_rope] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer3_attention_query_rope] Token 0 First 3: [14.8816 1.88106 -0.0485566 ]
[layer3_attention_query_rope] Token 0 Last 3: [-0.675705 0.421711 2.0073 ]
[layer3_attention_query_rope] Token 1 First 3: [8.91038 -0.703805 -1.28915 ]
[layer3_attention_query_rope] Token 1 Last 3: [0.120084 0.552075 0.270572 ]
[layer3_attention_query_rope] Token 2 First 3: [-3.47466 0.7901 -3.21992 ]
[layer3_attention_query_rope] Token 2 Last 3: [-0.421627 -0.765898 0.561888 ]
--------------------
[layer3_attention_key_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer3_attention_value_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer3_attention_attn_probs] Shape: Shape: 1:1:6:3 [ FP32 : NCHW ]

[layer3_attention_attn_probs] Token 0 First 3: [1 1 1 ]
[layer3_attention_attn_probs] Token 0 Last 3: [1 1 1 ]
[layer3_attention_attn_probs] Token 1 First 3: [0.078805 0.97622 0.552341 ]
[layer3_attention_attn_probs] Token 1 Last 3: [0.078805 0.97622 0.552341 ]
[layer3_attention_attn_probs] Token 2 First 3: [0.921195 0.0237796 0.447659 ]
[layer3_attention_attn_probs] Token 2 Last 3: [0.921195 0.0237796 0.447659 ]
[layer3_attention_attn_probs] Token 3 First 3: [0.00027921 0.0921351 0.651136 ]
[layer3_attention_attn_probs] Token 3 Last 3: [0.00027921 0.0921351 0.651136 ]
[layer3_attention_attn_probs] Token 4 First 3: [2.89398e-05 0.00152897 0.158974 ]
[layer3_attention_attn_probs] Token 4 Last 3: [2.89398e-05 0.00152897 0.158974 ]
[layer3_attention_attn_probs] Token 5 First 3: [0.999692 0.906336 0.18989 ]
[layer3_attention_attn_probs] Token 5 Last 3: [0.999692 0.906336 0.18989 ]
--------------------
[layer3_attention_attention_out_core] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer3_attention_attention_out_core] Token 0 First 3: [0.607422 -0.420166 0.434082 ]
[layer3_attention_attention_out_core] Token 0 Last 3: [-0.556641 0.539551 -0.00440216 ]
[layer3_attention_attention_out_core] Token 1 First 3: [1.55201 -3.60634 0.303639 ]
[layer3_attention_attention_out_core] Token 1 Last 3: [0.320753 1.12994 -0.043935 ]
[layer3_attention_attention_out_core] Token 2 First 3: [0.0387182 -0.857387 0.0443055 ]
[layer3_attention_attention_out_core] Token 2 Last 3: [-0.076772 0.679673 0.0885589 ]
--------------------
[layer3_attention_out input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer3_attention_out input] First 3: [0.607422 -0.420166 0.434082 ]
[layer3_attention_out input] Last 3: [-0.076772 0.679673 0.0885589 ]
--------------------
[layer3_attention_out_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer3_attention_out_weight] First 3: [-0.0182081 0.00922823 0.00901432 ]
[layer3_attention_out_weight] Last 3: [0.00399009 -0.0074626 -0.00954604 ]
--------------------
[layer3_attention_out] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer3_attention_out] First 3: [0.077364 -0.136648 0.0228948 ]
[layer3_attention_out] Last 3: [-0.380125 0.00338682 0.0599604 ]
--------------------
[layer3_post_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer3_post_attention_norm] Token 0 First 3: [2.58618 -0.407705 0.0196821 ]
[layer3_post_attention_norm] Token 0 Last 3: [-0.657512 0.216566 1.08423 ]
[layer3_post_attention_norm] Token 1 First 3: [10.187 -0.939034 0.132227 ]
[layer3_post_attention_norm] Token 1 Last 3: [-1.19637 0.0479067 0.55823 ]
[layer3_post_attention_norm] Token 2 First 3: [9.81094 -0.30115 0.029651 ]
[layer3_post_attention_norm] Token 2 Last 3: [-2.98785 0.0102556 0.282687 ]
--------------------
[layer3_post_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer3_post_attention_norm input] Token 0 First 3: [0.077364 -0.136648 0.0228948 ]
[layer3_post_attention_norm input] Token 0 Last 3: [-0.08007 0.0684573 0.22013 ]
[layer3_post_attention_norm input] Token 1 First 3: [0.539303 -0.556985 0.272202 ]
[layer3_post_attention_norm input] Token 1 Last 3: [-0.257833 0.0267998 0.200575 ]
[layer3_post_attention_norm input] Token 2 First 3: [0.306614 -0.105448 0.0360335 ]
[layer3_post_attention_norm input] Token 2 Last 3: [-0.380125 0.00338682 0.0599604 ]
--------------------
[layer3_post_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer3_post_attention_norm_weight] First 3: [7.85646 0.701214 0.202042 ]
[layer3_post_attention_norm_weight] Last 3: [1.92993 0.743495 1.15758 ]
--------------------
[layer3pre_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer3pre_ffn_norm] Token 0 First 3: [0.9975 -1.75177 0.250428 ]
[layer3pre_ffn_norm] Token 0 Last 3: [-0.0399882 0.535468 0.213174 ]
[layer3pre_ffn_norm] Token 1 First 3: [1.70943 -6.15137 -2.26838 ]
[layer3pre_ffn_norm] Token 1 Last 3: [-8.71083 -1.15559 -1.28927 ]
[layer3pre_ffn_norm] Token 2 First 3: [1.55516 -0.143105 -1.14613 ]
[layer3pre_ffn_norm] Token 2 Last 3: [-1.85811 1.76094 -0.0442672 ]
--------------------
[layer3_ffn_up input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer3_ffn_up input] First 3: [0.9975 -1.75177 0.250428 ]
[layer3_ffn_up input] Last 3: [-1.85811 1.76094 -0.0442672 ]
--------------------
[layer3_ffn_up_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer3_ffn_up_weight] First 3: [-0.00125217 -0.00951028 0.00651315 ]
[layer3_ffn_up_weight] Last 3: [-0.00867311 0.00456075 0.00364573 ]
--------------------
[layer3_ffn_up] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer3_ffn_up] First 3: [-0.113988 -0.186416 -0.0864066 ]
[layer3_ffn_up] Last 3: [0.097657 -0.158572 -0.126895 ]
--------------------
[layer3_ffn_gate input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer3_ffn_gate input] First 3: [0.9975 -1.75177 0.250428 ]
[layer3_ffn_gate input] Last 3: [-1.85811 1.76094 -0.0442672 ]
--------------------
[layer3_ffn_gate_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer3_ffn_gate_weight] First 3: [0.00341326 -0.00340947 0.00816155 ]
[layer3_ffn_gate_weight] Last 3: [-0.00278708 -0.000159793 -0.00528316 ]
--------------------
[layer3_ffn_gate] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer3_ffn_gate] First 3: [-0.0708374 -0.182851 -0.222775 ]
[layer3_ffn_gate] Last 3: [-0.0896087 -0.0280209 0.087543 ]
--------------------
[layer3_ffn_down input] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer3_ffn_down input] First 3: [0.00380932 0.0145706 0.00792797 ]
[layer3_ffn_down input] Last 3: [-0.00406305 0.002172 -0.00594187 ]
--------------------
[layer3_ffn_down_weight] Shape: Shape: 1:1:1152:768 [ FP32 : NCHW ]

[layer3_ffn_down_weight] First 3: [0.00194626 -0.00461579 -0.00337072 ]
[layer3_ffn_down_weight] Last 3: [-0.00399397 -0.000386345 0.00192588 ]
--------------------
[layer3_ffn_down] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer3_ffn_down] First 3: [0.00988716 -0.00163213 -0.0028101 ]
[layer3_ffn_down] Last 3: [-0.00541948 0.00950986 0.000224711 ]
--------------------
[layer3post_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer3post_ffn_norm] Token 0 First 3: [15.6907 -0.194233 -0.166269 ]
[layer3post_ffn_norm] Token 0 Last 3: [-3.6309 -0.000911695 -0.055251 ]
[layer3post_ffn_norm] Token 1 First 3: [16.4132 0.227653 0.3969 ]
[layer3post_ffn_norm] Token 1 Last 3: [-2.27319 0.105831 -2.27205 ]
[layer3post_ffn_norm] Token 2 First 3: [0.796989 -0.184041 0.203222 ]
[layer3post_ffn_norm] Token 2 Last 3: [-1.10418 1.20207 0.0309371 ]
--------------------
[layer4_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer4_attention_norm] Token 0 First 3: [4.03526 -2.3739 0.26382 ]
[layer4_attention_norm] Token 0 Last 3: [-2.09016 0.659276 0.237719 ]
[layer4_attention_norm] Token 1 First 3: [9.34886 -7.6789 -3.19706 ]
[layer4_attention_norm] Token 1 Last 3: [-15.2222 -1.3082 -7.91715 ]
[layer4_attention_norm] Token 2 First 3: [2.42698 -0.238701 -1.33371 ]
[layer4_attention_norm] Token 2 Last 3: [-2.04961 2.1284 -0.0231055 ]
--------------------
[layer4_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer4_attention_norm input] Token 0 First 3: [33.5754 -2.53741 0.221397 ]
[layer4_attention_norm input] Token 0 Last 3: [-3.7202 0.866078 0.245142 ]
[layer4_attention_norm input] Token 1 First 3: [28.4389 -3.00077 -0.980886 ]
[layer4_attention_norm input] Token 1 Last 3: [-9.90532 -0.628301 -2.98489 ]
[layer4_attention_norm input] Token 2 First 3: [30.949 -0.391033 -1.71535 ]
[layer4_attention_norm input] Token 2 Last 3: [-5.591 4.28523 -0.0365175 ]
--------------------
[layer4_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer4_attention_norm_weight] First 3: [4.5012 35.0388 44.6287 ]
[layer4_attention_norm_weight] Last 3: [21.0422 28.5094 36.3181 ]
--------------------
[layer4_wv input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer4_wv input] First 3: [4.03526 -2.3739 0.26382 ]
[layer4_wv input] Last 3: [-2.04961 2.1284 -0.0231055 ]
--------------------
[layer4_wv_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer4_wv_weight] First 3: [-0.00628117 -0.000903098 -0.00225539 ]
[layer4_wv_weight] Last 3: [0.00414543 0.0152911 -0.000279226 ]
--------------------
[layer4_wv] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer4_wv] First 3: [0.35554 0.0743715 -0.268908 ]
[layer4_wv] Last 3: [-0.0774472 0.233383 -0.162845 ]
--------------------
[layer4_wk input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer4_wk input] First 3: [4.03526 -2.3739 0.26382 ]
[layer4_wk input] Last 3: [-2.04961 2.1284 -0.0231055 ]
--------------------
[layer4_wk_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer4_wk_weight] First 3: [0.00164668 -0.00545619 0.0109468 ]
[layer4_wk_weight] Last 3: [0.0078556 -0.00755361 0.00768446 ]
--------------------
[layer4_wk] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer4_wk] First 3: [0.174722 0.0383404 -0.501972 ]
[layer4_wk] Last 3: [0.73473 0.0225098 0.689875 ]
--------------------
[layer4_k_norm] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer4_k_norm] Token 0 First 3: [0.173707 0.0557076 -0.844731 ]
[layer4_k_norm] Token 0 Last 3: [0.566841 -0.356887 0.811845 ]
[layer4_k_norm] Token 1 First 3: [1.66377 2.96819 1.54971 ]
[layer4_k_norm] Token 1 Last 3: [0.513952 0.717711 0.411598 ]
[layer4_k_norm] Token 2 First 3: [0.030753 -0.0534787 -0.109068 ]
[layer4_k_norm] Token 2 Last 3: [0.6401 0.0322551 1.03948 ]
--------------------
[layer4_wq input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer4_wq input] First 3: [4.03526 -2.3739 0.26382 ]
[layer4_wq input] Last 3: [-2.04961 2.1284 -0.0231055 ]
--------------------
[layer4_wq_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer4_wq_weight] First 3: [0.0159236 -0.0166286 -0.0266677 ]
[layer4_wq_weight] Last 3: [-0.0136584 -0.00613161 0.00130188 ]
--------------------
[layer4_wq] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer4_wq] First 3: [-1.30032 0.225907 0.186313 ]
[layer4_wq] Last 3: [-0.176933 -0.19172 -0.0399614 ]
--------------------
[layer4_q_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer4_q_norm] Token 0 First 3: [-4.08212 0.652714 0.545279 ]
[layer4_q_norm] Token 0 Last 3: [-0.183363 0.570894 -1.17153 ]
[layer4_q_norm] Token 1 First 3: [-3.36027 -0.339923 -0.0909859 ]
[layer4_q_norm] Token 1 Last 3: [0.0983937 1.00224 0.501609 ]
[layer4_q_norm] Token 2 First 3: [-0.857496 1.46705 1.40407 ]
[layer4_q_norm] Token 2 Last 3: [-0.689943 -0.553049 -0.105136 ]
--------------------
[layer4_attention_query_rope] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer4_attention_query_rope] Token 0 First 3: [-4.08212 0.652714 0.545279 ]
[layer4_attention_query_rope] Token 0 Last 3: [-0.183363 0.570894 -1.17153 ]
[layer4_attention_query_rope] Token 1 First 3: [4.70378 3.24438 2.12848 ]
[layer4_attention_query_rope] Token 1 Last 3: [0.0984047 1.00234 0.501665 ]
[layer4_attention_query_rope] Token 2 First 3: [0.430958 -0.839989 -0.49275 ]
[layer4_attention_query_rope] Token 2 Last 3: [-0.689704 -0.552675 -0.105612 ]
--------------------
[layer4_attention_key_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer4_attention_value_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer4_attention_attn_probs] Shape: Shape: 1:1:6:3 [ FP32 : NCHW ]

[layer4_attention_attn_probs] Token 0 First 3: [1 1 1 ]
[layer4_attention_attn_probs] Token 0 Last 3: [1 1 1 ]
[layer4_attention_attn_probs] Token 1 First 3: [0.457045 0.983395 0.553467 ]
[layer4_attention_attn_probs] Token 1 Last 3: [0.457045 0.983395 0.553467 ]
[layer4_attention_attn_probs] Token 2 First 3: [0.542955 0.0166052 0.446533 ]
[layer4_attention_attn_probs] Token 2 Last 3: [0.542955 0.0166052 0.446533 ]
[layer4_attention_attn_probs] Token 3 First 3: [0.111786 0.255095 0.228173 ]
[layer4_attention_attn_probs] Token 3 Last 3: [0.111786 0.255095 0.228173 ]
[layer4_attention_attn_probs] Token 4 First 3: [0.00241417 0.00287685 0.000237713 ]
[layer4_attention_attn_probs] Token 4 Last 3: [0.00241417 0.00287685 0.000237713 ]
[layer4_attention_attn_probs] Token 5 First 3: [0.8858 0.742028 0.77159 ]
[layer4_attention_attn_probs] Token 5 Last 3: [0.8858 0.742028 0.77159 ]
--------------------
[layer4_attention_attention_out_core] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer4_attention_attention_out_core] Token 0 First 3: [0.355469 0.0744019 -0.268799 ]
[layer4_attention_attention_out_core] Token 0 Last 3: [-0.118896 -0.301514 0.145264 ]
[layer4_attention_attention_out_core] Token 1 First 3: [-1.22249 -0.503382 1.78279 ]
[layer4_attention_attention_out_core] Token 1 Last 3: [-0.144188 -0.0258099 -0.124662 ]
[layer4_attention_attention_out_core] Token 2 First 3: [0.452906 -0.0747373 -0.148087 ]
[layer4_attention_attention_out_core] Token 2 Last 3: [-0.0869331 0.111366 -0.092611 ]
--------------------
[layer4_attention_out input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer4_attention_out input] First 3: [0.355469 0.0744019 -0.268799 ]
[layer4_attention_out input] Last 3: [-0.0869331 0.111366 -0.092611 ]
--------------------
[layer4_attention_out_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer4_attention_out_weight] First 3: [0.00493999 -0.00394484 0.00969866 ]
[layer4_attention_out_weight] Last 3: [-0.0099477 -0.0119646 0.011855 ]
--------------------
[layer4_attention_out] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer4_attention_out] First 3: [0.0701361 -0.0227842 0.035656 ]
[layer4_attention_out] Last 3: [-0.204451 -0.168891 -0.108884 ]
--------------------
[layer4_post_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer4_post_attention_norm] Token 0 First 3: [2.81184 -0.116252 0.0302645 ]
[layer4_post_attention_norm] Token 0 Last 3: [0.00801113 0.0291095 0.112404 ]
[layer4_post_attention_norm] Token 1 First 3: [-0.348499 -0.0401771 -0.000504429 ]
[layer4_post_attention_norm] Token 1 Last 3: [-1.92755 -0.81553 1.01853 ]
[layer4_post_attention_norm] Token 2 First 3: [8.39022 -0.502662 -0.00259881 ]
[layer4_post_attention_norm] Token 2 Last 3: [-2.33788 -0.947838 -1.25888 ]
--------------------
[layer4_post_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer4_post_attention_norm input] Token 0 First 3: [0.0701361 -0.0227842 0.035656 ]
[layer4_post_attention_norm input] Token 0 Last 3: [0.000625193 0.00462872 0.00867593 ]
[layer4_post_attention_norm input] Token 1 First 3: [-0.0100667 -0.00911897 -0.000688231 ]
[layer4_post_attention_norm input] Token 1 Last 3: [-0.174205 -0.150176 0.0910426 ]
[layer4_post_attention_norm input] Token 2 First 3: [0.234514 -0.110396 -0.00343099 ]
[layer4_post_attention_norm input] Token 2 Last 3: [-0.204451 -0.168891 -0.108884 ]
--------------------
[layer4_post_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer4_post_attention_norm_weight] First 3: [8.19985 1.04358 0.173603 ]
[layer4_post_attention_norm_weight] Last 3: [2.62082 1.28627 2.64985 ]
--------------------
[layer4pre_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer4pre_ffn_norm] Token 0 First 3: [1.93523 -1.97031 0.186471 ]
[layer4pre_ffn_norm] Token 0 Last 3: [-1.45306 0.543796 0.191735 ]
[layer4pre_ffn_norm] Token 1 First 3: [3.50712 -5.3004 -1.70705 ]
[layer4pre_ffn_norm] Token 1 Last 3: [-10.8731 -2.05896 -2.47538 ]
[layer4pre_ffn_norm] Token 2 First 3: [1.33133 -0.422237 -0.809995 ]
[layer4pre_ffn_norm] Token 2 Last 3: [-1.97488 1.29005 -0.442026 ]
--------------------
[layer4_ffn_up input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer4_ffn_up input] First 3: [1.93523 -1.97031 0.186471 ]
[layer4_ffn_up input] Last 3: [-1.97488 1.29005 -0.442026 ]
--------------------
[layer4_ffn_up_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer4_ffn_up_weight] First 3: [0.0112715 -0.00104578 0.00426211 ]
[layer4_ffn_up_weight] Last 3: [0.00338334 0.00405563 0.0116214 ]
--------------------
[layer4_ffn_up] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer4_ffn_up] First 3: [-0.0748197 0.208235 -0.1834 ]
[layer4_ffn_up] Last 3: [-0.285568 0.216558 0.156426 ]
--------------------
[layer4_ffn_gate input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer4_ffn_gate input] First 3: [1.93523 -1.97031 0.186471 ]
[layer4_ffn_gate input] Last 3: [-1.97488 1.29005 -0.442026 ]
--------------------
[layer4_ffn_gate_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer4_ffn_gate_weight] First 3: [0.00696178 -0.00517407 0.0036724 ]
[layer4_ffn_gate_weight] Last 3: [0.00296205 -0.0117492 -0.00120754 ]
--------------------
[layer4_ffn_gate] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer4_ffn_gate] First 3: [-0.243303 -0.21996 -0.0252647 ]
[layer4_ffn_gate] Last 3: [0.596924 0.189348 -0.146706 ]
--------------------
[layer4_ffn_down input] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer4_ffn_down input] First 3: [0.00735236 -0.0189147 0.00227008 ]
[layer4_ffn_down input] Last 3: [-0.123529 0.0235814 -0.0101361 ]
--------------------
[layer4_ffn_down_weight] Shape: Shape: 1:1:1152:768 [ FP32 : NCHW ]

[layer4_ffn_down_weight] First 3: [0.00315967 0.00559821 0.00437504 ]
[layer4_ffn_down_weight] Last 3: [-0.00293801 -0.00150149 -0.00222707 ]
--------------------
[layer4_ffn_down] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer4_ffn_down] First 3: [0.0165064 0.0127301 0.00443648 ]
[layer4_ffn_down] Last 3: [-0.000655325 0.00170316 -0.000874045 ]
--------------------
[layer4post_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer4post_ffn_norm] Token 0 First 3: [13.6176 1.29544 0.134809 ]
[layer4post_ffn_norm] Token 0 Last 3: [0.872135 0.505199 0.36572 ]
[layer4post_ffn_norm] Token 1 First 3: [1.38519 1.89834 -0.076065 ]
[layer4post_ffn_norm] Token 1 Last 3: [-2.21131 -1.8646 -2.78551 ]
[layer4post_ffn_norm] Token 2 First 3: [17.4742 -0.468287 0.160967 ]
[layer4post_ffn_norm] Token 2 Last 3: [-0.29935 0.574863 -0.329891 ]
--------------------
[layer5_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer5_attention_norm] Token 0 First 3: [7.12504 -1.24024 0.994759 ]
[layer5_attention_norm] Token 0 Last 3: [-1.46161 0.910927 0.51596 ]
[layer5_attention_norm] Token 1 First 3: [4.44922 -1.1053 -2.88344 ]
[layer5_attention_norm] Token 1 Last 3: [-7.65682 -2.27984 -3.59111 ]
[layer5_attention_norm] Token 2 First 3: [1.74477 -0.268053 -0.863773 ]
[layer5_attention_norm] Token 2 Last 3: [-0.912692 0.548498 -0.249897 ]
--------------------
[layer5_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer5_attention_norm input] Token 0 First 3: [50.0049 -1.35822 0.38647 ]
[layer5_attention_norm input] Token 0 Last 3: [-2.84005 1.40039 0.723266 ]
[layer5_attention_norm input] Token 1 First 3: [29.4756 -1.14261 -1.05746 ]
[layer5_attention_norm input] Token 1 Last 3: [-14.0442 -3.30843 -4.75187 ]
[layer5_attention_norm input] Token 2 First 3: [56.8135 -1.36198 -1.55699 ]
[layer5_attention_norm input] Token 2 Last 3: [-8.22823 3.91225 -1.62529 ]
--------------------
[layer5_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer5_attention_norm_weight] First 3: [2.27427 14.5748 41.0837 ]
[layer5_attention_norm_weight] Last 3: [8.21434 10.3825 11.3864 ]
--------------------
[layer5_wv input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer5_wv input] First 3: [7.12504 -1.24024 0.994759 ]
[layer5_wv input] Last 3: [-0.912692 0.548498 -0.249897 ]
--------------------
[layer5_wv_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer5_wv_weight] First 3: [0.00647135 -0.00163155 -0.00294039 ]
[layer5_wv_weight] Last 3: [0.0115102 0.000855833 -0.0019759 ]
--------------------
[layer5_wv] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer5_wv] First 3: [-0.18729 -0.664273 -0.663033 ]
[layer5_wv] Last 3: [0.0743525 0.031495 -0.187117 ]
--------------------
[layer5_wk input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer5_wk input] First 3: [7.12504 -1.24024 0.994759 ]
[layer5_wk input] Last 3: [-0.912692 0.548498 -0.249897 ]
--------------------
[layer5_wk_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer5_wk_weight] First 3: [0.00180028 0.00320952 -0.000216853 ]
[layer5_wk_weight] Last 3: [-0.00382065 0.00201735 0.00854593 ]
--------------------
[layer5_wk] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer5_wk] First 3: [-0.328561 0.425355 0.501259 ]
[layer5_wk] Last 3: [-0.022441 -0.712513 0.482298 ]
--------------------
[layer5_k_norm] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer5_k_norm] Token 0 First 3: [-0.526515 0.487174 1.42196 ]
[layer5_k_norm] Token 0 Last 3: [-2.19437 -6.74645 3.32282 ]
[layer5_k_norm] Token 1 First 3: [-1.90521 0.564396 3.39769 ]
[layer5_k_norm] Token 1 Last 3: [-2.54086 -8.97261 -0.364387 ]
[layer5_k_norm] Token 2 First 3: [-0.0648673 0.0348245 0.0999765 ]
[layer5_k_norm] Token 2 Last 3: [-0.208126 -5.12075 3.32787 ]
--------------------
[layer5_wq input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer5_wq input] First 3: [7.12504 -1.24024 0.994759 ]
[layer5_wq input] Last 3: [-0.912692 0.548498 -0.249897 ]
--------------------
[layer5_wq_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer5_wq_weight] First 3: [-0.00045388 -0.000474587 0.000249782 ]
[layer5_wq_weight] Last 3: [-0.000114993 0.0107914 0.00551624 ]
--------------------
[layer5_wq] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer5_wq] First 3: [0.343607 -0.82391 -0.585085 ]
[layer5_wq] Last 3: [0.145968 -0.180923 0.0748975 ]
--------------------
[layer5_q_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer5_q_norm] Token 0 First 3: [0.749199 -1.09891 -0.580017 ]
[layer5_q_norm] Token 0 Last 3: [-0.412094 0.106604 0.367614 ]
[layer5_q_norm] Token 1 First 3: [-0.0971719 -0.759837 -0.279657 ]
[layer5_q_norm] Token 1 Last 3: [-0.339041 -0.448483 0.28172 ]
[layer5_q_norm] Token 2 First 3: [2.23277 -0.661451 -1.48871 ]
[layer5_q_norm] Token 2 Last 3: [0.289593 -0.461351 0.210498 ]
--------------------
[layer5_attention_query_rope] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer5_attention_query_rope] Token 0 First 3: [0.749199 -1.09891 -0.580017 ]
[layer5_attention_query_rope] Token 0 Last 3: [-0.412094 0.106604 0.367614 ]
[layer5_attention_query_rope] Token 1 First 3: [1.18556 -0.425961 -0.686494 ]
[layer5_attention_query_rope] Token 1 Last 3: [-0.339087 -0.448498 0.281697 ]
[layer5_attention_query_rope] Token 2 First 3: [1.37018 1.71083 -0.518566 ]
[layer5_attention_query_rope] Token 2 Last 3: [0.28936 -0.461504 0.210605 ]
--------------------
[layer5_attention_key_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer5_attention_value_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer5_attention_attn_probs] Shape: Shape: 1:1:6:3 [ FP32 : NCHW ]

[layer5_attention_attn_probs] Token 0 First 3: [1 1 1 ]
[layer5_attention_attn_probs] Token 0 Last 3: [1 1 1 ]
[layer5_attention_attn_probs] Token 1 First 3: [0.531753 0.885346 0.920429 ]
[layer5_attention_attn_probs] Token 1 Last 3: [0.531753 0.885346 0.920429 ]
[layer5_attention_attn_probs] Token 2 First 3: [0.468247 0.114654 0.0795711 ]
[layer5_attention_attn_probs] Token 2 Last 3: [0.468247 0.114654 0.0795711 ]
[layer5_attention_attn_probs] Token 3 First 3: [0.00166885 0.112949 0.00398054 ]
[layer5_attention_attn_probs] Token 3 Last 3: [0.00166885 0.112949 0.00398054 ]
[layer5_attention_attn_probs] Token 4 First 3: [6.79936e-09 3.61257e-05 0.000328798 ]
[layer5_attention_attn_probs] Token 4 Last 3: [6.79936e-09 3.61257e-05 0.000328798 ]
[layer5_attention_attn_probs] Token 5 First 3: [0.998331 0.887015 0.995691 ]
[layer5_attention_attn_probs] Token 5 Last 3: [0.998331 0.887015 0.995691 ]
--------------------
[layer5_attention_attention_out_core] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer5_attention_attention_out_core] Token 0 First 3: [-0.187256 -0.664062 -0.663086 ]
[layer5_attention_attention_out_core] Token 0 Last 3: [-0.621094 0.00471115 -0.169922 ]
[layer5_attention_attention_out_core] Token 1 First 3: [-0.481168 -0.564377 -0.246911 ]
[layer5_attention_attention_out_core] Token 1 Last 3: [-0.569579 0.0157688 -0.112847 ]
[layer5_attention_attention_out_core] Token 2 First 3: [-0.0959778 -0.137233 0.0569324 ]
[layer5_attention_attention_out_core] Token 2 Last 3: [0.0715568 0.0314244 -0.186824 ]
--------------------
[layer5_attention_out input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer5_attention_out input] First 3: [-0.187256 -0.664062 -0.663086 ]
[layer5_attention_out input] Last 3: [0.0715568 0.0314244 -0.186824 ]
--------------------
[layer5_attention_out_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer5_attention_out_weight] First 3: [0.00487415 -0.000488262 -0.00405926 ]
[layer5_attention_out_weight] Last 3: [0.00280357 -0.000386611 -0.00169234 ]
--------------------
[layer5_attention_out] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer5_attention_out] First 3: [0.32979 -0.026211 -0.0200033 ]
[layer5_attention_out] Last 3: [-0.0857229 -0.00930487 0.0582657 ]
--------------------
[layer5_post_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer5_post_attention_norm] Token 0 First 3: [13.3734 -0.14529 -0.00988136 ]
[layer5_post_attention_norm] Token 0 Last 3: [1.35536 0.637115 -0.108736 ]
[layer5_post_attention_norm] Token 1 First 3: [12.9346 -0.711106 -0.0284172 ]
[layer5_post_attention_norm] Token 1 Last 3: [0.330946 0.734255 0.981828 ]
[layer5_post_attention_norm] Token 2 First 3: [25.0944 0.356778 -0.0183025 ]
[layer5_post_attention_norm] Token 2 Last 3: [-4.07457 -0.116215 1.8427 ]
--------------------
[layer5_post_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer5_post_attention_norm input] Token 0 First 3: [0.32979 -0.026211 -0.0200033 ]
[layer5_post_attention_norm input] Token 0 Last 3: [0.0693622 0.124085 -0.00836344 ]
[layer5_post_attention_norm input] Token 1 First 3: [0.312921 -0.125854 -0.0564353 ]
[layer5_post_attention_norm input] Token 1 Last 3: [0.0166154 0.140293 0.0740854 ]
[layer5_post_attention_norm input] Token 2 First 3: [0.254401 0.0264602 -0.0152314 ]
[layer5_post_attention_norm input] Token 2 Last 3: [-0.0857229 -0.00930487 0.0582657 ]
--------------------
[layer5_post_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer5_post_attention_norm_weight] First 3: [10.0234 1.37014 0.122104 ]
[layer5_post_attention_norm_weight] Last 3: [4.82997 1.26914 3.21367 ]
--------------------
[layer5pre_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer5pre_ffn_norm] Token 0 First 3: [7.34492 -2.04741 0.502334 ]
[layer5pre_ffn_norm] Token 0 Last 3: [-0.956745 2.14358 0.515393 ]
[layer5pre_ffn_norm] Token 1 First 3: [5.8504 -3.00476 -1.72414 ]
[layer5pre_ffn_norm] Token 1 Last 3: [-10.5188 -3.22366 -3.76366 ]
[layer5pre_ffn_norm] Token 2 First 3: [2.5752 -0.371358 -0.570065 ]
[layer5pre_ffn_norm] Token 2 Last 3: [-2.15081 1.08346 0.0494686 ]
--------------------
[layer5_ffn_up input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer5_ffn_up input] First 3: [7.34492 -2.04741 0.502334 ]
[layer5_ffn_up input] Last 3: [-2.15081 1.08346 0.0494686 ]
--------------------
[layer5_ffn_up_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer5_ffn_up_weight] First 3: [0.00587593 -0.00491893 0.000709346 ]
[layer5_ffn_up_weight] Last 3: [-0.00660667 0.00964832 0.00893231 ]
--------------------
[layer5_ffn_up] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer5_ffn_up] First 3: [-0.224798 -0.0414224 -0.188577 ]
[layer5_ffn_up] Last 3: [-0.626581 0.0656562 0.194685 ]
--------------------
[layer5_ffn_gate input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer5_ffn_gate input] First 3: [7.34492 -2.04741 0.502334 ]
[layer5_ffn_gate input] Last 3: [-2.15081 1.08346 0.0494686 ]
--------------------
[layer5_ffn_gate_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer5_ffn_gate_weight] First 3: [0.00332621 0.00522365 0.00929155 ]
[layer5_ffn_gate_weight] Last 3: [0.00379308 0.0107659 0.00752407 ]
--------------------
[layer5_ffn_gate] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer5_ffn_gate] First 3: [0.0288309 -0.633197 -0.265088 ]
[layer5_ffn_gate] Last 3: [-0.936254 0.0275612 -0.306692 ]
--------------------
[layer5_ffn_down input] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer5_ffn_down input] First 3: [-0.00331509 0.00690768 0.0197696 ]
[layer5_ffn_down input] Last 3: [0.102492 0.000924675 -0.0226622 ]
--------------------
[layer5_ffn_down_weight] Shape: Shape: 1:1:1152:768 [ FP32 : NCHW ]

[layer5_ffn_down_weight] First 3: [0.00968688 -0.00293178 -0.000363675 ]
[layer5_ffn_down_weight] Last 3: [-0.00348301 -0.00719522 0.011045 ]
--------------------
[layer5_ffn_down] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer5_ffn_down] First 3: [-0.000964288 0.00301007 -0.0182817 ]
[layer5_ffn_down] Last 3: [-0.00238112 -0.0018029 -0.00514552 ]
--------------------
[layer5post_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer5post_ffn_norm] Token 0 First 3: [-0.350463 0.174642 -0.232913 ]
[layer5post_ffn_norm] Token 0 Last 3: [-3.80008 -0.774583 -0.117204 ]
[layer5post_ffn_norm] Token 1 First 3: [-6.31089 -0.306116 -0.028318 ]
[layer5post_ffn_norm] Token 1 Last 3: [-3.5284 -0.649722 -3.62326 ]
[layer5post_ffn_norm] Token 2 First 3: [-15.8107 -0.659103 -0.308306 ]
[layer5post_ffn_norm] Token 2 Last 3: [-1.3739 -0.597962 -2.50354 ]
--------------------
[layer6_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer6_attention_norm] Token 0 First 3: [9.46121 -1.86461 0.207904 ]
[layer6_attention_norm] Token 0 Last 3: [-2.78179 1.35817 0.391352 ]
[layer6_attention_norm] Token 1 First 3: [7.02032 -3.92619 -2.08873 ]
[layer6_attention_norm] Token 1 Last 3: [-11.7576 -4.49163 -7.53718 ]
[layer6_attention_norm] Token 2 First 3: [2.3431 -0.551487 -0.643665 ]
[layer6_attention_norm] Token 2 Last 3: [-1.70009 0.812198 -0.424834 ]
--------------------
[layer6_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer6_attention_norm input] Token 0 First 3: [63.0278 -1.32886 0.143675 ]
[layer6_attention_norm input] Token 0 Last 3: [-5.28477 1.26292 0.497326 ]
[layer6_attention_norm input] Token 1 First 3: [36.0993 -2.15983 -1.11419 ]
[layer6_attention_norm input] Token 1 Last 3: [-17.2416 -3.2239 -7.3933 ]
[layer6_attention_norm input] Token 2 First 3: [66.0971 -1.66431 -1.88359 ]
[layer6_attention_norm input] Token 2 Last 3: [-13.6767 3.19808 -2.28612 ]
--------------------
[layer6_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer6_attention_norm_weight] First 3: [3.44586 32.21 33.2171 ]
[layer6_attention_norm_weight] Last 3: [12.0832 24.6867 18.0638 ]
--------------------
[layer6_wv input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer6_wv input] First 3: [9.46121 -1.86461 0.207904 ]
[layer6_wv input] Last 3: [-1.70009 0.812198 -0.424834 ]
--------------------
[layer6_wv_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer6_wv_weight] First 3: [-0.00353955 -0.00133898 -0.00403017 ]
[layer6_wv_weight] Last 3: [0.00144972 0.00838298 0.00647674 ]
--------------------
[layer6_wv] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer6_wv] First 3: [0.191963 -0.204402 -0.850598 ]
[layer6_wv] Last 3: [0.17597 0.297786 -0.39767 ]
--------------------
[layer6_wk input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer6_wk input] First 3: [9.46121 -1.86461 0.207904 ]
[layer6_wk input] Last 3: [-1.70009 0.812198 -0.424834 ]
--------------------
[layer6_wk_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer6_wk_weight] First 3: [0.00425267 0.0380479 -0.000993985 ]
[layer6_wk_weight] Last 3: [-0.00269153 -0.00222733 -0.00787629 ]
--------------------
[layer6_wk] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer6_wk] First 3: [-0.303483 0.601994 0.320913 ]
[layer6_wk] Last 3: [-1.38308 -0.753451 0.672609 ]
--------------------
[layer6_k_norm] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer6_k_norm] Token 0 First 3: [-0.258339 0.464084 0.366632 ]
[layer6_k_norm] Token 0 Last 3: [-5.0324 -2.93371 3.20764 ]
[layer6_k_norm] Token 1 First 3: [-0.870018 0.368326 -0.653383 ]
[layer6_k_norm] Token 1 Last 3: [-1.48451 -0.929666 2.79185 ]
[layer6_k_norm] Token 2 First 3: [-0.0741061 0.217185 0.27591 ]
[layer6_k_norm] Token 2 Last 3: [-5.86717 -2.41298 3.19415 ]
--------------------
[layer6_wq input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer6_wq input] First 3: [9.46121 -1.86461 0.207904 ]
[layer6_wq input] Last 3: [-1.70009 0.812198 -0.424834 ]
--------------------
[layer6_wq_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer6_wq_weight] First 3: [0.00935921 -0.019258 0.0138344 ]
[layer6_wq_weight] Last 3: [-0.00308292 -0.00617976 0.00896006 ]
--------------------
[layer6_wq] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer6_wq] First 3: [0.143298 0.296871 -0.174475 ]
[layer6_wq] Last 3: [-0.245073 -0.51377 0.176757 ]
--------------------
[layer6_q_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer6_q_norm] Token 0 First 3: [0.18 0.330519 -0.191528 ]
[layer6_q_norm] Token 0 Last 3: [-0.307633 -0.356978 0.0860548 ]
[layer6_q_norm] Token 1 First 3: [0.487466 1.19763 0.729 ]
[layer6_q_norm] Token 1 Last 3: [0.733239 -0.60502 0.536795 ]
[layer6_q_norm] Token 2 First 3: [-1.14261 0.274161 -0.795204 ]
[layer6_q_norm] Token 2 Last 3: [-0.322281 -0.967989 0.217262 ]
--------------------
[layer6_attention_query_rope] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer6_attention_query_rope] Token 0 First 3: [0.18 0.330519 -0.191528 ]
[layer6_attention_query_rope] Token 0 Last 3: [-0.307633 -0.356978 0.0860548 ]
[layer6_attention_query_rope] Token 1 First 3: [0.104654 1.63161 1.45465 ]
[layer6_attention_query_rope] Token 1 Last 3: [0.733287 -0.605197 0.536853 ]
[layer6_attention_query_rope] Token 2 First 3: [1.02789 0.787716 0.321496 ]
[layer6_attention_query_rope] Token 2 Last 3: [-0.322347 -0.968497 0.217228 ]
--------------------
[layer6_attention_key_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer6_attention_value_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer6_attention_attn_probs] Shape: Shape: 1:1:6:3 [ FP32 : NCHW ]

[layer6_attention_attn_probs] Token 0 First 3: [1 1 1 ]
[layer6_attention_attn_probs] Token 0 Last 3: [1 1 1 ]
[layer6_attention_attn_probs] Token 1 First 3: [0.672298 0.433913 0.648463 ]
[layer6_attention_attn_probs] Token 1 Last 3: [0.672298 0.433913 0.648463 ]
[layer6_attention_attn_probs] Token 2 First 3: [0.327702 0.566087 0.351537 ]
[layer6_attention_attn_probs] Token 2 Last 3: [0.327702 0.566087 0.351537 ]
[layer6_attention_attn_probs] Token 3 First 3: [0.207682 0.0286967 0.189862 ]
[layer6_attention_attn_probs] Token 3 Last 3: [0.207682 0.0286967 0.189862 ]
[layer6_attention_attn_probs] Token 4 First 3: [0.098544 0.00151376 0.0514655 ]
[layer6_attention_attn_probs] Token 4 Last 3: [0.098544 0.00151376 0.0514655 ]
[layer6_attention_attn_probs] Token 5 First 3: [0.693774 0.96979 0.758673 ]
[layer6_attention_attn_probs] Token 5 Last 3: [0.693774 0.96979 0.758673 ]
--------------------
[layer6_attention_attention_out_core] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer6_attention_attention_out_core] Token 0 First 3: [0.192017 -0.204346 -0.850586 ]
[layer6_attention_attention_out_core] Token 0 Last 3: [0.706055 0.549805 0.279541 ]
[layer6_attention_attention_out_core] Token 1 First 3: [0.357588 -0.304432 -0.440639 ]
[layer6_attention_attention_out_core] Token 1 Last 3: [0.667777 0.0278197 0.440977 ]
[layer6_attention_attention_out_core] Token 2 First 3: [0.218432 -0.176812 -0.252118 ]
[layer6_attention_attention_out_core] Token 2 Last 3: [0.298332 0.282236 -0.210633 ]
--------------------
[layer6_attention_out input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer6_attention_out input] First 3: [0.192017 -0.204346 -0.850586 ]
[layer6_attention_out input] Last 3: [0.298332 0.282236 -0.210633 ]
--------------------
[layer6_attention_out_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer6_attention_out_weight] First 3: [0.00716766 0.00804153 -0.00512167 ]
[layer6_attention_out_weight] Last 3: [-0.00623235 -0.0106056 -0.00195094 ]
--------------------
[layer6_attention_out] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer6_attention_out] First 3: [0.0502962 0.0315944 -0.100838 ]
[layer6_attention_out] Last 3: [-0.0048918 -0.11784 -0.0424002 ]
--------------------
[layer6_post_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer6_post_attention_norm] Token 0 First 3: [1.10961 0.3345 -0.0384885 ]
[layer6_post_attention_norm] Token 0 Last 3: [1.29402 0.043847 -1.40893 ]
[layer6_post_attention_norm] Token 1 First 3: [-0.378881 0.42617 -0.0669018 ]
[layer6_post_attention_norm] Token 1 Last 3: [2.30963 -2.11082 -2.56648 ]
[layer6_post_attention_norm] Token 2 First 3: [1.16225 0.251111 -0.049853 ]
[layer6_post_attention_norm] Token 2 Last 3: [-0.12607 -2.96028 -1.63796 ]
--------------------
[layer6_post_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer6_post_attention_norm input] Token 0 First 3: [0.0502962 0.0315944 -0.100838 ]
[layer6_post_attention_norm input] Token 0 Last 3: [0.0973966 0.00338567 -0.0707458 ]
[layer6_post_attention_norm input] Token 1 First 3: [-0.0172168 0.0403533 -0.175717 ]
[layer6_post_attention_norm input] Token 1 Last 3: [0.174273 -0.163395 -0.129191 ]
[layer6_post_attention_norm input] Token 2 First 3: [0.0271594 0.0122274 -0.0673346 ]
[layer6_post_attention_norm input] Token 2 Last 3: [-0.0048918 -0.11784 -0.0424002 ]
--------------------
[layer6_post_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer6_post_attention_norm_weight] First 3: [5.34219 2.56372 0.0924253 ]
[layer6_post_attention_norm_weight] Last 3: [3.21722 3.13602 4.82252 ]
--------------------
[layer6pre_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer6pre_ffn_norm] Token 0 First 3: [6.33558 -1.21868 0.191064 ]
[layer6pre_ffn_norm] Token 0 Last 3: [-1.7571 1.07253 -0.51364 ]
[layer6pre_ffn_norm] Token 1 First 3: [3.80722 -2.29258 -2.31481 ]
[layer6pre_ffn_norm] Token 1 Last 3: [-7.09377 -4.72432 -6.05505 ]
[layer6pre_ffn_norm] Token 2 First 3: [1.37375 -0.358117 -0.72615 ]
[layer6pre_ffn_norm] Token 2 Last 3: [-1.25657 0.040355 -0.457159 ]
--------------------
[layer6_ffn_up input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer6_ffn_up input] First 3: [6.33558 -1.21868 0.191064 ]
[layer6_ffn_up input] Last 3: [-1.25657 0.040355 -0.457159 ]
--------------------
[layer6_ffn_up_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer6_ffn_up_weight] First 3: [0.0034226 0.00345691 0.00122854 ]
[layer6_ffn_up_weight] Last 3: [0.0138304 0.0115409 -0.000713927 ]
--------------------
[layer6_ffn_up] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer6_ffn_up] First 3: [0.0302972 -0.300892 0.221391 ]
[layer6_ffn_up] Last 3: [0.056741 0.19181 0.143125 ]
--------------------
[layer6_ffn_gate input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer6_ffn_gate input] First 3: [6.33558 -1.21868 0.191064 ]
[layer6_ffn_gate input] Last 3: [-1.25657 0.040355 -0.457159 ]
--------------------
[layer6_ffn_gate_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer6_ffn_gate_weight] First 3: [0.00518712 0.0172753 0.00766082 ]
[layer6_ffn_gate_weight] Last 3: [0.00499255 0.01516 0.000399711 ]
--------------------
[layer6_ffn_gate] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer6_ffn_gate] First 3: [-0.543966 -0.687828 -0.88878 ]
[layer6_ffn_gate] Last 3: [-0.0810311 -0.0999807 0.021585 ]
--------------------
[layer6_ffn_down input] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer6_ffn_down input] First 3: [-0.00483337 0.050883 -0.0368327 ]
[layer6_ffn_down input] Last 3: [-0.00215042 -0.00882502 0.00157128 ]
--------------------
[layer6_ffn_down_weight] Shape: Shape: 1:1:1152:768 [ FP32 : NCHW ]

[layer6_ffn_down_weight] First 3: [0.00493515 -0.00275053 0.00351923 ]
[layer6_ffn_down_weight] Last 3: [-0.00196799 -0.000411739 0.00382857 ]
--------------------
[layer6_ffn_down] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer6_ffn_down] First 3: [0.00634289 0.00437453 0.00126944 ]
[layer6_ffn_down] Last 3: [0.0018802 0.000486676 -0.00147554 ]
--------------------
[layer6post_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer6post_ffn_norm] Token 0 First 3: [2.41856 0.519522 0.0201822 ]
[layer6post_ffn_norm] Token 0 Last 3: [-1.27953 2.98772 -2.2154 ]
[layer6post_ffn_norm] Token 1 First 3: [3.79675 0.406029 -0.11492 ]
[layer6post_ffn_norm] Token 1 Last 3: [-4.95943 0.700284 -7.01973 ]
[layer6post_ffn_norm] Token 2 First 3: [-3.39524 -3.7871 0.121106 ]
[layer6post_ffn_norm] Token 2 Last 3: [3.14315 0.801182 -3.27413 ]
--------------------
[layer7_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer7_attention_norm] Token 0 First 3: [18.5147 -0.675044 0.332534 ]
[layer7_attention_norm] Token 0 Last 3: [-3.50561 3.66303 -2.34663 ]
[layer7_attention_norm] Token 1 First 3: [13.5068 -2.31897 -4.22368 ]
[layer7_attention_norm] Token 1 Last 3: [-16.2567 -4.85694 -15.6559 ]
[layer7_attention_norm] Token 2 First 3: [15.5134 -6.45553 -4.19766 ]
[layer7_attention_norm] Token 2 Last 3: [-6.19145 0.773852 -4.71697 ]
--------------------
[layer7_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer7_attention_norm input] Token 0 First 3: [66.5559 -0.474842 0.125369 ]
[layer7_attention_norm input] Token 0 Last 3: [-5.27029 4.29449 -3.127 ]
[layer7_attention_norm input] Token 1 First 3: [39.5171 -1.32763 -1.29601 ]
[layer7_attention_norm input] Token 1 Last 3: [-19.8914 -4.63443 -16.9795 ]
[layer7_attention_norm input] Token 2 First 3: [63.8641 -5.2003 -1.81234 ]
[layer7_attention_norm input] Token 2 Last 3: [-10.6596 1.03898 -7.19822 ]
--------------------
[layer7_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer7_attention_norm_weight] First 3: [5.93347 30.3223 56.5751 ]
[layer7_attention_norm_weight] Last 3: [14.1876 18.1932 16.0065 ]
--------------------
[layer7_wv input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer7_wv input] First 3: [18.5147 -0.675044 0.332534 ]
[layer7_wv input] Last 3: [-6.19145 0.773852 -4.71697 ]
--------------------
[layer7_wv_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer7_wv_weight] First 3: [0.00123662 0.00495114 0.00692825 ]
[layer7_wv_weight] Last 3: [0.00368901 -0.00814211 -0.000113715 ]
--------------------
[layer7_wv] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer7_wv] First 3: [0.440255 -0.501262 -0.142344 ]
[layer7_wv] Last 3: [-0.323228 -1.27656 -1.08015 ]
--------------------
[layer7_wk input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer7_wk input] First 3: [18.5147 -0.675044 0.332534 ]
[layer7_wk input] Last 3: [-6.19145 0.773852 -4.71697 ]
--------------------
[layer7_wk_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer7_wk_weight] First 3: [0.0106932 -0.01964 -0.0015153 ]
[layer7_wk_weight] Last 3: [-0.00540332 -0.00367641 -0.0100845 ]
--------------------
[layer7_wk] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer7_wk] First 3: [-3.81564 -0.705234 0.170743 ]
[layer7_wk] Last 3: [0.762528 0.146572 5.32511 ]
--------------------
[layer7_k_norm] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer7_k_norm] Token 0 First 3: [-1.71765 -0.16753 0.070326 ]
[layer7_k_norm] Token 0 Last 3: [0.42057 0.164615 2.42539 ]
[layer7_k_norm] Token 1 First 3: [-2.8173 0.221503 1.44225 ]
[layer7_k_norm] Token 1 Last 3: [1.34204 0.999578 -0.18538 ]
[layer7_k_norm] Token 2 First 3: [-1.90328 0.123691 0.130356 ]
[layer7_k_norm] Token 2 Last 3: [0.374352 0.0640492 2.54473 ]
--------------------
[layer7_wq input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer7_wq input] First 3: [18.5147 -0.675044 0.332534 ]
[layer7_wq input] Last 3: [-6.19145 0.773852 -4.71697 ]
--------------------
[layer7_wq_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer7_wq_weight] First 3: [-0.0169332 -0.00140211 -0.0217213 ]
[layer7_wq_weight] Last 3: [0.0138518 0.0041004 5.37548e-05 ]
--------------------
[layer7_wq] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer7_wq] First 3: [-4.22264 -0.51653 3.63275 ]
[layer7_wq] Last 3: [-0.405337 -0.0738461 0.349349 ]
--------------------
[layer7_q_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer7_q_norm] Token 0 First 3: [-0.84849 0.0113554 2.4266 ]
[layer7_q_norm] Token 0 Last 3: [0.618995 -0.153567 0.858021 ]
[layer7_q_norm] Token 1 First 3: [-0.430717 0.00415539 3.08401 ]
[layer7_q_norm] Token 1 Last 3: [-0.491252 -1.14152 0.679824 ]
[layer7_q_norm] Token 2 First 3: [-0.632633 0.168668 3.36606 ]
[layer7_q_norm] Token 2 Last 3: [-0.261504 -0.0577403 0.18244 ]
--------------------
[layer7_attention_query_rope] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer7_attention_query_rope] Token 0 First 3: [-0.84849 0.0113554 2.4266 ]
[layer7_attention_query_rope] Token 0 Last 3: [0.618995 -0.153567 0.858021 ]
[layer7_attention_query_rope] Token 1 First 3: [-8.36329 5.67307 2.25777 ]
[layer7_attention_query_rope] Token 1 Last 3: [-0.491419 -1.14149 0.679868 ]
[layer7_attention_query_rope] Token 2 First 3: [-2.50288 -0.692619 -1.70586 ]
[layer7_attention_query_rope] Token 2 Last 3: [-0.261742 -0.0580925 0.182581 ]
--------------------
[layer7_attention_key_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer7_attention_value_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer7_attention_attn_probs] Shape: Shape: 1:1:6:3 [ FP32 : NCHW ]

[layer7_attention_attn_probs] Token 0 First 3: [1 1 1 ]
[layer7_attention_attn_probs] Token 0 Last 3: [1 1 1 ]
[layer7_attention_attn_probs] Token 1 First 3: [0.673059 0.928694 0.738598 ]
[layer7_attention_attn_probs] Token 1 Last 3: [0.673059 0.928694 0.738598 ]
[layer7_attention_attn_probs] Token 2 First 3: [0.326941 0.0713062 0.261402 ]
[layer7_attention_attn_probs] Token 2 Last 3: [0.326941 0.0713062 0.261402 ]
[layer7_attention_attn_probs] Token 3 First 3: [0.397683 0.406267 0.597349 ]
[layer7_attention_attn_probs] Token 3 Last 3: [0.397683 0.406267 0.597349 ]
[layer7_attention_attn_probs] Token 4 First 3: [0.381824 0.0413713 0.0964603 ]
[layer7_attention_attn_probs] Token 4 Last 3: [0.381824 0.0413713 0.0964603 ]
[layer7_attention_attn_probs] Token 5 First 3: [0.220493 0.552362 0.306191 ]
[layer7_attention_attn_probs] Token 5 Last 3: [0.220493 0.552362 0.306191 ]
--------------------
[layer7_attention_attention_out_core] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer7_attention_attention_out_core] Token 0 First 3: [0.440186 -0.501465 -0.142334 ]
[layer7_attention_attention_out_core] Token 0 Last 3: [0.473145 -0.181152 -0.146851 ]
[layer7_attention_attention_out_core] Token 1 First 3: [0.600703 -0.0953429 -0.301893 ]
[layer7_attention_attention_out_core] Token 1 Last 3: [0.864609 0.0725914 -0.110341 ]
[layer7_attention_attention_out_core] Token 2 First 3: [0.0891729 -0.09941 -0.29918 ]
[layer7_attention_attention_out_core] Token 2 Last 3: [0.373753 -0.422863 -0.419124 ]
--------------------
[layer7_attention_out input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer7_attention_out input] First 3: [0.440186 -0.501465 -0.142334 ]
[layer7_attention_out input] Last 3: [0.373753 -0.422863 -0.419124 ]
--------------------
[layer7_attention_out_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer7_attention_out_weight] First 3: [0.0116012 -0.00213546 -0.00463799 ]
[layer7_attention_out_weight] Last 3: [0.0086222 -0.00452778 -0.00360781 ]
--------------------
[layer7_attention_out] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer7_attention_out] First 3: [0.441954 -0.114568 -0.762861 ]
[layer7_attention_out] Last 3: [0.569789 -0.187088 -0.100557 ]
--------------------
[layer7_post_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer7_post_attention_norm] Token 0 First 3: [7.11913 -1.96566 -0.154844 ]
[layer7_post_attention_norm] Token 0 Last 3: [2.38058 0.213956 -7.37324 ]
[layer7_post_attention_norm] Token 1 First 3: [3.93184 0.866718 -0.149666 ]
[layer7_post_attention_norm] Token 1 Last 3: [2.97305 0.00105962 -0.0266497 ]
[layer7_post_attention_norm] Token 2 First 3: [-1.91334 -0.952262 -0.18588 ]
[layer7_post_attention_norm] Token 2 Last 3: [4.83769 -1.12093 -1.43044 ]
--------------------
[layer7_post_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer7_post_attention_norm input] Token 0 First 3: [0.441954 -0.114568 -0.762861 ]
[layer7_post_attention_norm input] Token 0 Last 3: [0.207072 0.0263726 -0.382792 ]
[layer7_post_attention_norm input] Token 1 First 3: [0.250148 0.0517707 -0.755657 ]
[layer7_post_attention_norm input] Token 1 Last 3: [0.265028 0.000133853 -0.00141791 ]
[layer7_post_attention_norm input] Token 2 First 3: [-0.160835 -0.0751537 -1.24 ]
[layer7_post_attention_norm input] Token 2 Last 3: [0.569789 -0.187088 -0.100557 ]
--------------------
[layer7_post_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer7_post_attention_norm_weight] First 3: [5.75112 6.12559 0.0724688 ]
[layer7_post_attention_norm_weight] Last 3: [4.10455 2.8965 6.87698 ]
--------------------
[layer7pre_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer7pre_ffn_norm] Token 0 First 3: [5.96707 -1.35948 -0.0324763 ]
[layer7pre_ffn_norm] Token 0 Last 3: [-0.94409 2.26241 -3.49615 ]
[layer7pre_ffn_norm] Token 1 First 3: [4.06391 -0.296509 -1.83956 ]
[layer7pre_ffn_norm] Token 1 Last 3: [-6.38327 -2.68514 -6.53914 ]
[layer7pre_ffn_norm] Token 2 First 3: [4.15511 -2.83822 -1.82329 ]
[layer7pre_ffn_norm] Token 2 Last 3: [-1.57515 -0.0340552 -2.37918 ]
--------------------
[layer7_ffn_up input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer7_ffn_up input] First 3: [5.96707 -1.35948 -0.0324763 ]
[layer7_ffn_up input] Last 3: [-1.57515 -0.0340552 -2.37918 ]
--------------------
[layer7_ffn_up_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer7_ffn_up_weight] First 3: [0.00195278 -0.000531716 0.00128539 ]
[layer7_ffn_up_weight] Last 3: [0.00577862 -0.00675632 0.00142148 ]
--------------------
[layer7_ffn_up] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer7_ffn_up] First 3: [-0.24799 0.616098 0.343704 ]
[layer7_ffn_up] Last 3: [-0.179269 -0.14034 0.294422 ]
--------------------
[layer7_ffn_gate input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer7_ffn_gate input] First 3: [5.96707 -1.35948 -0.0324763 ]
[layer7_ffn_gate input] Last 3: [-1.57515 -0.0340552 -2.37918 ]
--------------------
[layer7_ffn_gate_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer7_ffn_gate_weight] First 3: [0.00201177 -0.00200406 -0.00329853 ]
[layer7_ffn_gate_weight] Last 3: [-0.00270209 -0.00142102 0.00316276 ]
--------------------
[layer7_ffn_gate] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer7_ffn_gate] First 3: [0.15251 -0.257377 0.316914 ]
[layer7_ffn_gate] Last 3: [-0.157476 -0.203049 -0.0402259 ]
--------------------
[layer7_ffn_down input] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer7_ffn_down input] First 3: [-0.0212026 -0.0631819 0.0680053 ]
[layer7_ffn_down input] Last 3: [0.012349 0.0119555 -0.00573168 ]
--------------------
[layer7_ffn_down_weight] Shape: Shape: 1:1:1152:768 [ FP32 : NCHW ]

[layer7_ffn_down_weight] First 3: [0.00492609 -0.00637701 0.00492759 ]
[layer7_ffn_down_weight] Last 3: [0.00232531 0.00352739 0.000158688 ]
--------------------
[layer7_ffn_down] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer7_ffn_down] First 3: [0.0553664 0.0184056 0.00880655 ]
[layer7_ffn_down] Last 3: [0.0163543 -0.00230634 -0.00840957 ]
--------------------
[layer7post_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer7post_ffn_norm] Token 0 First 3: [17.5954 2.71893 0.107759 ]
[layer7post_ffn_norm] Token 0 Last 3: [-8.10971 -4.21566 -13.2569 ]
[layer7post_ffn_norm] Token 1 First 3: [-4.8336 -5.8963 -0.305743 ]
[layer7post_ffn_norm] Token 1 Last 3: [-4.1847 1.6571 -4.85011 ]
[layer7post_ffn_norm] Token 2 First 3: [5.67992 0.87589 0.0878926 ]
[layer7post_ffn_norm] Token 2 Last 3: [6.56453 -1.0076 -5.54947 ]
--------------------
[layer8_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer8_attention_norm] Token 0 First 3: [6.03109 0.111116 0.0389482 ]
[layer8_attention_norm] Token 0 Last 3: [-2.16077 0.0675263 -4.34285 ]
[layer8_attention_norm] Token 1 First 3: [9.14607 -9.09369 -3.12326 ]
[layer8_attention_norm] Token 1 Last 3: [-14.8591 -2.46046 -14.3207 ]
[layer8_attention_norm] Token 2 First 3: [6.07792 -2.86398 -1.2926 ]
[layer8_attention_norm] Token 2 Last 3: [0.198396 -0.341765 -3.52488 ]
--------------------
[layer8_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer8_attention_norm input] Token 0 First 3: [91.2705 0.278428 0.0782849 ]
[layer8_attention_norm input] Token 0 Last 3: [-10.9994 0.292778 -23.7572 ]
[layer8_attention_norm input] Token 1 First 3: [38.6154 -6.35721 -1.75142 ]
[layer8_attention_norm input] Token 1 Last 3: [-21.1031 -2.97628 -21.8563 ]
[layer8_attention_norm input] Token 2 First 3: [67.6307 -5.27667 -1.91033 ]
[layer8_attention_norm input] Token 2 Last 3: [0.742589 -1.08955 -14.1781 ]
--------------------
[layer8_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer8_attention_norm_weight] First 3: [5.93026 35.8157 44.6496 ]
[layer8_attention_norm_weight] Last 3: [17.6298 20.6987 16.4055 ]
--------------------
[layer8_wv input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer8_wv input] First 3: [6.03109 0.111116 0.0389482 ]
[layer8_wv input] Last 3: [0.198396 -0.341765 -3.52488 ]
--------------------
[layer8_wv_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer8_wv_weight] First 3: [7.2041e-05 -0.00743501 -0.00850107 ]
[layer8_wv_weight] Last 3: [0.0170686 -0.00506921 0.0184916 ]
--------------------
[layer8_wv] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer8_wv] First 3: [-0.116021 -0.266858 -0.644647 ]
[layer8_wv] Last 3: [0.353368 -0.949048 0.210392 ]
--------------------
[layer8_wk input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer8_wk input] First 3: [6.03109 0.111116 0.0389482 ]
[layer8_wk input] Last 3: [0.198396 -0.341765 -3.52488 ]
--------------------
[layer8_wk_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer8_wk_weight] First 3: [-0.0226833 0.008814 0.0136395 ]
[layer8_wk_weight] Last 3: [-0.00474919 0.00939343 -0.00171755 ]
--------------------
[layer8_wk] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer8_wk] First 3: [0.213301 -1.04786 -0.0117328 ]
[layer8_wk] Last 3: [-2.36282 -1.78998 -2.06576 ]
--------------------
[layer8_k_norm] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer8_k_norm] Token 0 First 3: [0.307075 -0.489185 -0.0082611 ]
[layer8_k_norm] Token 0 Last 3: [-2.78464 -0.807169 -1.99663 ]
[layer8_k_norm] Token 1 First 3: [2.17939 -1.31495 -0.312887 ]
[layer8_k_norm] Token 1 Last 3: [-1.94855 0.0324297 -0.934571 ]
[layer8_k_norm] Token 2 First 3: [1.94419 -0.734007 0.218636 ]
[layer8_k_norm] Token 2 Last 3: [-2.17992 -0.981841 -2.27698 ]
--------------------
[layer8_wq input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer8_wq input] First 3: [6.03109 0.111116 0.0389482 ]
[layer8_wq input] Last 3: [0.198396 -0.341765 -3.52488 ]
--------------------
[layer8_wq_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer8_wq_weight] First 3: [-0.0107967 0.00651396 -0.00234769 ]
[layer8_wq_weight] Last 3: [0.00997735 -0.00948776 0.00046198 ]
--------------------
[layer8_wq] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer8_wq] First 3: [0.231404 0.999817 0.63846 ]
[layer8_wq] Last 3: [-0.986216 -0.0477734 -0.693696 ]
--------------------
[layer8_q_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer8_q_norm] Token 0 First 3: [0.602885 1.08381 2.51157 ]
[layer8_q_norm] Token 0 Last 3: [-0.580223 -0.505369 -0.816357 ]
[layer8_q_norm] Token 1 First 3: [0.442547 1.55057 2.61438 ]
[layer8_q_norm] Token 1 Last 3: [-1.11937 -1.49674 -0.181245 ]
[layer8_q_norm] Token 2 First 3: [0.803617 0.273473 4.30328 ]
[layer8_q_norm] Token 2 Last 3: [-1.25447 -0.0845554 -0.586026 ]
--------------------
[layer8_attention_query_rope] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer8_attention_query_rope] Token 0 First 3: [0.602885 1.08381 2.51157 ]
[layer8_attention_query_rope] Token 0 Last 3: [-0.580223 -0.505369 -0.816357 ]
[layer8_attention_query_rope] Token 1 First 3: [-0.444318 -0.915914 2.69216 ]
[layer8_attention_query_rope] Token 1 Last 3: [-1.11946 -1.4968 -0.181406 ]
[layer8_attention_query_rope] Token 2 First 3: [-1.70171 -0.534142 -0.962954 ]
[layer8_attention_query_rope] Token 2 Last 3: [-1.2546 -0.0842476 -0.586231 ]
--------------------
[layer8_attention_key_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer8_attention_value_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer8_attention_attn_probs] Shape: Shape: 1:1:6:3 [ FP32 : NCHW ]

[layer8_attention_attn_probs] Token 0 First 3: [1 1 1 ]
[layer8_attention_attn_probs] Token 0 Last 3: [1 1 1 ]
[layer8_attention_attn_probs] Token 1 First 3: [0.847348 0.808983 0.55501 ]
[layer8_attention_attn_probs] Token 1 Last 3: [0.847348 0.808983 0.55501 ]
[layer8_attention_attn_probs] Token 2 First 3: [0.152652 0.191017 0.44499 ]
[layer8_attention_attn_probs] Token 2 Last 3: [0.152652 0.191017 0.44499 ]
[layer8_attention_attn_probs] Token 3 First 3: [0.553752 0.870748 0.261811 ]
[layer8_attention_attn_probs] Token 3 Last 3: [0.553752 0.870748 0.261811 ]
[layer8_attention_attn_probs] Token 4 First 3: [0.0353747 0.017881 0.00195011 ]
[layer8_attention_attn_probs] Token 4 Last 3: [0.0353747 0.017881 0.00195011 ]
[layer8_attention_attn_probs] Token 5 First 3: [0.410874 0.111371 0.736239 ]
[layer8_attention_attn_probs] Token 5 Last 3: [0.410874 0.111371 0.736239 ]
--------------------
[layer8_attention_attention_out_core] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer8_attention_attention_out_core] Token 0 First 3: [-0.116028 -0.266846 -0.644531 ]
[layer8_attention_attention_out_core] Token 0 Last 3: [-0.299072 -0.51709 -0.562012 ]
[layer8_attention_attention_out_core] Token 1 First 3: [-0.34563 -0.251193 -0.329836 ]
[layer8_attention_attention_out_core] Token 1 Last 3: [-0.830865 0.499564 -0.664133 ]
[layer8_attention_attention_out_core] Token 2 First 3: [0.0322149 -0.103323 0.145417 ]
[layer8_attention_attention_out_core] Token 2 Last 3: [0.178878 -0.830785 0.00625666 ]
--------------------
[layer8_attention_out input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer8_attention_out input] First 3: [-0.116028 -0.266846 -0.644531 ]
[layer8_attention_out input] Last 3: [0.178878 -0.830785 0.00625666 ]
--------------------
[layer8_attention_out_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer8_attention_out_weight] First 3: [0.00589645 0.00586515 -0.00348742 ]
[layer8_attention_out_weight] Last 3: [-0.0016676 -0.0041287 -0.0126112 ]
--------------------
[layer8_attention_out] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer8_attention_out] First 3: [0.132642 -0.0328519 0.728735 ]
[layer8_attention_out] Last 3: [0.0651846 0.0134688 0.134706 ]
--------------------
[layer8_post_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer8_post_attention_norm] Token 0 First 3: [3.83034 -1.57252 0.0541429 ]
[layer8_post_attention_norm] Token 0 Last 3: [1.48832 2.36716 5.19094 ]
[layer8_post_attention_norm] Token 1 First 3: [5.4163 -1.48111 0.0432235 ]
[layer8_post_attention_norm] Token 1 Last 3: [3.68932 2.71479 8.26338 ]
[layer8_post_attention_norm] Token 2 First 3: [0.272558 4.66033 0.066527 ]
[layer8_post_attention_norm] Token 2 Last 3: [1.37707 0.258348 6.3465 ]
--------------------
[layer8_post_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer8_post_attention_norm input] Token 0 First 3: [0.132642 -0.0328519 0.728735 ]
[layer8_post_attention_norm input] Token 0 Last 3: [0.0640085 0.112125 0.100104 ]
[layer8_post_attention_norm input] Token 1 First 3: [0.233087 -0.0384527 0.72297 ]
[layer8_post_attention_norm input] Token 1 Last 3: [0.197179 0.159803 0.198032 ]
[layer8_post_attention_norm input] Token 2 First 3: [0.0103885 0.10716 0.985539 ]
[layer8_post_attention_norm input] Token 2 Last 3: [0.0651846 0.0134688 0.134706 ]
--------------------
[layer8_post_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer8_post_attention_norm_weight] First 3: [12.2996 20.3878 0.0316452 ]
[layer8_post_attention_norm_weight] Last 3: [9.90362 8.99207 22.0866 ]
--------------------
[layer8pre_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer8pre_ffn_norm] Token 0 First 3: [8.20482 -0.330162 0.127644 ]
[layer8pre_ffn_norm] Token 0 Last 3: [-2.17855 0.863889 -3.30664 ]
[layer8pre_ffn_norm] Token 1 First 3: [6.16321 -3.24447 -2.67125 ]
[layer8pre_ffn_norm] Token 1 Last 3: [-6.47121 -0.137783 -3.92764 ]
[layer8pre_ffn_norm] Token 2 First 3: [6.93594 -0.18617 -2.10408 ]
[layer8pre_ffn_norm] Token 2 Last 3: [0.574819 -0.319613 -1.65137 ]
--------------------
[layer8_ffn_up input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer8_ffn_up input] First 3: [8.20482 -0.330162 0.127644 ]
[layer8_ffn_up input] Last 3: [0.574819 -0.319613 -1.65137 ]
--------------------
[layer8_ffn_up_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer8_ffn_up_weight] First 3: [0.00802135 -0.00202213 0.00467117 ]
[layer8_ffn_up_weight] Last 3: [-0.00809046 -0.00333057 0.00927764 ]
--------------------
[layer8_ffn_up] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer8_ffn_up] First 3: [-0.179347 -0.103548 0.197826 ]
[layer8_ffn_up] Last 3: [-0.41494 -0.00835359 -0.31133 ]
--------------------
[layer8_ffn_gate input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer8_ffn_gate input] First 3: [8.20482 -0.330162 0.127644 ]
[layer8_ffn_gate input] Last 3: [0.574819 -0.319613 -1.65137 ]
--------------------
[layer8_ffn_gate_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer8_ffn_gate_weight] First 3: [0.00439655 0.00685611 0.00642457 ]
[layer8_ffn_gate_weight] Last 3: [0.00694308 0.00558779 -0.00394624 ]
--------------------
[layer8_ffn_gate] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer8_ffn_gate] First 3: [0.535483 -0.0454831 -0.257508 ]
[layer8_ffn_gate] Last 3: [-0.499947 -0.0313404 -0.289097 ]
--------------------
[layer8_ffn_down input] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer8_ffn_down input] First 3: [-0.0675912 0.00226942 -0.0202952 ]
[layer8_ffn_down input] Last 3: [0.0640164 0.00012763 0.0347653 ]
--------------------
[layer8_ffn_down_weight] Shape: Shape: 1:1:1152:768 [ FP32 : NCHW ]

[layer8_ffn_down_weight] First 3: [0.00276598 0.00676104 -0.00344165 ]
[layer8_ffn_down_weight] Last 3: [0.000644742 -0.000164404 0.000730609 ]
--------------------
[layer8_ffn_down] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer8_ffn_down] First 3: [0.0268669 -0.00611138 0.0247689 ]
[layer8_ffn_down] Last 3: [-0.0565839 0.0307179 0.0525379 ]
--------------------
[layer8post_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer8post_ffn_norm] Token 0 First 3: [8.97278 -1.35931 0.22023 ]
[layer8post_ffn_norm] Token 0 Last 3: [1.16656 -5.16836 4.30723 ]
[layer8post_ffn_norm] Token 1 First 3: [-4.35619 4.30812 0.230336 ]
[layer8post_ffn_norm] Token 1 Last 3: [3.53669 -0.309135 -18.5122 ]
[layer8post_ffn_norm] Token 2 First 3: [13.5467 -1.13989 0.230574 ]
[layer8post_ffn_norm] Token 2 Last 3: [-9.51359 6.30076 17.3691 ]
--------------------
[layer9_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer9_attention_norm] Token 0 First 3: [5.22512 -0.495722 0.240116 ]
[layer9_attention_norm] Token 0 Last 3: [-1.019 -0.372044 -1.28381 ]
[layer9_attention_norm] Token 1 First 3: [4.35024 -1.44036 -2.19755 ]
[layer9_attention_norm] Token 1 Last 3: [-3.70087 -0.184832 -6.3128 ]
[layer9_attention_norm] Token 2 First 3: [3.90047 -0.312959 -1.04769 ]
[layer9_attention_norm] Token 2 Last 3: [-0.861224 0.773775 0.819056 ]
--------------------
[layer9_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer9_attention_norm input] Token 0 First 3: [104.074 -2.6534 0.352658 ]
[layer9_attention_norm input] Token 0 Last 3: [-8.34454 -2.50843 -14.259 ]
[layer9_attention_norm input] Token 1 First 3: [39.6755 -3.5302 -1.47786 ]
[layer9_attention_norm input] Token 1 Last 3: [-13.8771 -0.570623 -32.1051 ]
[layer9_attention_norm input] Token 2 First 3: [81.45 -1.75623 -1.61323 ]
[layer9_attention_norm input] Token 2 Last 3: [-7.39393 5.46956 9.53743 ]
--------------------
[layer9_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer9_attention_norm_weight] First 3: [4.21545 15.6864 57.1685 ]
[layer9_attention_norm_weight] Last 3: [10.2532 12.4532 7.55962 ]
--------------------
[layer9_wv input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer9_wv input] First 3: [5.22512 -0.495722 0.240116 ]
[layer9_wv input] Last 3: [-0.861224 0.773775 0.819056 ]
--------------------
[layer9_wv_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer9_wv_weight] First 3: [-0.0051037 0.0137163 0.00820035 ]
[layer9_wv_weight] Last 3: [-0.0035729 -0.0233551 0.0118472 ]
--------------------
[layer9_wv] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer9_wv] First 3: [-0.12673 0.565209 -0.21171 ]
[layer9_wv] Last 3: [0.0814355 -0.394384 0.0337624 ]
--------------------
[layer9_wk input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer9_wk input] First 3: [5.22512 -0.495722 0.240116 ]
[layer9_wk input] Last 3: [-0.861224 0.773775 0.819056 ]
--------------------
[layer9_wk_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer9_wk_weight] First 3: [-0.00308929 0.00824316 -0.0108908 ]
[layer9_wk_weight] Last 3: [-9.03429e-05 -0.0105303 -0.00550899 ]
--------------------
[layer9_wk] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer9_wk] First 3: [0.0761418 -0.0290176 -1.40514 ]
[layer9_wk] Last 3: [-0.291857 -0.862401 -1.03168 ]
--------------------
[layer9_k_norm] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer9_k_norm] Token 0 First 3: [0.128455 -0.0480702 0.0189988 ]
[layer9_k_norm] Token 0 Last 3: [-0.946795 -2.94773 -1.36681 ]
[layer9_k_norm] Token 1 First 3: [1.03247 1.12661 0.00244284 ]
[layer9_k_norm] Token 1 Last 3: [0.704557 2.61182 -0.600912 ]
[layer9_k_norm] Token 2 First 3: [-0.114996 -0.0184465 0.0150868 ]
[layer9_k_norm] Token 2 Last 3: [-0.394552 -2.15183 -1.52878 ]
--------------------
[layer9_wq input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer9_wq input] First 3: [5.22512 -0.495722 0.240116 ]
[layer9_wq input] Last 3: [-0.861224 0.773775 0.819056 ]
--------------------
[layer9_wq_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer9_wq_weight] First 3: [0.00333168 0.00979576 -0.00901843 ]
[layer9_wq_weight] Last 3: [-0.00689393 -0.0057532 -0.00282792 ]
--------------------
[layer9_wq] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer9_wq] First 3: [-0.600456 -0.19669 0.485761 ]
[layer9_wq] Last 3: [0.0180322 -0.554965 -0.39161 ]
--------------------
[layer9_q_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer9_q_norm] Token 0 First 3: [-0.982254 -0.387368 0.512013 ]
[layer9_q_norm] Token 0 Last 3: [0.664578 -0.129275 -0.388384 ]
[layer9_q_norm] Token 1 First 3: [-0.976129 -2.03328 0.286989 ]
[layer9_q_norm] Token 1 Last 3: [-0.85593 0.288639 -0.0505456 ]
[layer9_q_norm] Token 2 First 3: [-1.25832 -0.242077 0.0969528 ]
[layer9_q_norm] Token 2 Last 3: [0.0556119 -0.859176 -0.895175 ]
--------------------
[layer9_attention_query_rope] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer9_attention_query_rope] Token 0 First 3: [-0.982254 -0.387368 0.512013 ]
[layer9_attention_query_rope] Token 0 Last 3: [0.664578 -0.129275 -0.388384 ]
[layer9_attention_query_rope] Token 1 First 3: [-0.88922 1.21462 -0.148105 ]
[layer9_attention_query_rope] Token 1 Last 3: [-0.855763 0.288684 -0.0507158 ]
[layer9_attention_query_rope] Token 2 First 3: [1.49891 1.18771 -1.03893 ]
[layer9_attention_query_rope] Token 2 Last 3: [0.055824 -0.858665 -0.895481 ]
--------------------
[layer9_attention_key_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer9_attention_value_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer9_attention_attn_probs] Shape: Shape: 1:1:6:3 [ FP32 : NCHW ]

[layer9_attention_attn_probs] Token 0 First 3: [1 1 1 ]
[layer9_attention_attn_probs] Token 0 Last 3: [1 1 1 ]
[layer9_attention_attn_probs] Token 1 First 3: [0.764944 0.817784 0.536074 ]
[layer9_attention_attn_probs] Token 1 Last 3: [0.764944 0.817784 0.536074 ]
[layer9_attention_attn_probs] Token 2 First 3: [0.235056 0.182216 0.463926 ]
[layer9_attention_attn_probs] Token 2 Last 3: [0.235056 0.182216 0.463926 ]
[layer9_attention_attn_probs] Token 3 First 3: [0.342822 0.303539 0.351719 ]
[layer9_attention_attn_probs] Token 3 Last 3: [0.342822 0.303539 0.351719 ]
[layer9_attention_attn_probs] Token 4 First 3: [0.0017932 0.0161542 0.00130152 ]
[layer9_attention_attn_probs] Token 4 Last 3: [0.0017932 0.0161542 0.00130152 ]
[layer9_attention_attn_probs] Token 5 First 3: [0.655384 0.680307 0.646979 ]
[layer9_attention_attn_probs] Token 5 Last 3: [0.655384 0.680307 0.646979 ]
--------------------
[layer9_attention_attention_out_core] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer9_attention_attention_out_core] Token 0 First 3: [-0.126709 0.56543 -0.21167 ]
[layer9_attention_attention_out_core] Token 0 Last 3: [0.0363159 -0.230591 -0.333252 ]
[layer9_attention_attention_out_core] Token 1 First 3: [0.0845312 0.00854998 -0.421762 ]
[layer9_attention_attention_out_core] Token 1 Last 3: [0.26978 -0.0693606 -0.461126 ]
[layer9_attention_attention_out_core] Token 2 First 3: [0.105471 1.29977 -0.167511 ]
[layer9_attention_attention_out_core] Token 2 Last 3: [0.0661529 -0.336047 -0.0961664 ]
--------------------
[layer9_attention_out input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer9_attention_out input] First 3: [-0.126709 0.56543 -0.21167 ]
[layer9_attention_out input] Last 3: [0.0661529 -0.336047 -0.0961664 ]
--------------------
[layer9_attention_out_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer9_attention_out_weight] First 3: [0.00870713 0.00794372 -0.0088894 ]
[layer9_attention_out_weight] Last 3: [0.00535692 0.012863 0.0059447 ]
--------------------
[layer9_attention_out] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer9_attention_out] First 3: [0.0592098 -0.0170009 0.206408 ]
[layer9_attention_out] Last 3: [-0.139062 0.0231103 0.0228204 ]
--------------------
[layer9_post_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer9_post_attention_norm] Token 0 First 3: [2.94217 -1.89374 0.0123838 ]
[layer9_post_attention_norm] Token 0 Last 3: [-5.05849 4.88479 -4.22799 ]
[layer9_post_attention_norm] Token 1 First 3: [4.56133 -10.7549 0.00849871 ]
[layer9_post_attention_norm] Token 1 Last 3: [-2.16676 0.901661 4.29305 ]
[layer9_post_attention_norm] Token 2 First 3: [3.09445 -0.890412 0.0137245 ]
[layer9_post_attention_norm] Token 2 Last 3: [-4.03585 0.810966 2.03744 ]
--------------------
[layer9_post_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer9_post_attention_norm input] Token 0 First 3: [0.0592098 -0.0170009 0.206408 ]
[layer9_post_attention_norm input] Token 0 Last 3: [-0.104448 0.083417 -0.0283778 ]
[layer9_post_attention_norm input] Token 1 First 3: [0.0851123 -0.0895232 0.131341 ]
[layer9_post_attention_norm input] Token 1 Last 3: [-0.0414825 0.0142767 0.0267169 ]
[layer9_post_attention_norm input] Token 2 First 3: [0.103921 -0.0133394 0.381735 ]
[layer9_post_attention_norm input] Token 2 Last 3: [-0.139062 0.0231103 0.0228204 ]
--------------------
[layer9_post_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer9_post_attention_norm_weight] First 3: [6.0255 13.5072 0.00727522 ]
[layer9_post_attention_norm_weight] Last 3: [5.87272 7.10083 18.0665 ]
--------------------
[layer9pre_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer9pre_ffn_norm] Token 0 First 3: [7.21812 -0.613439 0.209813 ]
[layer9pre_ffn_norm] Token 0 Last 3: [-2.1239 0.413573 -1.87004 ]
[layer9pre_ffn_norm] Token 1 First 3: [5.6396 -3.64255 -1.59627 ]
[layer9pre_ffn_norm] Token 1 Last 3: [-4.80538 0.108894 -5.31749 ]
[layer9pre_ffn_norm] Token 2 First 3: [5.09461 -0.31899 -0.821344 ]
[layer9pre_ffn_norm] Token 2 Last 3: [-1.61815 0.976529 1.04605 ]
--------------------
[layer9_ffn_up input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer9_ffn_up input] First 3: [7.21812 -0.613439 0.209813 ]
[layer9_ffn_up input] Last 3: [-1.61815 0.976529 1.04605 ]
--------------------
[layer9_ffn_up_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer9_ffn_up_weight] First 3: [-0.00152924 0.00984593 -0.0012934 ]
[layer9_ffn_up_weight] Last 3: [-0.00141902 -0.00709267 0.0053155 ]
--------------------
[layer9_ffn_up] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer9_ffn_up] First 3: [-0.223717 0.347647 -0.28438 ]
[layer9_ffn_up] Last 3: [0.134146 0.0351029 -0.727639 ]
--------------------
[layer9_ffn_gate input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer9_ffn_gate input] First 3: [7.21812 -0.613439 0.209813 ]
[layer9_ffn_gate input] Last 3: [-1.61815 0.976529 1.04605 ]
--------------------
[layer9_ffn_gate_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer9_ffn_gate_weight] First 3: [-5.71652e-05 0.00559223 0.00917139 ]
[layer9_ffn_gate_weight] Last 3: [-0.000317483 -0.00445014 -0.00762386 ]
--------------------
[layer9_ffn_gate] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer9_ffn_gate] First 3: [-0.438377 -0.639591 -0.169766 ]
[layer9_ffn_gate] Last 3: [-0.00938739 -0.174651 0.0490341 ]
--------------------
[layer9_ffn_down input] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer9_ffn_down input] First 3: [0.0324209 -0.0580968 0.020885 ]
[layer9_ffn_down input] Last 3: [-0.000624926 -0.00264038 -0.0185372 ]
--------------------
[layer9_ffn_down_weight] Shape: Shape: 1:1:1152:768 [ FP32 : NCHW ]

[layer9_ffn_down_weight] First 3: [0.00195207 0.000333329 0.00385658 ]
[layer9_ffn_down_weight] Last 3: [0.000531874 -0.00275887 0.000723187 ]
--------------------
[layer9_ffn_down] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer9_ffn_down] First 3: [0.0457391 0.0105721 0.0160319 ]
[layer9_ffn_down] Last 3: [-0.00610108 0.01196 0.00375596 ]
--------------------
[layer9post_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer9post_ffn_norm] Token 0 First 3: [28.4912 8.67263 0.16776 ]
[layer9post_ffn_norm] Token 0 Last 3: [-15.3547 2.91325 -9.56476 ]
[layer9post_ffn_norm] Token 1 First 3: [-8.31894 -34.492 -0.0403616 ]
[layer9post_ffn_norm] Token 1 Last 3: [-1.59643 -3.60618 -27.2063 ]
[layer9post_ffn_norm] Token 2 First 3: [5.9498 2.7611 -0.170967 ]
[layer9post_ffn_norm] Token 2 Last 3: [-3.3896 8.87501 5.50221 ]
--------------------
[layer10_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer10_attention_norm] Token 0 First 3: [8.83951 0.448116 0.417323 ]
[layer10_attention_norm] Token 0 Last 3: [-3.26771 0.625328 -2.06381 ]
[layer10_attention_norm] Token 1 First 3: [5.79402 -13.1019 -2.9242 ]
[layer10_attention_norm] Token 1 Last 3: [-4.95676 -0.957453 -10.0097 ]
[layer10_attention_norm] Token 2 First 3: [5.70892 0.0120235 -1.3411 ]
[layer10_attention_norm] Token 2 Last 3: [-1.6285 1.73269 1.21504 ]
--------------------
[layer10_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer10_attention_norm input] Token 0 First 3: [135.507 4.12549 0.532802 ]
[layer10_attention_norm input] Token 0 Last 3: [-28.7577 5.28961 -28.0517 ]
[layer10_attention_norm input] Token 1 First 3: [35.9179 -48.7771 -1.50972 ]
[layer10_attention_norm input] Token 1 Last 3: [-17.6402 -3.27514 -55.0184 ]
[layer10_attention_norm input] Token 2 First 3: [90.4942 0.114459 -1.77047 ]
[layer10_attention_norm input] Token 2 Last 3: [-14.8194 15.1555 17.0771 ]
--------------------
[layer10_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer10_attention_norm_weight] First 3: [7.7824 12.9587 93.4445 ]
[layer10_attention_norm_weight] Last 3: [13.5562 14.1036 8.77721 ]
--------------------
[layer10_wv input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer10_wv input] First 3: [8.83951 0.448116 0.417323 ]
[layer10_wv input] Last 3: [-1.6285 1.73269 1.21504 ]
--------------------
[layer10_wv_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer10_wv_weight] First 3: [0.000109055 0.00312905 0.00265395 ]
[layer10_wv_weight] Last 3: [0.00187692 -0.00496043 0.0130821 ]
--------------------
[layer10_wv] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer10_wv] First 3: [-0.614919 1.07638 -0.560054 ]
[layer10_wv] Last 3: [0.129719 0.0813397 0.0180429 ]
--------------------
[layer10_wk input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer10_wk input] First 3: [8.83951 0.448116 0.417323 ]
[layer10_wk input] Last 3: [-1.6285 1.73269 1.21504 ]
--------------------
[layer10_wk_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer10_wk_weight] First 3: [0.0108338 0.0182575 0.000826243 ]
[layer10_wk_weight] Last 3: [0.0127096 -0.0147154 -0.00466373 ]
--------------------
[layer10_wk] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer10_wk] First 3: [0.570669 0.154146 0.476221 ]
[layer10_wk] Last 3: [-0.150789 0.169182 2.81252 ]
--------------------
[layer10_k_norm] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer10_k_norm] Token 0 First 3: [0.223496 0.106616 0.328481 ]
[layer10_k_norm] Token 0 Last 3: [-0.0110272 0.100442 4.75112 ]
[layer10_k_norm] Token 1 First 3: [0.823026 -0.209139 0.349519 ]
[layer10_k_norm] Token 1 Last 3: [-1.47714 0.743384 3.68453 ]
[layer10_k_norm] Token 2 First 3: [0.595187 -0.0774742 0.122225 ]
[layer10_k_norm] Token 2 Last 3: [-0.226516 0.222374 5.79491 ]
--------------------
[layer10_wq input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer10_wq input] First 3: [8.83951 0.448116 0.417323 ]
[layer10_wq input] Last 3: [-1.6285 1.73269 1.21504 ]
--------------------
[layer10_wq_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer10_wq_weight] First 3: [0.00682145 -0.00191117 -0.00692999 ]
[layer10_wq_weight] Last 3: [-0.00441789 0.00442306 -0.032971 ]
--------------------
[layer10_wq] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer10_wq] First 3: [-1.06779 -0.0728123 0.2084 ]
[layer10_wq] Last 3: [0.170153 -0.0626763 -0.513314 ]
--------------------
[layer10_q_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer10_q_norm] Token 0 First 3: [-2.70159 -0.186762 0.420186 ]
[layer10_q_norm] Token 0 Last 3: [-0.0396456 -0.189924 -0.189635 ]
[layer10_q_norm] Token 1 First 3: [-4.63901 2.06401 -0.21808 ]
[layer10_q_norm] Token 1 Last 3: [-0.685218 0.13015 0.886417 ]
[layer10_q_norm] Token 2 First 3: [-1.52893 0.0890604 -0.311841 ]
[layer10_q_norm] Token 2 Last 3: [0.124997 -0.0548308 -0.293408 ]
--------------------
[layer10_attention_query_rope] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer10_attention_query_rope] Token 0 First 3: [-2.70159 -0.186762 0.420186 ]
[layer10_attention_query_rope] Token 0 Last 3: [-0.0396456 -0.189924 -0.189635 ]
[layer10_attention_query_rope] Token 1 First 3: [4.56599 0.883731 0.909553 ]
[layer10_attention_query_rope] Token 1 Last 3: [-0.685214 0.13015 0.886423 ]
[layer10_attention_query_rope] Token 2 First 3: [2.36555 -0.63758 0.600796 ]
[layer10_attention_query_rope] Token 2 Last 3: [0.125002 -0.0547552 -0.293384 ]
--------------------
[layer10_attention_key_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer10_attention_value_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer10_attention_attn_probs] Shape: Shape: 1:1:6:3 [ FP32 : NCHW ]

[layer10_attention_attn_probs] Token 0 First 3: [1 1 1 ]
[layer10_attention_attn_probs] Token 0 Last 3: [1 1 1 ]
[layer10_attention_attn_probs] Token 1 First 3: [0.612393 0.336692 0.872247 ]
[layer10_attention_attn_probs] Token 1 Last 3: [0.612393 0.336692 0.872247 ]
[layer10_attention_attn_probs] Token 2 First 3: [0.387607 0.663308 0.127753 ]
[layer10_attention_attn_probs] Token 2 Last 3: [0.387607 0.663308 0.127753 ]
[layer10_attention_attn_probs] Token 3 First 3: [0.705218 0.256489 0.497659 ]
[layer10_attention_attn_probs] Token 3 Last 3: [0.705218 0.256489 0.497659 ]
[layer10_attention_attn_probs] Token 4 First 3: [0.123532 0.0142537 0.0252253 ]
[layer10_attention_attn_probs] Token 4 Last 3: [0.123532 0.0142537 0.0252253 ]
[layer10_attention_attn_probs] Token 5 First 3: [0.17125 0.729257 0.477116 ]
[layer10_attention_attn_probs] Token 5 Last 3: [0.17125 0.729257 0.477116 ]
--------------------
[layer10_attention_attention_out_core] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer10_attention_attention_out_core] Token 0 First 3: [-0.614746 1.07617 -0.560059 ]
[layer10_attention_attention_out_core] Token 0 Last 3: [0.12854 0.0961304 0.570312 ]
[layer10_attention_attention_out_core] Token 1 First 3: [-0.469535 2.33817 -0.967538 ]
[layer10_attention_attention_out_core] Token 1 Last 3: [0.104056 -0.0399113 0.590835 ]
[layer10_attention_attention_out_core] Token 2 First 3: [-0.477156 1.42076 -0.592801 ]
[layer10_attention_attention_out_core] Token 2 Last 3: [0.124288 0.0622212 0.310865 ]
--------------------
[layer10_attention_out input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer10_attention_out input] First 3: [-0.614746 1.07617 -0.560059 ]
[layer10_attention_out input] Last 3: [0.124288 0.0622212 0.310865 ]
--------------------
[layer10_attention_out_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer10_attention_out_weight] First 3: [-0.00791129 0.000389489 -0.0104881 ]
[layer10_attention_out_weight] Last 3: [-0.0098946 -0.0111841 -0.0147935 ]
--------------------
[layer10_attention_out] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer10_attention_out] First 3: [-0.0142031 -0.20689 -0.235544 ]
[layer10_attention_out] Last 3: [-0.11229 -0.231076 -0.0904993 ]
--------------------
[layer10_post_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer10_post_attention_norm] Token 0 First 3: [-0.376611 -43.319 0.062337 ]
[layer10_post_attention_norm] Token 0 Last 3: [-1.12573 -5.51559 -10.1558 ]
[layer10_post_attention_norm] Token 1 First 3: [2.00221 37.9768 -0.00643147 ]
[layer10_post_attention_norm] Token 1 Last 3: [-4.30714 -12.1672 -12.3394 ]
[layer10_post_attention_norm] Token 2 First 3: [1.88188 25.5371 0.0374083 ]
[layer10_post_attention_norm] Token 2 Last 3: [-4.0752 -11.2017 -10.7474 ]
--------------------
[layer10_post_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer10_post_attention_norm input] Token 0 First 3: [-0.0142031 -0.20689 -0.235544 ]
[layer10_post_attention_norm input] Token 0 Last 3: [-0.0403287 -0.147928 -0.111183 ]
[layer10_post_attention_norm input] Token 1 First 3: [0.0991937 0.238266 0.0319242 ]
[layer10_post_attention_norm input] Token 1 Last 3: [-0.202699 -0.428679 -0.177462 ]
[layer10_post_attention_norm input] Token 2 First 3: [0.0545879 0.0938091 -0.108719 ]
[layer10_post_attention_norm input] Token 2 Last 3: [-0.11229 -0.231076 -0.0904993 ]
--------------------
[layer10_post_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer10_post_attention_norm_weight] First 3: [8.25771 65.2064 -0.0824184 ]
[layer10_post_attention_norm_weight] Last 3: [8.69304 11.6116 28.4461 ]
--------------------
[layer10pre_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer10pre_ffn_norm] Token 0 First 3: [4.13693 -0.905286 0.193664 ]
[layer10pre_ffn_norm] Token 0 Last 3: [-1.75332 -0.0121171 -1.13599 ]
[layer10pre_ffn_norm] Token 1 First 3: [2.38678 -0.512892 -1.01436 ]
[layer10pre_ffn_norm] Token 1 Last 3: [-2.64747 -1.70244 -4.11746 ]
[layer10pre_ffn_norm] Token 2 First 3: [2.84497 0.596044 -0.567333 ]
[layer10pre_ffn_norm] Token 2 Last 3: [-1.11522 0.213281 0.189319 ]
--------------------
[layer10_ffn_up input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer10_ffn_up input] First 3: [4.13693 -0.905286 0.193664 ]
[layer10_ffn_up input] Last 3: [-1.11522 0.213281 0.189319 ]
--------------------
[layer10_ffn_up_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer10_ffn_up_weight] First 3: [0.0162465 0.000716229 -0.00345485 ]
[layer10_ffn_up_weight] Last 3: [-0.000299963 0.00210077 -0.010256 ]
--------------------
[layer10_ffn_up] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer10_ffn_up] First 3: [-0.0737996 -0.207996 0.214728 ]
[layer10_ffn_up] Last 3: [-0.0721291 -0.029988 -0.191507 ]
--------------------
[layer10_ffn_gate input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer10_ffn_gate input] First 3: [4.13693 -0.905286 0.193664 ]
[layer10_ffn_gate input] Last 3: [-1.11522 0.213281 0.189319 ]
--------------------
[layer10_ffn_gate_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer10_ffn_gate_weight] First 3: [0.012142 0.00122491 0.00107506 ]
[layer10_ffn_gate_weight] Last 3: [-0.00312346 0.00113301 -0.00675839 ]
--------------------
[layer10_ffn_gate] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer10_ffn_gate] First 3: [0.0881416 -0.0607325 -0.100532 ]
[layer10_ffn_gate] Last 3: [-0.23057 -0.265751 -0.153268 ]
--------------------
[layer10_ffn_down input] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer10_ffn_down input] First 3: [-0.00348084 0.00601019 -0.00992925 ]
[layer10_ffn_down input] Last 3: [0.00679915 0.00314966 0.0128882 ]
--------------------
[layer10_ffn_down_weight] Shape: Shape: 1:1:1152:768 [ FP32 : NCHW ]

[layer10_ffn_down_weight] First 3: [-0.00945866 0.00166636 -0.0072893 ]
[layer10_ffn_down_weight] Last 3: [0.00183759 -0.00323307 -0.00967391 ]
--------------------
[layer10_ffn_down] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer10_ffn_down] First 3: [0.0104564 0.000566037 -0.00153376 ]
[layer10_ffn_down] Last 3: [-0.00171482 0.00342613 0.00121957 ]
--------------------
[layer10post_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer10post_ffn_norm] Token 0 First 3: [34.1217 3.60734 -0.0499454 ]
[layer10post_ffn_norm] Token 0 Last 3: [-4.26998 4.8199 -1.79593 ]
[layer10post_ffn_norm] Token 1 First 3: [3.80437 -6.15153 -0.16694 ]
[layer10post_ffn_norm] Token 1 Last 3: [0.0574949 -10.2653 -2.81399 ]
[layer10post_ffn_norm] Token 2 First 3: [47.9114 -9.67007 0.0382772 ]
[layer10post_ffn_norm] Token 2 Last 3: [-5.12952 13.9752 10.8935 ]
--------------------
[layer11_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer11_attention_norm] Token 0 First 3: [1.83482 -0.374026 0.241068 ]
[layer11_attention_norm] Token 0 Last 3: [-0.586613 0.0417873 -0.229486 ]
[layer11_attention_norm] Token 1 First 3: [1.40738 -0.55437 -2.31558 ]
[layer11_attention_norm] Token 1 Last 3: [-1.16983 -0.727586 -1.25252 ]
[layer11_attention_norm] Token 2 First 3: [2.12888 0.235132 -1.049 ]
[layer11_attention_norm] Token 2 Last 3: [-0.577615 0.228292 0.138307 ]
--------------------
[layer11_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer11_attention_norm input] Token 0 First 3: [169.252 -35.5861 0.545193 ]
[layer11_attention_norm input] Token 0 Last 3: [-34.1534 4.59392 -40.0034 ]
[layer11_attention_norm input] Token 1 First 3: [41.7244 -16.9518 -1.6831 ]
[layer11_attention_norm input] Token 1 Last 3: [-21.8899 -25.7076 -70.1718 ]
[layer11_attention_norm input] Token 2 First 3: [140.287 15.9815 -1.69479 ]
[layer11_attention_norm input] Token 2 Last 3: [-24.0241 17.9291 17.2231 ]
--------------------
[layer11_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer11_attention_norm_weight] First 3: [1.75085 1.6975 71.413 ]
[layer11_attention_norm_weight] Last 3: [2.774 1.46909 0.926504 ]
--------------------
[layer11_wv input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer11_wv input] First 3: [1.83482 -0.374026 0.241068 ]
[layer11_wv input] Last 3: [-0.577615 0.228292 0.138307 ]
--------------------
[layer11_wv_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer11_wv_weight] First 3: [-0.000957145 0.0007542 0.0018174 ]
[layer11_wv_weight] Last 3: [-0.0082522 -0.0159171 -0.00465451 ]
--------------------
[layer11_wv] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer11_wv] First 3: [-0.153515 0.0056542 -0.0056265 ]
[layer11_wv] Last 3: [0.099134 0.123019 -0.00987217 ]
--------------------
[layer11_wk input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer11_wk input] First 3: [1.83482 -0.374026 0.241068 ]
[layer11_wk input] Last 3: [-0.577615 0.228292 0.138307 ]
--------------------
[layer11_wk_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer11_wk_weight] First 3: [0.00788751 0.020067 0.0086193 ]
[layer11_wk_weight] Last 3: [0.00217386 -0.0251729 -0.00269745 ]
--------------------
[layer11_wk] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer11_wk] First 3: [0.0460736 0.174457 0.0122748 ]
[layer11_wk] Last 3: [0.61948 0.729346 0.719741 ]
--------------------
[layer11_k_norm] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer11_k_norm] Token 0 First 3: [0.281437 0.00312247 0.0514828 ]
[layer11_k_norm] Token 0 Last 3: [17.9569 20.1204 19.6495 ]
[layer11_k_norm] Token 1 First 3: [-0.690678 0.0122315 0.609015 ]
[layer11_k_norm] Token 1 Last 3: [11.3034 26.4271 13.3015 ]
[layer11_k_norm] Token 2 First 3: [1.1811 0.00192367 0.0113886 ]
[layer11_k_norm] Token 2 Last 3: [21.1876 19.0988 18.8467 ]
--------------------
[layer11_wq input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer11_wq input] First 3: [1.83482 -0.374026 0.241068 ]
[layer11_wq input] Last 3: [-0.577615 0.228292 0.138307 ]
--------------------
[layer11_wq_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer11_wq_weight] First 3: [0.019849 -0.00417023 0.0198054 ]
[layer11_wq_weight] Last 3: [0.0128342 -0.016885 -0.00440726 ]
--------------------
[layer11_wq] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer11_wq] First 3: [-0.0915599 0.345937 0.198464 ]
[layer11_wq] Last 3: [0.280231 0.211123 0.0882106 ]
--------------------
[layer11_q_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer11_q_norm] Token 0 First 3: [-0.819191 -0.0452683 4.82725 ]
[layer11_q_norm] Token 0 Last 3: [0.120981 0.220739 0.137363 ]
[layer11_q_norm] Token 1 First 3: [-0.63013 -0.0341328 -9.55789 ]
[layer11_q_norm] Token 1 Last 3: [0.0946267 0.784127 0.215893 ]
[layer11_q_norm] Token 2 First 3: [0.728222 -0.0491031 4.10951 ]
[layer11_q_norm] Token 2 Last 3: [0.373719 0.463981 0.187293 ]
--------------------
[layer11_attention_query_rope] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer11_attention_query_rope] Token 0 First 3: [-0.819191 -0.0452683 4.82725 ]
[layer11_attention_query_rope] Token 0 Last 3: [0.120981 0.220739 0.137363 ]
[layer11_attention_query_rope] Token 1 First 3: [6.0056 -5.96455 -6.22299 ]
[layer11_attention_query_rope] Token 1 Last 3: [0.0945921 0.784175 0.215918 ]
[layer11_attention_query_rope] Token 2 First 3: [2.47546 -0.507209 -0.701021 ]
[layer11_attention_query_rope] Token 2 Last 3: [0.373635 0.464065 0.187408 ]
--------------------
[layer11_attention_key_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer11_attention_value_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer11_attention_attn_probs] Shape: Shape: 1:1:6:3 [ FP32 : NCHW ]

[layer11_attention_attn_probs] Token 0 First 3: [1 1 1 ]
[layer11_attention_attn_probs] Token 0 Last 3: [1 1 1 ]
[layer11_attention_attn_probs] Token 1 First 3: [0.0357788 0.658897 0.0534897 ]
[layer11_attention_attn_probs] Token 1 Last 3: [0.0357788 0.658897 0.0534897 ]
[layer11_attention_attn_probs] Token 2 First 3: [0.964221 0.341103 0.94651 ]
[layer11_attention_attn_probs] Token 2 Last 3: [0.964221 0.341103 0.94651 ]
[layer11_attention_attn_probs] Token 3 First 3: [0.0736483 0.176145 0.0185286 ]
[layer11_attention_attn_probs] Token 3 Last 3: [0.0736483 0.176145 0.0185286 ]
[layer11_attention_attn_probs] Token 4 First 3: [0.00223999 0.0806134 0.0004893 ]
[layer11_attention_attn_probs] Token 4 Last 3: [0.00223999 0.0806134 0.0004893 ]
[layer11_attention_attn_probs] Token 5 First 3: [0.924112 0.743241 0.980982 ]
[layer11_attention_attn_probs] Token 5 Last 3: [0.924112 0.743241 0.980982 ]
--------------------
[layer11_attention_attention_out_core] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer11_attention_attention_out_core] Token 0 First 3: [-0.153564 0.00565338 -0.00562668 ]
[layer11_attention_attention_out_core] Token 0 Last 3: [0.206543 0.0636597 -0.0631104 ]
[layer11_attention_attention_out_core] Token 1 First 3: [-0.373669 0.415222 -0.683819 ]
[layer11_attention_attention_out_core] Token 1 Last 3: [0.442246 -0.259335 0.231172 ]
[layer11_attention_attention_out_core] Token 2 First 3: [-0.147307 0.0461647 -0.0171327 ]
[layer11_attention_attention_out_core] Token 2 Last 3: [0.101286 0.12175 -0.0107328 ]
--------------------
[layer11_attention_out input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer11_attention_out input] First 3: [-0.153564 0.00565338 -0.00562668 ]
[layer11_attention_out input] Last 3: [0.101286 0.12175 -0.0107328 ]
--------------------
[layer11_attention_out_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer11_attention_out_weight] First 3: [-0.0044518 -0.00322494 -0.0178836 ]
[layer11_attention_out_weight] Last 3: [-0.00633536 -0.00575687 0.000699627 ]
--------------------
[layer11_attention_out] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer11_attention_out] First 3: [-0.0205277 -0.00461375 0.0408228 ]
[layer11_attention_out] Last 3: [-0.0314592 0.00611067 0.0393813 ]
--------------------
[layer11_post_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer11_post_attention_norm] Token 0 First 3: [-9.17991 -6.56202 0.00275255 ]
[layer11_post_attention_norm] Token 0 Last 3: [0.638188 -3.23656 8.20727 ]
[layer11_post_attention_norm] Token 1 First 3: [-3.58318 15.0194 0.00143681 ]
[layer11_post_attention_norm] Token 1 Last 3: [9.26715 3.39778 3.10084 ]
[layer11_post_attention_norm] Token 2 First 3: [-8.25922 -16.3741 0.00129762 ]
[layer11_post_attention_norm] Token 2 Last 3: [-6.22797 1.07721 19.2629 ]
--------------------
[layer11_post_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer11_post_attention_norm input] Token 0 First 3: [-0.0205277 -0.00461375 0.0408228 ]
[layer11_post_attention_norm input] Token 0 Last 3: [0.00174346 -0.00992969 0.00907465 ]
[layer11_post_attention_norm input] Token 1 First 3: [-0.0199907 0.0263467 0.0531648 ]
[layer11_post_attention_norm input] Token 1 Last 3: [0.0631638 0.0260078 0.00855399 ]
[layer11_post_attention_norm input] Token 2 First 3: [-0.034149 -0.0212868 0.0355837 ]
[layer11_post_attention_norm input] Token 2 Last 3: [-0.0314592 0.00611067 0.0393813 ]
--------------------
[layer11_post_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer11_post_attention_norm_weight] First 3: [20.0125 63.6484 0.00301743 ]
[layer11_post_attention_norm_weight] Last 3: [16.381 14.5866 40.4738 ]
--------------------
[layer11pre_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer11pre_ffn_norm] Token 0 First 3: [3.28048 -0.380939 0.273243 ]
[layer11pre_ffn_norm] Token 0 Last 3: [-1.04229 0.0424353 -0.398207 ]
[layer11pre_ffn_norm] Token 1 First 3: [2.17147 -0.0485209 -2.32962 ]
[layer11pre_ffn_norm] Token 1 Last 3: [-1.09052 -1.93761 -2.33348 ]
[layer11pre_ffn_norm] Token 2 First 3: [3.66952 -0.00481227 -1.14529 ]
[layer11pre_ffn_norm] Token 2 Last 3: [-1.27591 0.805843 0.619701 ]
--------------------
[layer11_ffn_up input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer11_ffn_up input] First 3: [3.28048 -0.380939 0.273243 ]
[layer11_ffn_up input] Last 3: [-1.27591 0.805843 0.619701 ]
--------------------
[layer11_ffn_up_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer11_ffn_up_weight] First 3: [-0.00245252 0.00239866 -0.00288312 ]
[layer11_ffn_up_weight] Last 3: [0.0100402 0.00648673 -0.000696481 ]
--------------------
[layer11_ffn_up] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer11_ffn_up] First 3: [0.117836 -0.12829 0.101608 ]
[layer11_ffn_up] Last 3: [0.187891 0.0653761 0.193271 ]
--------------------
[layer11_ffn_gate input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer11_ffn_gate input] First 3: [3.28048 -0.380939 0.273243 ]
[layer11_ffn_gate input] Last 3: [-1.27591 0.805843 0.619701 ]
--------------------
[layer11_ffn_gate_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer11_ffn_gate_weight] First 3: [-0.000663899 0.0050625 -0.00380767 ]
[layer11_ffn_gate_weight] Last 3: [-0.00640728 -0.00119501 -0.00353221 ]
--------------------
[layer11_ffn_gate] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer11_ffn_gate] First 3: [0.036191 -0.104864 0.247049 ]
[layer11_ffn_gate] Last 3: [0.0955045 -0.0369989 -0.183627 ]
--------------------
[layer11_ffn_down input] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer11_ffn_down input] First 3: [0.00219386 0.00616474 0.015 ]
[layer11_ffn_down input] Last 3: [0.00965487 -0.00117373 -0.0151596 ]
--------------------
[layer11_ffn_down_weight] Shape: Shape: 1:1:1152:768 [ FP32 : NCHW ]

[layer11_ffn_down_weight] First 3: [0.000873509 -0.00381164 0.00876246 ]
[layer11_ffn_down_weight] Last 3: [0.00733191 -0.00389158 0.00544492 ]
--------------------
[layer11_ffn_down] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer11_ffn_down] First 3: [0.00183531 7.93663e-05 0.00123929 ]
[layer11_ffn_down] Last 3: [0.000320188 -0.000292348 -0.00190322 ]
--------------------
[layer11post_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer11post_ffn_norm] Token 0 First 3: [10.506 0.952147 1.38887 ]
[layer11post_ffn_norm] Token 0 Last 3: [-3.01414 -1.94962 -18.3282 ]
[layer11post_ffn_norm] Token 1 First 3: [-10.9847 7.68059 1.89867 ]
[layer11post_ffn_norm] Token 1 Last 3: [8.52261 7.58824 -1.46396 ]
[layer11post_ffn_norm] Token 2 First 3: [-27.6981 15.5679 3.62073 ]
[layer11post_ffn_norm] Token 2 Last 3: [1.41161 -1.49497 -22.0811 ]
--------------------
[layer12_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer12_attention_norm] Token 0 First 3: [5.86297 -1.46784 0.60899 ]
[layer12_attention_norm] Token 0 Last 3: [-2.29047 -0.0397407 -1.56815 ]
[layer12_attention_norm] Token 1 First 3: [2.03726 0.44702 0.148932 ]
[layer12_attention_norm] Token 1 Last 3: [-0.561122 -2.15602 -4.6798 ]
[layer12_attention_norm] Token 2 First 3: [4.45506 0.671756 0.752846 ]
[layer12_attention_norm] Token 2 Last 3: [-2.24665 1.45979 0.559889 ]
--------------------
[layer12_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer12_attention_norm input] Token 0 First 3: [170.578 -41.196 1.93682 ]
[layer12_attention_norm input] Token 0 Last 3: [-36.5294 -0.592263 -50.1243 ]
[layer12_attention_norm input] Token 1 First 3: [27.1566 5.7481 0.217016 ]
[layer12_attention_norm input] Token 1 Last 3: [-4.10013 -14.7216 -68.5349 ]
[layer12_attention_norm input] Token 2 First 3: [104.33 15.1753 1.92724 ]
[layer12_attention_norm input] Token 2 Last 3: [-28.8405 17.5113 14.405 ]
--------------------
[layer12_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer12_attention_norm_weight] First 3: [7.65111 7.9315 69.9924 ]
[layer12_attention_norm_weight] Last 3: [13.9577 14.9366 6.96416 ]
--------------------
[layer12_wv input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer12_wv input] First 3: [5.86297 -1.46784 0.60899 ]
[layer12_wv input] Last 3: [-2.24665 1.45979 0.559889 ]
--------------------
[layer12_wv_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer12_wv_weight] First 3: [0.00271251 0.00511753 -0.0037066 ]
[layer12_wv_weight] Last 3: [0.00559548 0.00376156 0.0243784 ]
--------------------
[layer12_wv] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer12_wv] First 3: [0.0211459 -0.344031 -0.0451108 ]
[layer12_wv] Last 3: [-0.0034311 0.289965 -0.823288 ]
--------------------
[layer12_wk input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer12_wk input] First 3: [5.86297 -1.46784 0.60899 ]
[layer12_wk input] Last 3: [-2.24665 1.45979 0.559889 ]
--------------------
[layer12_wk_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer12_wk_weight] First 3: [0.00174896 -0.0111027 -0.00045168 ]
[layer12_wk_weight] Last 3: [-0.0151768 0.00632126 0.0036681 ]
--------------------
[layer12_wk] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer12_wk] First 3: [0.210499 -0.0196792 0.357348 ]
[layer12_wk] Last 3: [2.39957 1.49067 0.284341 ]
--------------------
[layer12_k_norm] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer12_k_norm] Token 0 First 3: [0.134234 -0.0244955 0.209327 ]
[layer12_k_norm] Token 0 Last 3: [10.0829 2.16908 -0.73785 ]
[layer12_k_norm] Token 1 First 3: [0.507067 0.705523 0.417629 ]
[layer12_k_norm] Token 1 Last 3: [11.6612 4.16272 0.857001 ]
[layer12_k_norm] Token 2 First 3: [-0.0410582 0.323264 0.136258 ]
[layer12_k_norm] Token 2 Last 3: [11.7487 3.15176 0.333779 ]
--------------------
[layer12_wq input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer12_wq input] First 3: [5.86297 -1.46784 0.60899 ]
[layer12_wq input] Last 3: [-2.24665 1.45979 0.559889 ]
--------------------
[layer12_wq_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer12_wq_weight] First 3: [0.00894941 0.0168542 0.00193584 ]
[layer12_wq_weight] Last 3: [0.00802083 -0.00536757 -0.00534549 ]
--------------------
[layer12_wq] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer12_wq] First 3: [-0.804142 0.377045 -0.852879 ]
[layer12_wq] Last 3: [0.476152 0.82154 -1.66246 ]
--------------------
[layer12_q_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer12_q_norm] Token 0 First 3: [-0.981181 0.350495 -2.1247 ]
[layer12_q_norm] Token 0 Last 3: [0.356211 0.796762 -4.59156 ]
[layer12_q_norm] Token 1 First 3: [-1.04137 -0.345306 -1.9149 ]
[layer12_q_norm] Token 1 Last 3: [0.252051 0.376071 -4.89137 ]
[layer12_q_norm] Token 2 First 3: [-0.607212 -0.322297 -1.77979 ]
[layer12_q_norm] Token 2 Last 3: [0.310876 1.23821 -8.92413 ]
--------------------
[layer12_attention_query_rope] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer12_attention_query_rope] Token 0 First 3: [-0.981181 0.350495 -2.1247 ]
[layer12_attention_query_rope] Token 0 Last 3: [0.356211 0.796762 -4.59156 ]
[layer12_attention_query_rope] Token 1 First 3: [-0.319721 -0.451555 -1.26063 ]
[layer12_attention_query_rope] Token 1 Last 3: [0.251994 0.375754 -4.89146 ]
[layer12_attention_query_rope] Token 2 First 3: [0.796731 0.0564409 0.32175 ]
[layer12_attention_query_rope] Token 2 Last 3: [0.310819 1.23778 -8.92418 ]
--------------------
[layer12_attention_key_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer12_attention_value_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer12_attention_attn_probs] Shape: Shape: 1:1:6:3 [ FP32 : NCHW ]

[layer12_attention_attn_probs] Token 0 First 3: [1 1 1 ]
[layer12_attention_attn_probs] Token 0 Last 3: [1 1 1 ]
[layer12_attention_attn_probs] Token 1 First 3: [0.488332 0.875458 0.218455 ]
[layer12_attention_attn_probs] Token 1 Last 3: [0.488332 0.875458 0.218455 ]
[layer12_attention_attn_probs] Token 2 First 3: [0.511668 0.124542 0.781545 ]
[layer12_attention_attn_probs] Token 2 Last 3: [0.511668 0.124542 0.781545 ]
[layer12_attention_attn_probs] Token 3 First 3: [0.451086 0.476885 0.2306 ]
[layer12_attention_attn_probs] Token 3 Last 3: [0.451086 0.476885 0.2306 ]
[layer12_attention_attn_probs] Token 4 First 3: [0.0915053 0.129176 0.321702 ]
[layer12_attention_attn_probs] Token 4 Last 3: [0.0915053 0.129176 0.321702 ]
[layer12_attention_attn_probs] Token 5 First 3: [0.457408 0.393939 0.447697 ]
[layer12_attention_attn_probs] Token 5 Last 3: [0.457408 0.393939 0.447697 ]
--------------------
[layer12_attention_attention_out_core] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer12_attention_attention_out_core] Token 0 First 3: [0.0211487 -0.343994 -0.045105 ]
[layer12_attention_attention_out_core] Token 0 Last 3: [-0.234741 -0.553223 -0.489746 ]
[layer12_attention_attention_out_core] Token 1 First 3: [0.154609 -1.07589 -0.985901 ]
[layer12_attention_attention_out_core] Token 1 Last 3: [0.150593 -1.14358 0.348277 ]
[layer12_attention_attention_out_core] Token 2 First 3: [0.155948 -0.462936 -0.123933 ]
[layer12_attention_attention_out_core] Token 2 Last 3: [0.0274283 -0.418701 -0.294101 ]
--------------------
[layer12_attention_out input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer12_attention_out input] First 3: [0.0211487 -0.343994 -0.045105 ]
[layer12_attention_out input] Last 3: [0.0274283 -0.418701 -0.294101 ]
--------------------
[layer12_attention_out_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer12_attention_out_weight] First 3: [-0.00329454 0.00393142 -0.0167477 ]
[layer12_attention_out_weight] Last 3: [-0.000729236 -0.00140699 -0.00734983 ]
--------------------
[layer12_attention_out] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer12_attention_out] First 3: [-0.041068 -0.0687522 0.245123 ]
[layer12_attention_out] Last 3: [0.148677 -0.0565569 0.0386376 ]
--------------------
[layer12_post_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer12_post_attention_norm] Token 0 First 3: [-1.83269 -18.7862 5.43615 ]
[layer12_post_attention_norm] Token 0 Last 3: [7.26527 1.64656 -7.85444 ]
[layer12_post_attention_norm] Token 1 First 3: [-8.16421 -30.3847 1.24845 ]
[layer12_post_attention_norm] Token 1 Last 3: [9.58983 3.37359 21.0908 ]
[layer12_post_attention_norm] Token 2 First 3: [-6.40251 -28.6099 2.63836 ]
[layer12_post_attention_norm] Token 2 Last 3: [7.12218 -4.66601 6.35927 ]
--------------------
[layer12_post_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer12_post_attention_norm input] Token 0 First 3: [-0.041068 -0.0687522 0.245123 ]
[layer12_post_attention_norm input] Token 0 Last 3: [0.138006 0.0181607 -0.0434244 ]
[layer12_post_attention_norm input] Token 1 First 3: [-0.222672 -0.135344 0.0685175 ]
[layer12_post_attention_norm input] Token 1 Last 3: [0.221714 0.0452882 0.141922 ]
[layer12_post_attention_norm input] Token 2 First 3: [-0.15767 -0.115066 0.130741 ]
[layer12_post_attention_norm input] Token 2 Last 3: [0.148677 -0.0565569 0.0386376 ]
--------------------
[layer12_post_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer12_post_attention_norm_weight] First 3: [7.66818 46.9527 3.81078 ]
[layer12_post_attention_norm_weight] Last 3: [9.0461 15.5794 31.0805 ]
--------------------
[layer12pre_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer12pre_ffn_norm] Token 0 First 3: [3.12554 -0.510005 1.12162 ]
[layer12pre_ffn_norm] Token 0 Last 3: [-0.87527 0.0309043 -0.770713 ]
[layer12pre_ffn_norm] Token 1 First 3: [0.771088 -0.45916 0.488666 ]
[layer12pre_ffn_norm] Token 1 Last 3: [0.359905 -0.729138 -1.38241 ]
[layer12pre_ffn_norm] Token 2 First 3: [2.31868 -0.146022 0.887859 ]
[layer12pre_ffn_norm] Token 2 Last 3: [-0.830375 0.481331 0.352844 ]
--------------------
[layer12_ffn_up input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer12_ffn_up input] First 3: [3.12554 -0.510005 1.12162 ]
[layer12_ffn_up input] Last 3: [-0.830375 0.481331 0.352844 ]
--------------------
[layer12_ffn_up_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer12_ffn_up_weight] First 3: [0.00636492 0.000272753 6.75154e-05 ]
[layer12_ffn_up_weight] Last 3: [-0.00595103 0.0145875 -0.00270721 ]
--------------------
[layer12_ffn_up] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer12_ffn_up] First 3: [0.0212263 0.070386 0.0604088 ]
[layer12_ffn_up] Last 3: [0.235121 0.0145145 0.192782 ]
--------------------
[layer12_ffn_gate input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer12_ffn_gate input] First 3: [3.12554 -0.510005 1.12162 ]
[layer12_ffn_gate input] Last 3: [-0.830375 0.481331 0.352844 ]
--------------------
[layer12_ffn_gate_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer12_ffn_gate_weight] First 3: [-0.00876697 -0.00420667 0.00691812 ]
[layer12_ffn_gate_weight] Last 3: [0.000389155 -0.00396392 -0.000459554 ]
--------------------
[layer12_ffn_gate] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer12_ffn_gate] First 3: [-0.0229958 -0.205303 -0.0895485 ]
[layer12_ffn_gate] Last 3: [-0.134427 -0.264167 -0.231585 ]
--------------------
[layer12_ffn_down input] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer12_ffn_down input] First 3: [-0.000239581 -0.00604998 -0.00251176 ]
[layer12_ffn_down input] Last 3: [-0.0141133 -0.00151772 -0.0182348 ]
--------------------
[layer12_ffn_down_weight] Shape: Shape: 1:1:1152:768 [ FP32 : NCHW ]

[layer12_ffn_down_weight] First 3: [-3.84175e-05 -0.00081664 -0.00440584 ]
[layer12_ffn_down_weight] Last 3: [0.00176467 -0.00391209 0.006388 ]
--------------------
[layer12_ffn_down] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer12_ffn_down] First 3: [0.00206366 -0.000232598 -0.00200268 ]
[layer12_ffn_down] Last 3: [-0.0013265 -0.000655449 -0.00120735 ]
--------------------
[layer12post_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer12post_ffn_norm] Token 0 First 3: [7.99997 -2.43067 -3.08807 ]
[layer12post_ffn_norm] Token 0 Last 3: [-4.91568 4.05066 -9.3948 ]
[layer12post_ffn_norm] Token 1 First 3: [-0.165017 1.19355 -8.24696 ]
[layer12post_ffn_norm] Token 1 Last 3: [-14.4295 1.13325 23.8032 ]
[layer12post_ffn_norm] Token 2 First 3: [5.10771 37.1119 -8.11364 ]
[layer12post_ffn_norm] Token 2 Last 3: [-4.60926 -3.45859 -12.6347 ]
--------------------
[layer13_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer13_attention_norm] Token 0 First 3: [7.68451 -0.814966 0.914296 ]
[layer13_attention_norm] Token 0 Last 3: [-2.15604 0.251037 -1.55007 ]
[layer13_attention_norm] Token 1 First 3: [1.56703 -0.586001 -2.77008 ]
[layer13_attention_norm] Token 1 Last 3: [-1.07953 -0.961602 -1.04123 ]
[layer13_attention_norm] Token 2 First 3: [3.57013 0.246393 -0.603345 ]
[layer13_attention_norm] Token 2 Last 3: [-1.32351 0.367866 0.149061 ]
--------------------
[layer13_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer13_attention_norm input] Token 0 First 3: [176.745 -62.4129 4.2849 ]
[layer13_attention_norm input] Token 0 Last 3: [-34.1798 5.10495 -67.3736 ]
[layer13_attention_norm input] Token 1 First 3: [18.8273 -23.443 -6.7815 ]
[layer13_attention_norm input] Token 1 Last 3: [-8.93977 -10.2148 -23.6409 ]
[layer13_attention_norm input] Token 2 First 3: [103.035 23.6773 -3.54805 ]
[layer13_attention_norm input] Token 2 Last 3: [-26.3275 9.38673 8.12962 ]
--------------------
[layer13_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer13_attention_norm_weight] First 3: [12.7527 3.82999 62.5863 ]
[layer13_attention_norm_weight] Last 3: [18.5021 14.4238 6.74833 ]
--------------------
[layer13_wv input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer13_wv input] First 3: [7.68451 -0.814966 0.914296 ]
[layer13_wv input] Last 3: [-1.32351 0.367866 0.149061 ]
--------------------
[layer13_wv_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer13_wv_weight] First 3: [0.00134092 -0.00367612 -0.00152158 ]
[layer13_wv_weight] Last 3: [-0.00473322 -0.00420514 0.00476151 ]
--------------------
[layer13_wv] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer13_wv] First 3: [0.450347 -0.408101 0.248851 ]
[layer13_wv] Last 3: [-0.0369949 -0.684553 -0.250988 ]
--------------------
[layer13_wk input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer13_wk input] First 3: [7.68451 -0.814966 0.914296 ]
[layer13_wk input] Last 3: [-1.32351 0.367866 0.149061 ]
--------------------
[layer13_wk_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer13_wk_weight] First 3: [-0.0175176 0.0505769 -0.00372174 ]
[layer13_wk_weight] Last 3: [0.000750696 -0.00328934 -0.0150461 ]
--------------------
[layer13_wk] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer13_wk] First 3: [0.00702868 0.119471 0.739097 ]
[layer13_wk] Last 3: [-0.997764 0.79104 1.52809 ]
--------------------
[layer13_k_norm] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer13_k_norm] Token 0 First 3: [0.0040926 0.135879 0.814141 ]
[layer13_k_norm] Token 0 Last 3: [-3.96182 3.80813 4.24583 ]
[layer13_k_norm] Token 1 First 3: [0.141289 0.689828 1.05294 ]
[layer13_k_norm] Token 1 Last 3: [-1.99297 1.06239 4.96306 ]
[layer13_k_norm] Token 2 First 3: [-0.237988 0.406928 1.00128 ]
[layer13_k_norm] Token 2 Last 3: [-2.42644 2.12391 4.6816 ]
--------------------
[layer13_wq input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer13_wq input] First 3: [7.68451 -0.814966 0.914296 ]
[layer13_wq input] Last 3: [-1.32351 0.367866 0.149061 ]
--------------------
[layer13_wq_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer13_wq_weight] First 3: [-0.017354 -0.0141596 -0.00161491 ]
[layer13_wq_weight] Last 3: [-0.0065797 -0.00636051 0.00535873 ]
--------------------
[layer13_wq] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer13_wq] First 3: [0.83647 -0.45318 0.77767 ]
[layer13_wq] Last 3: [-0.276073 -0.00281257 1.05336 ]
--------------------
[layer13_q_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer13_q_norm] Token 0 First 3: [1.74429 -1.10442 1.12245 ]
[layer13_q_norm] Token 0 Last 3: [-0.654731 0.320965 0.781034 ]
[layer13_q_norm] Token 1 First 3: [2.6845 -0.489083 0.559501 ]
[layer13_q_norm] Token 1 Last 3: [-0.210147 -0.398564 0.0764164 ]
[layer13_q_norm] Token 2 First 3: [0.177956 -1.18108 1.82493 ]
[layer13_q_norm] Token 2 Last 3: [-0.386795 -0.00308924 1.1832 ]
--------------------
[layer13_attention_query_rope] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer13_attention_query_rope] Token 0 First 3: [1.74429 -1.10442 1.12245 ]
[layer13_attention_query_rope] Token 0 Last 3: [-0.654731 0.320965 0.781034 ]
[layer13_attention_query_rope] Token 1 First 3: [0.388896 0.41962 0.247382 ]
[layer13_attention_query_rope] Token 1 Last 3: [-0.21011 -0.3986 0.0763675 ]
[layer13_attention_query_rope] Token 2 First 3: [-1.69495 0.723021 0.823148 ]
[layer13_attention_query_rope] Token 2 Last 3: [-0.38635 -0.00310944 1.18307 ]
--------------------
[layer13_attention_key_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer13_attention_value_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer13_attention_attn_probs] Shape: Shape: 1:1:6:3 [ FP32 : NCHW ]

[layer13_attention_attn_probs] Token 0 First 3: [1 1 1 ]
[layer13_attention_attn_probs] Token 0 Last 3: [1 1 1 ]
[layer13_attention_attn_probs] Token 1 First 3: [0.544518 0.560622 0.473494 ]
[layer13_attention_attn_probs] Token 1 Last 3: [0.544518 0.560622 0.473494 ]
[layer13_attention_attn_probs] Token 2 First 3: [0.455482 0.439378 0.526506 ]
[layer13_attention_attn_probs] Token 2 Last 3: [0.455482 0.439378 0.526506 ]
[layer13_attention_attn_probs] Token 3 First 3: [0.159946 0.0347951 0.311582 ]
[layer13_attention_attn_probs] Token 3 Last 3: [0.159946 0.0347951 0.311582 ]
[layer13_attention_attn_probs] Token 4 First 3: [0.0120086 0.00569012 0.0200576 ]
[layer13_attention_attn_probs] Token 4 Last 3: [0.0120086 0.00569012 0.0200576 ]
[layer13_attention_attn_probs] Token 5 First 3: [0.828045 0.959515 0.668361 ]
[layer13_attention_attn_probs] Token 5 Last 3: [0.828045 0.959515 0.668361 ]
--------------------
[layer13_attention_attention_out_core] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer13_attention_attention_out_core] Token 0 First 3: [0.450439 -0.408203 0.248901 ]
[layer13_attention_attention_out_core] Token 0 Last 3: [-0.241333 -0.125366 -0.072937 ]
[layer13_attention_attention_out_core] Token 1 First 3: [-0.252022 -0.640837 0.258298 ]
[layer13_attention_attention_out_core] Token 1 Last 3: [0.304776 -0.241118 0.471662 ]
[layer13_attention_attention_out_core] Token 2 First 3: [0.415949 0.195174 0.414212 ]
[layer13_attention_attention_out_core] Token 2 Last 3: [-0.083952 -0.503526 -0.171185 ]
--------------------
[layer13_attention_out input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer13_attention_out input] First 3: [0.450439 -0.408203 0.248901 ]
[layer13_attention_out input] Last 3: [-0.083952 -0.503526 -0.171185 ]
--------------------
[layer13_attention_out_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer13_attention_out_weight] First 3: [-0.0013225 -0.00461636 0.00393138 ]
[layer13_attention_out_weight] Last 3: [0.0166886 0.00212098 0.00336092 ]
--------------------
[layer13_attention_out] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer13_attention_out] First 3: [-0.152132 -0.0150422 -0.0228775 ]
[layer13_attention_out] Last 3: [0.169451 -0.097159 0.00849789 ]
--------------------
[layer13_post_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer13_post_attention_norm] Token 0 First 3: [-10.064 -4.022 -0.791807 ]
[layer13_post_attention_norm] Token 0 Last 3: [6.61918 4.30748 -4.1948 ]
[layer13_post_attention_norm] Token 1 First 3: [-1.13524 20.8218 1.56611 ]
[layer13_post_attention_norm] Token 1 Last 3: [0.451495 0.36246 -26.0485 ]
[layer13_post_attention_norm] Token 2 First 3: [-8.53718 1.13606 -0.973569 ]
[layer13_post_attention_norm] Token 2 Last 3: [12.9595 -10.2731 2.06776 ]
--------------------
[layer13_post_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer13_post_attention_norm input] Token 0 First 3: [-0.152132 -0.0150422 -0.0228775 ]
[layer13_post_attention_norm input] Token 0 Last 3: [0.101974 0.0479996 -0.020312 ]
[layer13_post_attention_norm input] Token 1 First 3: [-0.0176668 0.0801687 0.0465832 ]
[layer13_post_attention_norm input] Token 1 Last 3: [0.00716075 0.00415808 -0.129851 ]
[layer13_post_attention_norm input] Token 2 First 3: [-0.10953 0.0036061 -0.023874 ]
[layer13_post_attention_norm input] Token 2 Last 3: [0.169451 -0.097159 0.00849789 ]
--------------------
[layer13_post_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer13_post_attention_norm_weight] First 3: [14.8895 60.1815 7.79009 ]
[layer13_post_attention_norm_weight] Last 3: [14.6098 20.1984 46.4825 ]
--------------------
[layer13pre_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer13pre_ffn_norm] Token 0 First 3: [2.9444 -0.383035 0.336273 ]
[layer13pre_ffn_norm] Token 0 Last 3: [-0.787319 0.227136 -0.734002 ]
[layer13pre_ffn_norm] Token 1 First 3: [0.65954 -0.0318934 -1.05955 ]
[layer13pre_ffn_norm] Token 1 Last 3: [-0.511723 -0.501737 -1.07546 ]
[layer13pre_ffn_norm] Token 2 First 3: [1.25622 0.107662 -0.327575 ]
[layer13pre_ffn_norm] Token 2 Last 3: [-0.287385 -0.0160956 0.0787044 ]
--------------------
[layer13_ffn_up input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer13_ffn_up input] First 3: [2.9444 -0.383035 0.336273 ]
[layer13_ffn_up input] Last 3: [-0.287385 -0.0160956 0.0787044 ]
--------------------
[layer13_ffn_up_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer13_ffn_up_weight] First 3: [-0.00322827 0.0019816 0.00433715 ]
[layer13_ffn_up_weight] Last 3: [0.00435903 2.54399e-05 -0.00157865 ]
--------------------
[layer13_ffn_up] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer13_ffn_up] First 3: [-0.0722674 0.0660351 -0.0715293 ]
[layer13_ffn_up] Last 3: [0.0953518 -0.16895 -0.122368 ]
--------------------
[layer13_ffn_gate input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer13_ffn_gate input] First 3: [2.9444 -0.383035 0.336273 ]
[layer13_ffn_gate input] Last 3: [-0.287385 -0.0160956 0.0787044 ]
--------------------
[layer13_ffn_gate_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer13_ffn_gate_weight] First 3: [0.00366497 -0.00690365 -0.00549107 ]
[layer13_ffn_gate_weight] Last 3: [0.00914354 0.00209621 -0.00389371 ]
--------------------
[layer13_ffn_gate] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer13_ffn_gate] First 3: [0.00677617 -0.0996091 0.0739969 ]
[layer13_ffn_gate] Last 3: [0.0680155 -0.0953614 -0.0860769 ]
--------------------
[layer13_ffn_down input] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer13_ffn_down input] First 3: [-0.000246172 -0.0030279 -0.00280258 ]
[layer13_ffn_down input] Last 3: [0.00341854 0.00744365 0.00490526 ]
--------------------
[layer13_ffn_down_weight] Shape: Shape: 1:1:1152:768 [ FP32 : NCHW ]

[layer13_ffn_down_weight] First 3: [-0.00112541 0.00265411 0.00437139 ]
[layer13_ffn_down_weight] Last 3: [-0.00502029 0.0017941 -0.00427417 ]
--------------------
[layer13_ffn_down] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer13_ffn_down] First 3: [0.00421159 -0.0019049 -0.000400433 ]
[layer13_ffn_down] Last 3: [-0.00457554 -0.00264178 0.00226797 ]
--------------------
[layer13post_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer13post_ffn_norm] Token 0 First 3: [17.0554 -20.2011 -0.97275 ]
[layer13post_ffn_norm] Token 0 Last 3: [-3.24873 -14.8482 13.7306 ]
[layer13post_ffn_norm] Token 1 First 3: [15.6521 13.1436 1.43624 ]
[layer13post_ffn_norm] Token 1 Last 3: [-4.10466 -12.1044 25.542 ]
[layer13post_ffn_norm] Token 2 First 3: [35.5101 -10.5333 1.73784 ]
[layer13post_ffn_norm] Token 2 Last 3: [-27.6531 -24.838 42.7159 ]
--------------------
[layer14_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer14_attention_norm] Token 0 First 3: [7.53033 -1.56517 0.402957 ]
[layer14_attention_norm] Token 0 Last 3: [-1.74712 -0.269326 -1.32512 ]
[layer14_attention_norm] Token 1 First 3: [3.50422 0.487451 -1.54934 ]
[layer14_attention_norm] Token 1 Last 3: [-1.83114 -2.78955 -1.41863 ]
[layer14_attention_norm] Token 2 First 3: [3.14695 0.152369 -0.262867 ]
[layer14_attention_norm] Token 2 Last 3: [-1.37388 -0.752764 0.715996 ]
--------------------
[layer14_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer14_attention_norm input] Token 0 First 3: [183.737 -86.636 2.52034 ]
[layer14_attention_norm input] Token 0 Last 3: [-30.8093 -5.43581 -57.8377 ]
[layer14_attention_norm input] Token 1 First 3: [33.3442 10.5224 -3.77915 ]
[layer14_attention_norm input] Token 1 Last 3: [-12.5929 -21.9567 -24.1474 ]
[layer14_attention_norm input] Token 2 First 3: [130.008 14.2801 -2.78378 ]
[layer14_attention_norm input] Token 2 Last 3: [-41.0211 -25.7243 52.9132 ]
--------------------
[layer14_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer14_attention_norm_weight] First 3: [12.1487 5.35519 47.3927 ]
[layer14_attention_norm_weight] Last 3: [16.8094 14.6867 6.79135 ]
--------------------
[layer14_wv input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer14_wv input] First 3: [7.53033 -1.56517 0.402957 ]
[layer14_wv input] Last 3: [-1.37388 -0.752764 0.715996 ]
--------------------
[layer14_wv_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer14_wv_weight] First 3: [0.000983406 -0.017526 0.00522164 ]
[layer14_wv_weight] Last 3: [0.000956143 -0.0131067 0.0016848 ]
--------------------
[layer14_wv] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer14_wv] First 3: [-0.501371 0.923129 0.488396 ]
[layer14_wv] Last 3: [-0.0200702 -0.818834 0.183595 ]
--------------------
[layer14_wk input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer14_wk input] First 3: [7.53033 -1.56517 0.402957 ]
[layer14_wk input] Last 3: [-1.37388 -0.752764 0.715996 ]
--------------------
[layer14_wk_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer14_wk_weight] First 3: [0.0043328 -0.00350942 0.00561904 ]
[layer14_wk_weight] Last 3: [-0.0225372 0.00775027 0.00891551 ]
--------------------
[layer14_wk] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer14_wk] First 3: [-0.220759 -0.0477162 -0.0740662 ]
[layer14_wk] Last 3: [2.06552 -3.5308 0.404639 ]
--------------------
[layer14_k_norm] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer14_k_norm] Token 0 First 3: [-0.225663 -0.0512789 -0.0597275 ]
[layer14_k_norm] Token 0 Last 3: [3.35359 -5.04386 0.0860582 ]
[layer14_k_norm] Token 1 First 3: [0.0687813 -0.501802 -0.0501987 ]
[layer14_k_norm] Token 1 Last 3: [4.49313 -2.67906 -0.894553 ]
[layer14_k_norm] Token 2 First 3: [0.181924 0.0496847 0.00190973 ]
[layer14_k_norm] Token 2 Last 3: [5.05314 -5.2962 0.762684 ]
--------------------
[layer14_wq input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer14_wq input] First 3: [7.53033 -1.56517 0.402957 ]
[layer14_wq input] Last 3: [-1.37388 -0.752764 0.715996 ]
--------------------
[layer14_wq_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer14_wq_weight] First 3: [-0.00164959 0.00352377 0.00227595 ]
[layer14_wq_weight] Last 3: [-0.0216026 -0.00249741 0.00839174 ]
--------------------
[layer14_wq] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer14_wq] First 3: [0.0281226 0.445621 0.257784 ]
[layer14_wq] Last 3: [0.292517 -0.00545329 0.400295 ]
--------------------
[layer14_q_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer14_q_norm] Token 0 First 3: [0.100296 1.0524 0.872912 ]
[layer14_q_norm] Token 0 Last 3: [-0.0874562 -0.139891 -0.177368 ]
[layer14_q_norm] Token 1 First 3: [-2.79991 1.81105 0.688588 ]
[layer14_q_norm] Token 1 Last 3: [0.321821 -1.09233 0.220239 ]
[layer14_q_norm] Token 2 First 3: [-1.63416 -0.550232 0.68288 ]
[layer14_q_norm] Token 2 Last 3: [0.319324 -0.00880147 0.603621 ]
--------------------
[layer14_attention_query_rope] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer14_attention_query_rope] Token 0 First 3: [0.100296 1.0524 0.872912 ]
[layer14_attention_query_rope] Token 0 Last 3: [-0.0874562 -0.139891 -0.177368 ]
[layer14_attention_query_rope] Token 1 First 3: [-1.52058 1.40841 0.457225 ]
[layer14_attention_query_rope] Token 1 Last 3: [0.322039 -1.09234 0.220226 ]
[layer14_attention_query_rope] Token 2 First 3: [-0.0100066 0.346732 -0.0884081 ]
[layer14_attention_query_rope] Token 2 Last 3: [0.319681 -0.00856567 0.60373 ]
--------------------
[layer14_attention_key_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer14_attention_value_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer14_attention_attn_probs] Shape: Shape: 1:1:6:3 [ FP32 : NCHW ]

[layer14_attention_attn_probs] Token 0 First 3: [1 1 1 ]
[layer14_attention_attn_probs] Token 0 Last 3: [1 1 1 ]
[layer14_attention_attn_probs] Token 1 First 3: [0.834822 0.731721 0.691986 ]
[layer14_attention_attn_probs] Token 1 Last 3: [0.834822 0.731721 0.691986 ]
[layer14_attention_attn_probs] Token 2 First 3: [0.165178 0.268279 0.308014 ]
[layer14_attention_attn_probs] Token 2 Last 3: [0.165178 0.268279 0.308014 ]
[layer14_attention_attn_probs] Token 3 First 3: [0.423966 0.282362 0.323137 ]
[layer14_attention_attn_probs] Token 3 Last 3: [0.423966 0.282362 0.323137 ]
[layer14_attention_attn_probs] Token 4 First 3: [0.0557415 0.146854 0.0418541 ]
[layer14_attention_attn_probs] Token 4 Last 3: [0.0557415 0.146854 0.0418541 ]
[layer14_attention_attn_probs] Token 5 First 3: [0.520293 0.570783 0.635009 ]
[layer14_attention_attn_probs] Token 5 Last 3: [0.520293 0.570783 0.635009 ]
--------------------
[layer14_attention_attention_out_core] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer14_attention_attention_out_core] Token 0 First 3: [-0.501465 0.92334 0.488281 ]
[layer14_attention_attention_out_core] Token 0 Last 3: [-0.0491943 0.0586548 0.0966187 ]
[layer14_attention_attention_out_core] Token 1 First 3: [-0.577279 1.50832 0.437812 ]
[layer14_attention_attention_out_core] Token 1 Last 3: [0.0481504 0.637064 0.293056 ]
[layer14_attention_attention_out_core] Token 2 First 3: [-0.0364802 0.307283 0.522568 ]
[layer14_attention_attention_out_core] Token 2 Last 3: [-0.0174696 -0.419971 0.178541 ]
--------------------
[layer14_attention_out input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer14_attention_out input] First 3: [-0.501465 0.92334 0.488281 ]
[layer14_attention_out input] Last 3: [-0.0174696 -0.419971 0.178541 ]
--------------------
[layer14_attention_out_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer14_attention_out_weight] First 3: [-0.00621829 0.00860628 0.00564968 ]
[layer14_attention_out_weight] Last 3: [0.0194785 0.00314107 0.0106948 ]
--------------------
[layer14_attention_out] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer14_attention_out] First 3: [0.122671 -0.0228615 -0.013898 ]
[layer14_attention_out] Last 3: [-0.0209082 -0.0470277 0.0444366 ]
--------------------
[layer14_post_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer14_post_attention_norm] Token 0 First 3: [8.62522 -7.86113 -0.869933 ]
[layer14_post_attention_norm] Token 0 Last 3: [0.283154 8.84716 -5.5446 ]
[layer14_post_attention_norm] Token 1 First 3: [3.94478 4.51712 -1.11924 ]
[layer14_post_attention_norm] Token 1 Last 3: [-3.52429 15.0293 -6.3172 ]
[layer14_post_attention_norm] Token 2 First 3: [9.45089 -16.6814 -3.39178 ]
[layer14_post_attention_norm] Token 2 Last 3: [-1.60938 -6.47258 12.8397 ]
--------------------
[layer14_post_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer14_post_attention_norm input] Token 0 First 3: [0.122671 -0.0228615 -0.013898 ]
[layer14_post_attention_norm input] Token 0 Last 3: [0.00332776 0.0581504 -0.0173591 ]
[layer14_post_attention_norm input] Token 1 First 3: [0.065624 0.0153656 -0.0209149 ]
[layer14_post_attention_norm input] Token 1 Last 3: [-0.0484471 0.115546 -0.0231339 ]
[layer14_post_attention_norm input] Token 2 First 3: [0.148585 -0.0536268 -0.0598993 ]
[layer14_post_attention_norm input] Token 2 Last 3: [-0.0209082 -0.0470277 0.0444366 ]
--------------------
[layer14_post_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer14_post_attention_norm_weight] First 3: [13.0734 63.9356 11.6385 ]
[layer14_post_attention_norm_weight] Last 3: [15.821 28.2888 59.3888 ]
--------------------
[layer14pre_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer14pre_ffn_norm] Token 0 First 3: [2.36972 -0.363547 0.0828936 ]
[layer14pre_ffn_norm] Token 0 Last 3: [-0.569277 0.0495546 -0.424137 ]
[layer14pre_ffn_norm] Token 1 First 3: [1.14927 0.144757 -0.615529 ]
[layer14pre_ffn_norm] Token 1 Last 3: [-0.751981 -0.251764 -0.510034 ]
[layer14pre_ffn_norm] Token 2 First 3: [1.0205 -0.00548754 -0.184245 ]
[layer14pre_ffn_norm] Token 2 Last 3: [-0.47224 -0.277819 0.261363 ]
--------------------
[layer14_ffn_up input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer14_ffn_up input] First 3: [2.36972 -0.363547 0.0828936 ]
[layer14_ffn_up input] Last 3: [-0.47224 -0.277819 0.261363 ]
--------------------
[layer14_ffn_up_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer14_ffn_up_weight] First 3: [0.00849918 -0.00378022 -0.00202073 ]
[layer14_ffn_up_weight] Last 3: [0.00644861 -0.00430276 -0.00051768 ]
--------------------
[layer14_ffn_up] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer14_ffn_up] First 3: [0.0570615 -0.0037175 -0.111223 ]
[layer14_ffn_up] Last 3: [-0.00688198 0.0632586 -0.0481527 ]
--------------------
[layer14_ffn_gate input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer14_ffn_gate input] First 3: [2.36972 -0.363547 0.0828936 ]
[layer14_ffn_gate input] Last 3: [-0.47224 -0.277819 0.261363 ]
--------------------
[layer14_ffn_gate_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer14_ffn_gate_weight] First 3: [0.00158799 0.0050692 -0.00124136 ]
[layer14_ffn_gate_weight] Last 3: [0.00408048 -0.000350579 -0.000346993 ]
--------------------
[layer14_ffn_gate] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer14_ffn_gate] First 3: [0.148339 0.128315 -0.0201319 ]
[layer14_ffn_gate] Last 3: [-0.0240609 -0.0416593 -0.0109781 ]
--------------------
[layer14_ffn_down input] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer14_ffn_down input] First 3: [0.00473131 -0.000262856 0.00110159 ]
[layer14_ffn_down input] Last 3: [8.12039e-05 -0.00127387 0.000261998 ]
--------------------
[layer14_ffn_down_weight] Shape: Shape: 1:1:1152:768 [ FP32 : NCHW ]

[layer14_ffn_down_weight] First 3: [0.000924729 -0.00342065 0.00396971 ]
[layer14_ffn_down_weight] Last 3: [0.00578885 0.00331287 0.00014759 ]
--------------------
[layer14_ffn_down] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer14_ffn_down] First 3: [-0.00129872 -0.000898977 0.000272518 ]
[layer14_ffn_down] Last 3: [-0.00152197 0.000618443 0.000548885 ]
--------------------
[layer14post_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer14post_ffn_norm] Token 0 First 3: [-14.9323 -29.444 2.95015 ]
[layer14post_ffn_norm] Token 0 Last 3: [22.2811 5.98724 -20.3012 ]
[layer14post_ffn_norm] Token 1 First 3: [-7.03124 -6.42041 6.69584 ]
[layer14post_ffn_norm] Token 1 Last 3: [-8.84538 33.0534 69.4581 ]
[layer14post_ffn_norm] Token 2 First 3: [-9.54853 54.1056 7.32891 ]
[layer14post_ffn_norm] Token 2 Last 3: [-27.5137 19.4688 34.3503 ]
--------------------
[layer15_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer15_attention_norm] Token 0 First 3: [8.55668 -1.67657 0.496862 ]
[layer15_attention_norm] Token 0 Last 3: [-0.505703 0.355356 -1.56573 ]
[layer15_attention_norm] Token 1 First 3: [3.60848 0.288321 0.480055 ]
[layer15_attention_norm] Token 1 Last 3: [-3.78615 2.44277 1.80417 ]
[layer15_attention_norm] Token 2 First 3: [2.85125 0.318306 0.0566888 ]
[layer15_attention_norm] Token 2 Last 3: [-1.95796 -0.219017 0.852388 ]
--------------------
[layer15_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer15_attention_norm input] Token 0 First 3: [177.43 -123.941 4.60056 ]
[layer15_attention_norm input] Token 0 Last 3: [-8.2451 9.39859 -83.6836 ]
[layer15_attention_norm input] Token 1 First 3: [30.2578 8.61909 1.79745 ]
[layer15_attention_norm input] Token 1 Last 3: [-24.9626 26.126 38.9934 ]
[layer15_attention_norm input] Token 2 First 3: [129.911 51.7043 1.15335 ]
[layer15_attention_norm input] Token 2 Last 3: [-70.1442 -12.7281 100.103 ]
--------------------
[layer15_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer15_attention_norm_weight] First 3: [15.4535 4.33466 34.6077 ]
[layer15_attention_norm_weight] Last 3: [19.6539 12.1157 5.9955 ]
--------------------
[layer15_wv input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer15_wv input] First 3: [8.55668 -1.67657 0.496862 ]
[layer15_wv input] Last 3: [-1.95796 -0.219017 0.852388 ]
--------------------
[layer15_wv_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer15_wv_weight] First 3: [0.000801453 0.00784792 -0.00323226 ]
[layer15_wv_weight] Last 3: [0.00165266 0.0191337 0.00701328 ]
--------------------
[layer15_wv] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer15_wv] First 3: [-0.473626 -0.180081 -0.417157 ]
[layer15_wv] Last 3: [-0.0248909 -0.141291 0.151373 ]
--------------------
[layer15_wk input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer15_wk input] First 3: [8.55668 -1.67657 0.496862 ]
[layer15_wk input] Last 3: [-1.95796 -0.219017 0.852388 ]
--------------------
[layer15_wk_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer15_wk_weight] First 3: [0.00375537 -0.0216002 -0.00292443 ]
[layer15_wk_weight] Last 3: [-0.00584284 0.012955 -0.0033046 ]
--------------------
[layer15_wk] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer15_wk] First 3: [-0.295137 1.17686 0.00849073 ]
[layer15_wk] Last 3: [-0.840831 0.0865268 0.338033 ]
--------------------
[layer15_k_norm] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer15_k_norm] Token 0 First 3: [-0.427202 -0.25167 0.0114703 ]
[layer15_k_norm] Token 0 Last 3: [-4.46135 1.54689 3.252 ]
[layer15_k_norm] Token 1 First 3: [-1.25998 0.0126564 0.318515 ]
[layer15_k_norm] Token 1 Last 3: [-2.65085 2.93384 7.9968 ]
[layer15_k_norm] Token 2 First 3: [-0.154048 -0.11114 0.0115977 ]
[layer15_k_norm] Token 2 Last 3: [-4.59517 0.289129 2.70115 ]
--------------------
[layer15_wq input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer15_wq input] First 3: [8.55668 -1.67657 0.496862 ]
[layer15_wq input] Last 3: [-1.95796 -0.219017 0.852388 ]
--------------------
[layer15_wq_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer15_wq_weight] First 3: [-0.00521574 0.0174554 0.0011504 ]
[layer15_wq_weight] Last 3: [-0.0190411 0.0210189 0.0100511 ]
--------------------
[layer15_wq] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer15_wq] First 3: [0.275073 -3.55045 -0.622397 ]
[layer15_wq] Last 3: [-0.150215 -0.475758 -0.18292 ]
--------------------
[layer15_q_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer15_q_norm] Token 0 First 3: [0.376531 0.572771 -1.02765 ]
[layer15_q_norm] Token 0 Last 3: [0.0542783 1.43787 -0.451127 ]
[layer15_q_norm] Token 1 First 3: [1.21904 0.330558 -1.70142 ]
[layer15_q_norm] Token 1 Last 3: [-0.224476 0.620965 0.193768 ]
[layer15_q_norm] Token 2 First 3: [0.442123 0.372712 -0.411199 ]
[layer15_q_norm] Token 2 Last 3: [-0.197788 -1.12925 -0.174871 ]
--------------------
[layer15_attention_query_rope] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer15_attention_query_rope] Token 0 First 3: [0.376531 0.572771 -1.02765 ]
[layer15_attention_query_rope] Token 0 Last 3: [0.0542783 1.43787 -0.451127 ]
[layer15_attention_query_rope] Token 1 First 3: [-0.120998 0.443342 -0.517352 ]
[layer15_attention_query_rope] Token 1 Last 3: [-0.224368 0.620847 0.193919 ]
[layer15_attention_query_rope] Token 2 First 3: [-0.211726 -0.0888874 1.3041 ]
[layer15_attention_query_rope] Token 2 Last 3: [-0.19769 -1.12912 -0.175401 ]
--------------------
[layer15_attention_key_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer15_attention_value_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer15_attention_attn_probs] Shape: Shape: 1:1:6:3 [ FP32 : NCHW ]

[layer15_attention_attn_probs] Token 0 First 3: [1 1 1 ]
[layer15_attention_attn_probs] Token 0 Last 3: [1 1 1 ]
[layer15_attention_attn_probs] Token 1 First 3: [0.416824 0.566088 0.327463 ]
[layer15_attention_attn_probs] Token 1 Last 3: [0.416824 0.566088 0.327463 ]
[layer15_attention_attn_probs] Token 2 First 3: [0.583176 0.433912 0.672537 ]
[layer15_attention_attn_probs] Token 2 Last 3: [0.583176 0.433912 0.672537 ]
[layer15_attention_attn_probs] Token 3 First 3: [0.367736 0.102812 0.0888121 ]
[layer15_attention_attn_probs] Token 3 Last 3: [0.367736 0.102812 0.0888121 ]
[layer15_attention_attn_probs] Token 4 First 3: [0.0151369 0.00962795 0.0355887 ]
[layer15_attention_attn_probs] Token 4 Last 3: [0.0151369 0.00962795 0.0355887 ]
[layer15_attention_attn_probs] Token 5 First 3: [0.617127 0.88756 0.875599 ]
[layer15_attention_attn_probs] Token 5 Last 3: [0.617127 0.88756 0.875599 ]
--------------------
[layer15_attention_attention_out_core] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer15_attention_attention_out_core] Token 0 First 3: [-0.473633 -0.180054 -0.417236 ]
[layer15_attention_attention_out_core] Token 0 Last 3: [-0.159668 -0.325195 0.0556641 ]
[layer15_attention_attention_out_core] Token 1 First 3: [-0.158731 -0.134885 -0.874409 ]
[layer15_attention_attention_out_core] Token 1 Last 3: [1.0905 -1.55927 1.81779 ]
[layer15_attention_attention_out_core] Token 2 First 3: [-0.132262 0.0614306 -0.153733 ]
[layer15_attention_attention_out_core] Token 2 Last 3: [0.0245015 -0.229424 0.232708 ]
--------------------
[layer15_attention_out input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer15_attention_out input] First 3: [-0.473633 -0.180054 -0.417236 ]
[layer15_attention_out input] Last 3: [0.0245015 -0.229424 0.232708 ]
--------------------
[layer15_attention_out_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer15_attention_out_weight] First 3: [-0.000731818 -0.00764025 -0.00435154 ]
[layer15_attention_out_weight] Last 3: [0.00714437 0.00726864 0.00573115 ]
--------------------
[layer15_attention_out] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer15_attention_out] First 3: [-0.0534952 0.0290359 0.0453281 ]
[layer15_attention_out] Last 3: [0.00386682 -0.0209326 0.0568579 ]
--------------------
[layer15_post_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer15_post_attention_norm] Token 0 First 3: [-5.06143 7.70283 4.61465 ]
[layer15_post_attention_norm] Token 0 Last 3: [-7.2406 -6.90268 26.1049 ]
[layer15_post_attention_norm] Token 1 First 3: [2.6171 -15.4089 12.9529 ]
[layer15_post_attention_norm] Token 1 Last 3: [-32.1167 21.3015 -43.7037 ]
[layer15_post_attention_norm] Token 2 First 3: [-4.36528 0.550305 7.04139 ]
[layer15_post_attention_norm] Token 2 Last 3: [0.772964 -6.13477 34.089 ]
--------------------
[layer15_post_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer15_post_attention_norm input] Token 0 First 3: [-0.0534952 0.0290359 0.0453281 ]
[layer15_post_attention_norm input] Token 0 Last 3: [-0.063 -0.040965 0.0757305 ]
[layer15_post_attention_norm input] Token 1 First 3: [0.0387356 -0.0813401 0.178173 ]
[layer15_post_attention_norm input] Token 1 Last 3: [-0.391331 0.177032 -0.177547 ]
[layer15_post_attention_norm input] Token 2 First 3: [-0.0265267 0.00119266 0.0397664 ]
[layer15_post_attention_norm input] Token 2 Last 3: [0.00386682 -0.0209326 0.0568579 ]
--------------------
[layer15_post_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer15_post_attention_norm_weight] First 3: [27.8509 78.0902 29.9677 ]
[layer15_post_attention_norm_weight] Last 3: [33.8311 49.6005 101.469 ]
--------------------
[layer15pre_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer15pre_ffn_norm] Token 0 First 3: [2.4196 -0.429228 0.262687 ]
[layer15pre_ffn_norm] Token 0 Last 3: [-0.270498 0.0315472 -0.338963 ]
[layer15pre_ffn_norm] Token 1 First 3: [1.17893 -0.0640526 1.07417 ]
[layer15pre_ffn_norm] Token 1 Last 3: [-2.54713 1.53144 -0.0708398 ]
[layer15pre_ffn_norm] Token 2 First 3: [0.785794 0.0860372 0.104157 ]
[layer15pre_ffn_norm] Token 2 Last 3: [-0.540301 -0.106307 0.352242 ]
--------------------
[layer15_ffn_up input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer15_ffn_up input] First 3: [2.4196 -0.429228 0.262687 ]
[layer15_ffn_up input] Last 3: [-0.540301 -0.106307 0.352242 ]
--------------------
[layer15_ffn_up_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer15_ffn_up_weight] First 3: [-0.00466014 -0.00184233 -0.00349891 ]
[layer15_ffn_up_weight] Last 3: [0.00211317 0.00686908 -0.00714824 ]
--------------------
[layer15_ffn_up] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer15_ffn_up] First 3: [-0.0373146 -0.0580113 -0.00879719 ]
[layer15_ffn_up] Last 3: [-0.0648467 -0.00303104 0.0824965 ]
--------------------
[layer15_ffn_gate input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer15_ffn_gate input] First 3: [2.4196 -0.429228 0.262687 ]
[layer15_ffn_gate input] Last 3: [-0.540301 -0.106307 0.352242 ]
--------------------
[layer15_ffn_gate_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer15_ffn_gate_weight] First 3: [0.00345655 -0.000480662 -0.00664235 ]
[layer15_ffn_gate_weight] Last 3: [0.00824638 0.0020995 -0.00344564 ]
--------------------
[layer15_ffn_gate] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer15_ffn_gate] First 3: [-0.0186412 0.356465 0.135001 ]
[layer15_ffn_gate] Last 3: [-0.05292 0.0603688 -0.0388194 ]
--------------------
[layer15_ffn_down input] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer15_ffn_down input] First 3: [0.000342622 -0.0132189 -0.000657582 ]
[layer15_ffn_down input] Last 3: [0.00164343 -9.58945e-05 -0.00155165 ]
--------------------
[layer15_ffn_down_weight] Shape: Shape: 1:1:1152:768 [ FP32 : NCHW ]

[layer15_ffn_down_weight] First 3: [-0.00382673 -0.0014606 -0.00181644 ]
[layer15_ffn_down_weight] Last 3: [0.004815 -0.00163221 0.00742596 ]
--------------------
[layer15_ffn_down] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer15_ffn_down] First 3: [0.00145903 0.000823393 -6.07914e-05 ]
[layer15_ffn_down] Last 3: [-0.000564741 0.000843581 -0.000540338 ]
--------------------
[layer15post_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer15post_ffn_norm] Token 0 First 3: [24.4536 33.4186 -1.15356 ]
[layer15post_ffn_norm] Token 0 Last 3: [5.33883 12.0198 -132.328 ]
[layer15post_ffn_norm] Token 1 First 3: [35.8425 15.3046 -9.57456 ]
[layer15post_ffn_norm] Token 1 Last 3: [-28.1796 52.0765 -21.632 ]
[layer15post_ffn_norm] Token 2 First 3: [3.11029 3.35217 -2.46867 ]
[layer15post_ffn_norm] Token 2 Last 3: [-21.458 44.5914 -57.7463 ]
--------------------
[layer16_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer16_attention_norm] Token 0 First 3: [7.86288 -1.82643 0.422075 ]
[layer16_attention_norm] Token 0 Last 3: [-0.475372 0.462867 -2.98224 ]
[layer16_attention_norm] Token 1 First 3: [5.99049 0.409762 0.591331 ]
[layer16_attention_norm] Token 1 Last 3: [-8.71623 6.92382 -0.902699 ]
[layer16_attention_norm] Token 2 First 3: [2.01195 0.480042 0.117355 ]
[layer16_attention_norm] Token 2 Last 3: [-1.66574 0.321154 0.469933 ]
--------------------
[layer16_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer16_attention_norm input] Token 0 First 3: [196.822 -82.8196 8.06165 ]
[layer16_attention_norm input] Token 0 Last 3: [-10.1469 14.5157 -189.907 ]
[layer16_attention_norm input] Token 1 First 3: [68.7174 8.51479 5.1758 ]
[layer16_attention_norm input] Token 1 Last 3: [-85.2589 99.504 -26.3423 ]
[layer16_attention_norm input] Token 2 First 3: [128.656 55.6068 5.72607 ]
[layer16_attention_norm input] Token 2 Last 3: [-90.8293 25.7285 76.4458 ]
--------------------
[layer16_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer16_attention_norm_weight] First 3: [12.1848 6.72637 15.9689 ]
[layer16_attention_norm_weight] Last 3: [14.2893 9.72587 4.78974 ]
--------------------
[layer16_wv input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer16_wv input] First 3: [7.86288 -1.82643 0.422075 ]
[layer16_wv input] Last 3: [-1.66574 0.321154 0.469933 ]
--------------------
[layer16_wv_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer16_wv_weight] First 3: [0.00627374 -0.0111171 0.00308275 ]
[layer16_wv_weight] Last 3: [-0.000764425 0.00161612 -0.00157658 ]
--------------------
[layer16_wv] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer16_wv] First 3: [-0.641833 -0.392894 1.11509 ]
[layer16_wv] Last 3: [0.0724786 -0.108661 0.033978 ]
--------------------
[layer16_wk input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer16_wk input] First 3: [7.86288 -1.82643 0.422075 ]
[layer16_wk input] Last 3: [-1.66574 0.321154 0.469933 ]
--------------------
[layer16_wk_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer16_wk_weight] First 3: [-0.00166151 0.0105088 -0.00615557 ]
[layer16_wk_weight] Last 3: [0.00557434 -0.00692926 0.00394461 ]
--------------------
[layer16_wk] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer16_wk] First 3: [-1.04315 -0.685682 -0.153062 ]
[layer16_wk] Last 3: [1.05636 0.053039 -0.183043 ]
--------------------
[layer16_k_norm] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer16_k_norm] Token 0 First 3: [-2.33292 -1.27282 -0.00118006 ]
[layer16_k_norm] Token 0 Last 3: [5.87892 0.218635 -1.78973 ]
[layer16_k_norm] Token 1 First 3: [-4.29665 -1.40755 -0.00263111 ]
[layer16_k_norm] Token 1 Last 3: [7.40418 0.314921 -0.19188 ]
[layer16_k_norm] Token 2 First 3: [-0.421691 -0.377202 -0.000516195 ]
[layer16_k_norm] Token 2 Last 3: [5.90742 0.178202 -0.855051 ]
--------------------
[layer16_wq input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer16_wq input] First 3: [7.86288 -1.82643 0.422075 ]
[layer16_wq input] Last 3: [-1.66574 0.321154 0.469933 ]
--------------------
[layer16_wq_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer16_wq_weight] First 3: [0.00756079 -0.00976691 0.0259007 ]
[layer16_wq_weight] Last 3: [0.0052072 0.00457251 -0.00834888 ]
--------------------
[layer16_wq] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer16_wq] First 3: [0.0821783 -0.0387773 0.168281 ]
[layer16_wq] Last 3: [0.266051 0.111877 -0.125343 ]
--------------------
[layer16_q_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer16_q_norm] Token 0 First 3: [0.140662 -0.0579851 0.000603074 ]
[layer16_q_norm] Token 0 Last 3: [-0.32312 0.188211 0.0139937 ]
[layer16_q_norm] Token 1 First 3: [0.304319 0.680395 0.00883056 ]
[layer16_q_norm] Token 1 Last 3: [0.713871 -0.661022 -0.817628 ]
[layer16_q_norm] Token 2 First 3: [-0.0468968 0.17708 0.0134037 ]
[layer16_q_norm] Token 2 Last 3: [0.488007 0.356864 -0.272889 ]
--------------------
[layer16_attention_query_rope] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer16_attention_query_rope] Token 0 First 3: [0.140662 -0.0579851 0.000603074 ]
[layer16_attention_query_rope] Token 0 Last 3: [-0.32312 0.188211 0.0139937 ]
[layer16_attention_query_rope] Token 1 First 3: [-0.0525094 0.464852 0.417051 ]
[layer16_attention_query_rope] Token 1 Last 3: [0.713809 -0.661043 -0.817511 ]
[layer16_attention_query_rope] Token 2 First 3: [-0.00479791 0.0205707 -0.206194 ]
[layer16_attention_query_rope] Token 2 Last 3: [0.488116 0.356752 -0.272725 ]
--------------------
[layer16_attention_key_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer16_attention_value_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer16_attention_attn_probs] Shape: Shape: 1:1:6:3 [ FP32 : NCHW ]

[layer16_attention_attn_probs] Token 0 First 3: [1 1 1 ]
[layer16_attention_attn_probs] Token 0 Last 3: [1 1 1 ]
[layer16_attention_attn_probs] Token 1 First 3: [0.106826 0.12938 0.00878935 ]
[layer16_attention_attn_probs] Token 1 Last 3: [0.106826 0.12938 0.00878935 ]
[layer16_attention_attn_probs] Token 2 First 3: [0.893174 0.87062 0.991211 ]
[layer16_attention_attn_probs] Token 2 Last 3: [0.893174 0.87062 0.991211 ]
[layer16_attention_attn_probs] Token 3 First 3: [0.021309 0.0178118 0.00653055 ]
[layer16_attention_attn_probs] Token 3 Last 3: [0.021309 0.0178118 0.00653055 ]
[layer16_attention_attn_probs] Token 4 First 3: [0.0187739 0.0262016 0.00265254 ]
[layer16_attention_attn_probs] Token 4 Last 3: [0.0187739 0.0262016 0.00265254 ]
[layer16_attention_attn_probs] Token 5 First 3: [0.959917 0.955987 0.990817 ]
[layer16_attention_attn_probs] Token 5 Last 3: [0.959917 0.955987 0.990817 ]
--------------------
[layer16_attention_attention_out_core] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer16_attention_attention_out_core] Token 0 First 3: [-0.641602 -0.392822 1.11523 ]
[layer16_attention_attention_out_core] Token 0 Last 3: [-0.00419998 -0.228394 0.377197 ]
[layer16_attention_attention_out_core] Token 1 First 3: [-0.0387748 0.665423 1.26439 ]
[layer16_attention_attention_out_core] Token 1 Last 3: [-1.06094 -0.501485 0.778667 ]
[layer16_attention_attention_out_core] Token 2 First 3: [0.241229 -0.171494 -0.217453 ]
[layer16_attention_attention_out_core] Token 2 Last 3: [0.068917 -0.110473 0.0381923 ]
--------------------
[layer16_attention_out input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer16_attention_out input] First 3: [-0.641602 -0.392822 1.11523 ]
[layer16_attention_out input] Last 3: [0.068917 -0.110473 0.0381923 ]
--------------------
[layer16_attention_out_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer16_attention_out_weight] First 3: [0.000699805 -0.00923346 0.0056288 ]
[layer16_attention_out_weight] Last 3: [-0.00144535 0.000504394 0.0137554 ]
--------------------
[layer16_attention_out] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer16_attention_out] First 3: [-0.0424633 -0.00942039 -0.058746 ]
[layer16_attention_out] Last 3: [0.0188804 -0.0385151 0.0650631 ]
--------------------
[layer16_post_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer16_post_attention_norm] Token 0 First 3: [-6.5458 -7.08409 -14.3522 ]
[layer16_post_attention_norm] Token 0 Last 3: [14.4993 12.7688 47.9866 ]
[layer16_post_attention_norm] Token 1 First 3: [0.254828 -56.9416 -21.9961 ]
[layer16_post_attention_norm] Token 1 Last 3: [17.7504 15.9612 -39.3986 ]
[layer16_post_attention_norm] Token 2 First 3: [1.23666 -54.6575 -4.56588 ]
[layer16_post_attention_norm] Token 2 Last 3: [8.07647 -16.2068 48.7014 ]
--------------------
[layer16_post_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer16_post_attention_norm input] Token 0 First 3: [-0.0424633 -0.00942039 -0.058746 ]
[layer16_post_attention_norm input] Token 0 Last 3: [0.0429082 0.0384136 0.081155 ]
[layer16_post_attention_norm input] Token 1 First 3: [0.00256609 -0.117541 -0.139759 ]
[layer16_post_attention_norm input] Token 1 Last 3: [0.0815406 0.0745377 -0.103431 ]
[layer16_post_attention_norm input] Token 2 First 3: [0.0063372 -0.0574159 -0.0147633 ]
[layer16_post_attention_norm input] Token 2 Last 3: [0.0188804 -0.0385151 0.0650631 ]
--------------------
[layer16_post_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer16_post_attention_norm_weight] First 3: [21.2607 103.715 33.6952 ]
[layer16_post_attention_norm_weight] Last 3: [46.6054 45.8452 81.5516 ]
--------------------
[layer16pre_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer16pre_ffn_norm] Token 0 First 3: [4.54659 -0.517099 -0.170364 ]
[layer16pre_ffn_norm] Token 0 Last 3: [0.101275 0.446302 -1.11703 ]
[layer16pre_ffn_norm] Token 1 First 3: [3.26907 -0.552498 -0.903587 ]
[layer16pre_ffn_norm] Token 1 Last 3: [-3.11584 3.74638 -1.02637 ]
[layer16pre_ffn_norm] Token 2 First 3: [1.08681 0.00191195 0.0110023 ]
[layer16pre_ffn_norm] Token 2 Last 3: [-0.674249 0.0545375 0.344913 ]
--------------------
[layer16_ffn_up input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer16_ffn_up input] First 3: [4.54659 -0.517099 -0.170364 ]
[layer16_ffn_up input] Last 3: [-0.674249 0.0545375 0.344913 ]
--------------------
[layer16_ffn_up_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer16_ffn_up_weight] First 3: [-0.0067926 -0.00632937 -0.000757392 ]
[layer16_ffn_up_weight] Last 3: [0.00783318 -0.00428569 -0.014335 ]
--------------------
[layer16_ffn_up] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer16_ffn_up] First 3: [-0.0940079 0.0467099 -0.364863 ]
[layer16_ffn_up] Last 3: [-0.0487563 -0.131461 -0.107071 ]
--------------------
[layer16_ffn_gate input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer16_ffn_gate input] First 3: [4.54659 -0.517099 -0.170364 ]
[layer16_ffn_gate input] Last 3: [-0.674249 0.0545375 0.344913 ]
--------------------
[layer16_ffn_gate_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer16_ffn_gate_weight] First 3: [-0.00595434 0.00731773 0.0101548 ]
[layer16_ffn_gate_weight] Last 3: [0.0125881 0.00208253 0.00569225 ]
--------------------
[layer16_ffn_gate] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer16_ffn_gate] First 3: [-0.131483 -0.163901 -0.610438 ]
[layer16_ffn_gate] Last 3: [-0.0548495 -0.10166 0.0530718 ]
--------------------
[layer16_ffn_down input] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer16_ffn_down input] First 3: [0.00553375 -0.00332956 0.0603239 ]
[layer16_ffn_down input] Last 3: [0.00127864 0.00614108 -0.00296149 ]
--------------------
[layer16_ffn_down_weight] Shape: Shape: 1:1:1152:768 [ FP32 : NCHW ]

[layer16_ffn_down_weight] First 3: [0.00934299 0.00235736 0.00546526 ]
[layer16_ffn_down_weight] Last 3: [-0.00287544 0.00169369 0.0010951 ]
--------------------
[layer16_ffn_down] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer16_ffn_down] First 3: [0.0181418 0.00553668 -0.00724509 ]
[layer16_ffn_down] Last 3: [-0.00335296 0.000168297 0.000963791 ]
--------------------
[layer16post_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer16post_ffn_norm] Token 0 First 3: [33.8896 32.7639 -20.2567 ]
[layer16post_ffn_norm] Token 0 Last 3: [-76.941 -3.25433 -60.9183 ]
[layer16post_ffn_norm] Token 1 First 3: [20.1836 204.287 -34.3407 ]
[layer16post_ffn_norm] Token 1 Last 3: [3.29938 -19.2524 59.0428 ]
[layer16post_ffn_norm] Token 2 First 3: [56.4852 -20.351 -171.854 ]
[layer16post_ffn_norm] Token 2 Last 3: [-62.2623 3.91927 43.7454 ]
--------------------
[layer17_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer17_attention_norm] Token 0 First 3: [13.7558 -1.4549 -1.11017 ]
[layer17_attention_norm] Token 0 Last 3: [-2.61371 0.701168 -3.36007 ]
[layer17_attention_norm] Token 1 First 3: [13.5677 9.84161 -5.30574 ]
[layer17_attention_norm] Token 1 Last 3: [-5.73357 6.96202 -0.275162 ]
[layer17_attention_norm] Token 2 First 3: [6.16318 -0.26621 -3.84664 ]
[layer17_attention_norm] Token 2 Last 3: [-2.81383 0.211344 1.50766 ]
--------------------
[layer17_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer17_attention_norm input] Token 0 First 3: [224.166 -57.1399 -26.5472 ]
[layer17_attention_norm input] Token 0 Last 3: [-72.5885 24.0302 -202.839 ]
[layer17_attention_norm input] Token 1 First 3: [89.1558 155.86 -51.161 ]
[layer17_attention_norm input] Token 1 Last 3: [-64.2092 96.2128 -6.6981 ]
[layer17_attention_norm input] Token 2 First 3: [186.377 -19.4016 -170.694 ]
[layer17_attention_norm input] Token 2 Last 3: [-145.015 13.441 168.893 ]
--------------------
[layer17_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer17_attention_norm_weight] First 3: [21.2826 8.83081 14.5036 ]
[layer17_attention_norm_weight] Last 3: [12.4881 10.1198 5.7452 ]
--------------------
[layer17_wv input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer17_wv input] First 3: [13.7558 -1.4549 -1.11017 ]
[layer17_wv input] Last 3: [-2.81383 0.211344 1.50766 ]
--------------------
[layer17_wv_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer17_wv_weight] First 3: [-0.0107623 -0.00677671 0.00768865 ]
[layer17_wv_weight] Last 3: [-0.0185697 -0.00054755 0.00865621 ]
--------------------
[layer17_wv] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer17_wv] First 3: [1.88528 -0.435991 -1.0771 ]
[layer17_wv] Last 3: [-0.222606 0.237174 -1.18271 ]
--------------------
[layer17_wk input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer17_wk input] First 3: [13.7558 -1.4549 -1.11017 ]
[layer17_wk input] Last 3: [-2.81383 0.211344 1.50766 ]
--------------------
[layer17_wk_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer17_wk_weight] First 3: [-0.00595938 -0.00122833 0.00386595 ]
[layer17_wk_weight] Last 3: [-0.00189477 0.00473258 0.00159939 ]
--------------------
[layer17_wk] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer17_wk] First 3: [0.350494 -3.18035 0.233483 ]
[layer17_wk] Last 3: [8.95427 -0.340526 4.96275 ]
--------------------
[layer17_k_norm] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer17_k_norm] Token 0 First 3: [0.134795 -0.0200962 0.0479619 ]
[layer17_k_norm] Token 0 Last 3: [26.9288 -6.90643 1.81781 ]
[layer17_k_norm] Token 1 First 3: [-0.437252 0.00202921 0.262349 ]
[layer17_k_norm] Token 1 Last 3: [27.8714 -20.1325 -9.60493 ]
[layer17_k_norm] Token 2 First 3: [0.0659467 -0.00308178 0.00210185 ]
[layer17_k_norm] Token 2 Last 3: [29.9851 -0.83068 11.9719 ]
--------------------
[layer17_wq input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer17_wq input] First 3: [13.7558 -1.4549 -1.11017 ]
[layer17_wq input] Last 3: [-2.81383 0.211344 1.50766 ]
--------------------
[layer17_wq_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer17_wq_weight] First 3: [0.00619663 -0.000963191 -0.0017183 ]
[layer17_wq_weight] Last 3: [0.00261576 0.00727451 0.000511176 ]
--------------------
[layer17_wq] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer17_wq] First 3: [0.13862 -8.82716 0.062823 ]
[layer17_wq] Last 3: [1.56222 -0.597553 2.10675 ]
--------------------
[layer17_q_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer17_q_norm] Token 0 First 3: [0.180349 -0.0274065 0.136352 ]
[layer17_q_norm] Token 0 Last 3: [0.296402 -0.236444 0.497153 ]
[layer17_q_norm] Token 1 First 3: [0.662227 -0.0282086 -2.35076 ]
[layer17_q_norm] Token 1 Last 3: [0.193566 -0.593853 0.980314 ]
[layer17_q_norm] Token 2 First 3: [-0.646758 -0.0288631 -0.0167666 ]
[layer17_q_norm] Token 2 Last 3: [0.207033 -0.129842 0.892005 ]
--------------------
[layer17_attention_query_rope] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer17_attention_query_rope] Token 0 First 3: [0.180349 -0.0274065 0.136352 ]
[layer17_attention_query_rope] Token 0 Last 3: [0.296402 -0.236444 0.497153 ]
[layer17_attention_query_rope] Token 1 First 3: [0.904401 -0.0511159 -1.35772 ]
[layer17_attention_query_rope] Token 1 Last 3: [0.19357 -0.593841 0.980322 ]
[layer17_attention_query_rope] Token 2 First 3: [1.86225 -0.0334277 0.21657 ]
[layer17_attention_query_rope] Token 2 Last 3: [0.20702 -0.129897 0.89199 ]
--------------------
[layer17_attention_key_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer17_attention_value_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer17_attention_attn_probs] Shape: Shape: 1:1:6:3 [ FP32 : NCHW ]

[layer17_attention_attn_probs] Token 0 First 3: [1 1 1 ]
[layer17_attention_attn_probs] Token 0 Last 3: [1 1 1 ]
[layer17_attention_attn_probs] Token 1 First 3: [0.892172 0.531554 0.118443 ]
[layer17_attention_attn_probs] Token 1 Last 3: [0.892172 0.531554 0.118443 ]
[layer17_attention_attn_probs] Token 2 First 3: [0.107828 0.468446 0.881557 ]
[layer17_attention_attn_probs] Token 2 Last 3: [0.107828 0.468446 0.881557 ]
[layer17_attention_attn_probs] Token 3 First 3: [0.0786555 0.1113 0.460916 ]
[layer17_attention_attn_probs] Token 3 Last 3: [0.0786555 0.1113 0.460916 ]
[layer17_attention_attn_probs] Token 4 First 3: [0.235516 0.315569 0.0272987 ]
[layer17_attention_attn_probs] Token 4 Last 3: [0.235516 0.315569 0.0272987 ]
[layer17_attention_attn_probs] Token 5 First 3: [0.685829 0.573132 0.511785 ]
[layer17_attention_attn_probs] Token 5 Last 3: [0.685829 0.573132 0.511785 ]
--------------------
[layer17_attention_attention_out_core] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer17_attention_attention_out_core] Token 0 First 3: [1.88574 -0.436035 -1.07715 ]
[layer17_attention_attention_out_core] Token 0 Last 3: [-1.14746 0.0874023 -0.226807 ]
[layer17_attention_attention_out_core] Token 1 First 3: [1.90501 -0.342344 -0.830639 ]
[layer17_attention_attention_out_core] Token 1 Last 3: [0.256014 -2.24864 -0.20593 ]
[layer17_attention_attention_out_core] Token 2 First 3: [-0.04761 -0.569622 0.0823856 ]
[layer17_attention_attention_out_core] Token 2 Last 3: [-0.630699 0.0917187 -0.71533 ]
--------------------
[layer17_attention_out input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer17_attention_out input] First 3: [1.88574 -0.436035 -1.07715 ]
[layer17_attention_out input] Last 3: [-0.630699 0.0917187 -0.71533 ]
--------------------
[layer17_attention_out_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer17_attention_out_weight] First 3: [0.00264585 -0.000692966 -0.00140631 ]
[layer17_attention_out_weight] Last 3: [0.00170465 -0.00694371 0.0127335 ]
--------------------
[layer17_attention_out] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer17_attention_out] First 3: [-0.131041 -0.0348581 0.0451427 ]
[layer17_attention_out] Last 3: [0.528574 -0.397736 -0.258436 ]
--------------------
[layer17_post_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer17_post_attention_norm] Token 0 First 3: [-18.0437 -32.551 10.2179 ]
[layer17_post_attention_norm] Token 0 Last 3: [-26.8977 54.9956 165.504 ]
[layer17_post_attention_norm] Token 1 First 3: [-13.7735 -108.112 38.6214 ]
[layer17_post_attention_norm] Token 1 Last 3: [-24.4879 28.3524 94.5548 ]
[layer17_post_attention_norm] Token 2 First 3: [-6.26669 -39.639 -1.06015 ]
[layer17_post_attention_norm] Token 2 Last 3: [19.9641 -32.2215 -40.0779 ]
--------------------
[layer17_post_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer17_post_attention_norm input] Token 0 First 3: [-0.131041 -0.0348581 0.0451427 ]
[layer17_post_attention_norm input] Token 0 Last 3: [-0.264867 0.252484 0.39693 ]
[layer17_post_attention_norm input] Token 1 First 3: [-0.128537 -0.148769 0.219257 ]
[layer17_post_attention_norm input] Token 1 Last 3: [-0.309859 0.167262 0.291399 ]
[layer17_post_attention_norm input] Token 2 First 3: [-0.122367 -0.114131 -0.0125932 ]
[layer17_post_attention_norm input] Token 2 Last 3: [0.528574 -0.397736 -0.258436 ]
--------------------
[layer17_post_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer17_post_attention_norm_weight] First 3: [78.7078 533.78 129.383 ]
[layer17_post_attention_norm_weight] Last 3: [58.0482 124.507 238.34 ]
--------------------
[layer17pre_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer17pre_ffn_norm] Token 0 First 3: [3.16375 -0.305669 -0.201348 ]
[layer17pre_ffn_norm] Token 0 Last 3: [-1.23433 0.647755 -0.168482 ]
[layer17pre_ffn_norm] Token 1 First 3: [2.54541 0.357994 -0.340152 ]
[layer17pre_ffn_norm] Token 1 Last 3: [-2.42097 2.2462 0.872227 ]
[layer17pre_ffn_norm] Token 2 First 3: [1.32135 -0.0961735 -1.01225 ]
[layer17pre_ffn_norm] Token 2 Last 3: [-0.74158 -0.0735786 0.27785 ]
--------------------
[layer17_ffn_up input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer17_ffn_up input] First 3: [3.16375 -0.305669 -0.201348 ]
[layer17_ffn_up input] Last 3: [-0.74158 -0.0735786 0.27785 ]
--------------------
[layer17_ffn_up_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer17_ffn_up_weight] First 3: [0.00870749 0.00473848 -0.00611886 ]
[layer17_ffn_up_weight] Last 3: [-0.0100851 -0.00571075 0.00906961 ]
--------------------
[layer17_ffn_up] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer17_ffn_up] First 3: [-0.0194403 0.155175 -0.61245 ]
[layer17_ffn_up] Last 3: [-0.00567126 0.0154869 0.026507 ]
--------------------
[layer17_ffn_gate input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer17_ffn_gate input] First 3: [3.16375 -0.305669 -0.201348 ]
[layer17_ffn_gate input] Last 3: [-0.74158 -0.0735786 0.27785 ]
--------------------
[layer17_ffn_gate_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer17_ffn_gate_weight] First 3: [-0.00129866 -0.00628093 0.00156717 ]
[layer17_ffn_gate_weight] Last 3: [0.00783365 0.000369471 -0.00277829 ]
--------------------
[layer17_ffn_gate] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer17_ffn_gate] First 3: [-0.0200463 -0.25966 0.0643723 ]
[layer17_ffn_gate] Last 3: [-0.0106598 -0.0190609 -0.0710933 ]
--------------------
[layer17_ffn_down input] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer17_ffn_down input] First 3: [0.000191736 -0.0160191 -0.0207242 ]
[layer17_ffn_down input] Last 3: [2.99702e-05 -0.000145353 -0.000888832 ]
--------------------
[layer17_ffn_down_weight] Shape: Shape: 1:1:1152:768 [ FP32 : NCHW ]

[layer17_ffn_down_weight] First 3: [0.00385 0.0026552 -0.001458 ]
[layer17_ffn_down_weight] Last 3: [0.0100495 -0.0114793 0.0138299 ]
--------------------
[layer17_ffn_down] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer17_ffn_down] First 3: [0.0148297 -0.00220528 -0.0204542 ]
[layer17_ffn_down] Last 3: [0.00207102 0.00203798 -0.000742255 ]
--------------------
[layer17post_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer17post_ffn_norm] Token 0 First 3: [53.5846 -35.5749 -133.169 ]
[layer17post_ffn_norm] Token 0 Last 3: [-17.7629 107.784 -140.174 ]
[layer17post_ffn_norm] Token 1 First 3: [38.8493 -240.421 138.179 ]
[layer17post_ffn_norm] Token 1 Last 3: [45.5089 81.4118 -194.782 ]
[layer17post_ffn_norm] Token 2 First 3: [6.33258 154.491 30.642 ]
[layer17post_ffn_norm] Token 2 Last 3: [34.9437 48.4963 -34.7784 ]
--------------------
[layer18_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer18_attention_norm] Token 0 First 3: [14.3204 -1.56844 -4.18204 ]
[layer18_attention_norm] Token 0 Last 3: [-6.41757 5.66121 -2.83207 ]
[layer18_attention_norm] Token 1 First 3: [11.3974 -4.36521 6.35953 ]
[layer18_attention_norm] Token 1 Last 3: [-4.27734 11.2948 -3.08683 ]
[layer18_attention_norm] Token 2 First 3: [4.486 0.521498 -1.72249 ]
[layer18_attention_norm] Token 2 Last 3: [-2.1521 0.392951 0.654669 ]
--------------------
[layer18_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer18_attention_norm input] Token 0 First 3: [259.707 -125.266 -149.499 ]
[layer18_attention_norm input] Token 0 Last 3: [-117.249 186.809 -177.508 ]
[layer18_attention_norm input] Token 1 First 3: [114.232 -192.673 125.639 ]
[layer18_attention_norm input] Token 1 Last 3: [-43.1881 205.977 -106.925 ]
[layer18_attention_norm input] Token 2 First 3: [186.443 95.4499 -141.112 ]
[layer18_attention_norm input] Token 2 Last 3: [-90.1073 29.7157 94.0363 ]
--------------------
[layer18_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer18_attention_norm_weight] First 3: [23.3704 5.30678 11.8562 ]
[layer18_attention_norm_weight] Last 3: [23.1983 12.8442 6.76207 ]
--------------------
[layer18_wv input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer18_wv input] First 3: [14.3204 -1.56844 -4.18204 ]
[layer18_wv input] Last 3: [-2.1521 0.392951 0.654669 ]
--------------------
[layer18_wv_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer18_wv_weight] First 3: [0.00387182 0.00536057 -0.00820542 ]
[layer18_wv_weight] Last 3: [-0.0227261 -0.00776326 -0.0118178 ]
--------------------
[layer18_wv] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer18_wv] First 3: [-2.66483 1.79618 -0.138765 ]
[layer18_wv] Last 3: [0.580338 0.113364 -0.195439 ]
--------------------
[layer18_wk input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer18_wk input] First 3: [14.3204 -1.56844 -4.18204 ]
[layer18_wk input] Last 3: [-2.1521 0.392951 0.654669 ]
--------------------
[layer18_wk_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer18_wk_weight] First 3: [0.0185807 0.0173995 -0.000408757 ]
[layer18_wk_weight] Last 3: [-0.00310752 -0.00604692 0.0083899 ]
--------------------
[layer18_wk] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer18_wk] First 3: [0.162733 0.268406 -0.537047 ]
[layer18_wk] Last 3: [-1.86232 4.5015 -0.0446827 ]
--------------------
[layer18_k_norm] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer18_k_norm] Token 0 First 3: [0.0668303 0.0578192 -0.0448632 ]
[layer18_k_norm] Token 0 Last 3: [1.94607 3.14264 0.387934 ]
[layer18_k_norm] Token 1 First 3: [1.73408 -0.103477 -0.0812764 ]
[layer18_k_norm] Token 1 Last 3: [2.3344 2.34281 0.842811 ]
[layer18_k_norm] Token 2 First 3: [0.109518 -0.265856 0.0179602 ]
[layer18_k_norm] Token 2 Last 3: [-2.07189 3.57362 -0.0467222 ]
--------------------
[layer18_wq input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer18_wq input] First 3: [14.3204 -1.56844 -4.18204 ]
[layer18_wq input] Last 3: [-2.1521 0.392951 0.654669 ]
--------------------
[layer18_wq_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer18_wq_weight] First 3: [0.0103772 -0.0154059 -0.00664066 ]
[layer18_wq_weight] Last 3: [-0.00474295 -0.00462341 0.011806 ]
--------------------
[layer18_wq] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer18_wq] First 3: [-2.1776 -0.0420325 8.27566 ]
[layer18_wq] Last 3: [-2.55608 0.661627 -0.362995 ]
--------------------
[layer18_q_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer18_q_norm] Token 0 First 3: [-1.9476 -0.0152858 -0.159699 ]
[layer18_q_norm] Token 0 Last 3: [-5.00933 1.85936 -0.633494 ]
[layer18_q_norm] Token 1 First 3: [0.759575 0.0954156 -0.130354 ]
[layer18_q_norm] Token 1 Last 3: [-4.39539 0.13544 1.25445 ]
[layer18_q_norm] Token 2 First 3: [-0.610547 0.109946 -0.162064 ]
[layer18_q_norm] Token 2 Last 3: [-3.7689 0.971971 -0.436986 ]
--------------------
[layer18_attention_query_rope] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer18_attention_query_rope] Token 0 First 3: [-1.9476 -0.0152858 -0.159699 ]
[layer18_attention_query_rope] Token 0 Last 3: [-5.00933 1.85936 -0.633494 ]
[layer18_attention_query_rope] Token 1 First 3: [0.630997 0.892656 0.966572 ]
[layer18_attention_query_rope] Token 1 Last 3: [-4.39517 0.135499 1.25457 ]
[layer18_attention_query_rope] Token 2 First 3: [0.456291 -0.223938 -2.00971 ]
[layer18_attention_query_rope] Token 2 Last 3: [-3.76893 0.972094 -0.437181 ]
--------------------
[layer18_attention_key_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer18_attention_value_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer18_attention_attn_probs] Shape: Shape: 1:1:6:3 [ FP32 : NCHW ]

[layer18_attention_attn_probs] Token 0 First 3: [1 1 1 ]
[layer18_attention_attn_probs] Token 0 Last 3: [1 1 1 ]
[layer18_attention_attn_probs] Token 1 First 3: [0.476135 0.334625 0.40951 ]
[layer18_attention_attn_probs] Token 1 Last 3: [0.476135 0.334625 0.40951 ]
[layer18_attention_attn_probs] Token 2 First 3: [0.523865 0.665375 0.59049 ]
[layer18_attention_attn_probs] Token 2 Last 3: [0.523865 0.665375 0.59049 ]
[layer18_attention_attn_probs] Token 3 First 3: [0.0860713 0.0545459 0.102165 ]
[layer18_attention_attn_probs] Token 3 Last 3: [0.0860713 0.0545459 0.102165 ]
[layer18_attention_attn_probs] Token 4 First 3: [0.0193881 0.112307 0.0124471 ]
[layer18_attention_attn_probs] Token 4 Last 3: [0.0193881 0.112307 0.0124471 ]
[layer18_attention_attn_probs] Token 5 First 3: [0.894541 0.833147 0.885388 ]
[layer18_attention_attn_probs] Token 5 Last 3: [0.894541 0.833147 0.885388 ]
--------------------
[layer18_attention_attention_out_core] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer18_attention_attention_out_core] Token 0 First 3: [-2.66406 1.7959 -0.138794 ]
[layer18_attention_attention_out_core] Token 0 Last 3: [-0.700195 0.363037 -0.0971069 ]
[layer18_attention_attention_out_core] Token 1 First 3: [-0.871461 0.749575 0.223985 ]
[layer18_attention_attention_out_core] Token 1 Last 3: [-1.29011 0.707442 0.210356 ]
[layer18_attention_attention_out_core] Token 2 First 3: [-0.407558 -0.0191308 1.1091 ]
[layer18_attention_attention_out_core] Token 2 Last 3: [0.421341 0.14922 -0.177684 ]
--------------------
[layer18_attention_out input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer18_attention_out input] First 3: [-2.66406 1.7959 -0.138794 ]
[layer18_attention_out input] Last 3: [0.421341 0.14922 -0.177684 ]
--------------------
[layer18_attention_out_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer18_attention_out_weight] First 3: [-0.00911111 0.0214069 -0.00170555 ]
[layer18_attention_out_weight] Last 3: [0.0029886 0.00912454 -0.0179105 ]
--------------------
[layer18_attention_out] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer18_attention_out] First 3: [-0.028741 -0.242331 -0.176484 ]
[layer18_attention_out] Last 3: [-0.00684662 0.0765165 -0.00862999 ]
--------------------
[layer18_post_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer18_post_attention_norm] Token 0 First 3: [-2.05812 -74.6352 -21.956 ]
[layer18_post_attention_norm] Token 0 Last 3: [-18.225 19.1742 84.6066 ]
[layer18_post_attention_norm] Token 1 First 3: [5.80546 27.5509 15.6822 ]
[layer18_post_attention_norm] Token 1 Last 3: [-20.1403 0.203387 33.2358 ]
[layer18_post_attention_norm] Token 2 First 3: [31.0607 48.7424 107.168 ]
[layer18_post_attention_norm] Token 2 Last 3: [-3.7197 43.1996 -8.44949 ]
--------------------
[layer18_post_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer18_post_attention_norm input] Token 0 First 3: [-0.028741 -0.242331 -0.176484 ]
[layer18_post_attention_norm input] Token 0 Last 3: [-0.116116 0.117558 0.299117 ]
[layer18_post_attention_norm input] Token 1 First 3: [0.0746759 0.0823974 0.116111 ]
[layer18_post_attention_norm input] Token 1 Last 3: [-0.118197 0.0011486 0.108232 ]
[layer18_post_attention_norm input] Token 2 First 3: [0.12531 0.0457208 0.248862 ]
[layer18_post_attention_norm input] Token 2 Last 3: [-0.00684662 0.0765165 -0.00862999 ]
--------------------
[layer18_post_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer18_post_attention_norm_weight] First 3: [41.5639 178.765 72.2095 ]
[layer18_post_attention_norm_weight] Last 3: [91.1006 94.6704 164.176 ]
--------------------
[layer18pre_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer18pre_ffn_norm] Token 0 First 3: [2.49572 -0.405725 -1.20488 ]
[layer18pre_ffn_norm] Token 0 Last 3: [-1.14823 1.14087 -0.254737 ]
[layer18pre_ffn_norm] Token 1 First 3: [2.16467 -0.62392 1.84889 ]
[layer18pre_ffn_norm] Token 1 Last 3: [-0.999259 2.12598 -0.376166 ]
[layer18pre_ffn_norm] Token 2 First 3: [0.979571 0.136068 -0.110909 ]
[layer18pre_ffn_norm] Token 2 Last 3: [-0.369741 0.187768 0.109113 ]
--------------------
[layer18_ffn_up input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer18_ffn_up input] First 3: [2.49572 -0.405725 -1.20488 ]
[layer18_ffn_up input] Last 3: [-0.369741 0.187768 0.109113 ]
--------------------
[layer18_ffn_up_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer18_ffn_up_weight] First 3: [0.0121145 -0.00424093 -0.00513644 ]
[layer18_ffn_up_weight] Last 3: [-0.00500675 0.00201746 0.00955235 ]
--------------------
[layer18_ffn_up] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer18_ffn_up] First 3: [-0.0614686 -0.103846 -0.0892677 ]
[layer18_ffn_up] Last 3: [-0.0110642 -0.0416648 0.0205947 ]
--------------------
[layer18_ffn_gate input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer18_ffn_gate input] First 3: [2.49572 -0.405725 -1.20488 ]
[layer18_ffn_gate input] Last 3: [-0.369741 0.187768 0.109113 ]
--------------------
[layer18_ffn_gate_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer18_ffn_gate_weight] First 3: [0.00353378 0.00291633 0.00117704 ]
[layer18_ffn_gate_weight] Last 3: [-0.0027405 0.00530333 -2.90751e-05 ]
--------------------
[layer18_ffn_gate] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer18_ffn_gate] First 3: [-0.179866 -0.209996 -0.0240979 ]
[layer18_ffn_gate] Last 3: [0.00378399 -0.0811301 -0.0326188 ]
--------------------
[layer18_ffn_down input] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer18_ffn_down input] First 3: [0.00473899 0.00909011 0.0010549 ]
[layer18_ffn_down input] Last 3: [-2.09967e-05 0.00158085 -0.000327147 ]
--------------------
[layer18_ffn_down_weight] Shape: Shape: 1:1:1152:768 [ FP32 : NCHW ]

[layer18_ffn_down_weight] First 3: [0.00450131 0.00384535 0.000508586 ]
[layer18_ffn_down_weight] Last 3: [0.00212351 -0.00171069 0.00295958 ]
--------------------
[layer18_ffn_down] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer18_ffn_down] First 3: [0.00197103 0.00193175 -0.00261071 ]
[layer18_ffn_down] Last 3: [6.75592e-05 -0.00103811 -0.00171501 ]
--------------------
[layer18post_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer18post_ffn_norm] Token 0 First 3: [12.8588 60.4053 -30.2102 ]
[layer18post_ffn_norm] Token 0 Last 3: [1.72635 23.562 65.3535 ]
[layer18post_ffn_norm] Token 1 First 3: [60.676 95.0925 -75.257 ]
[layer18post_ffn_norm] Token 1 Last 3: [-40.8912 -122.831 28.3028 ]
[layer18post_ffn_norm] Token 2 First 3: [0.500075 3.2812 -23.6217 ]
[layer18post_ffn_norm] Token 2 Last 3: [4.96457 -69.8739 -219.072 ]
--------------------
[layer19_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer19_attention_norm] Token 0 First 3: [8.73419 -0.571697 -2.84351 ]
[layer19_attention_norm] Token 0 Last 3: [-2.37373 2.28019 -0.162675 ]
[layer19_attention_norm] Token 1 First 3: [11.3883 -0.560159 1.8181 ]
[layer19_attention_norm] Token 1 Last 3: [-3.61012 1.61597 -0.523094 ]
[layer19_attention_norm] Token 2 First 3: [4.23691 0.363798 -0.488578 ]
[layer19_attention_norm] Token 2 Last 3: [-0.949303 0.0181854 -0.474465 ]
--------------------
[layer19_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer19_attention_norm input] Token 0 First 3: [270.507 -139.496 -201.665 ]
[layer19_attention_norm input] Token 0 Last 3: [-133.748 229.546 -27.5482 ]
[layer19_attention_norm input] Token 1 First 3: [180.713 -70.0293 66.0643 ]
[layer19_attention_norm input] Token 1 Last 3: [-104.22 83.3496 -45.3863 ]
[layer19_attention_norm input] Token 2 First 3: [218.004 147.473 -57.5663 ]
[layer19_attention_norm input] Token 2 Last 3: [-88.8624 3.04144 -133.485 ]
--------------------
[layer19_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer19_attention_norm_weight] First 3: [20.158 2.55864 8.80296 ]
[layer19_attention_norm_weight] Last 3: [11.0803 6.20165 3.68666 ]
--------------------
[layer19_wv input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer19_wv input] First 3: [8.73419 -0.571697 -2.84351 ]
[layer19_wv input] Last 3: [-0.949303 0.0181854 -0.474465 ]
--------------------
[layer19_wv_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer19_wv_weight] First 3: [0.00372817 0.000407108 0.0085875 ]
[layer19_wv_weight] Last 3: [-0.00371776 0.00586323 0.01471 ]
--------------------
[layer19_wv] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer19_wv] First 3: [-0.0931659 -1.01342 -0.523367 ]
[layer19_wv] Last 3: [0.329242 0.289572 -0.141313 ]
--------------------
[layer19_wk input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer19_wk input] First 3: [8.73419 -0.571697 -2.84351 ]
[layer19_wk input] Last 3: [-0.949303 0.0181854 -0.474465 ]
--------------------
[layer19_wk_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer19_wk_weight] First 3: [0.00554922 0.00176914 0.0100287 ]
[layer19_wk_weight] Last 3: [0.0185849 0.00298501 -0.0141911 ]
--------------------
[layer19_wk] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer19_wk] First 3: [0.871452 -1.00273 -0.0833228 ]
[layer19_wk] Last 3: [-0.831013 0.766903 -2.8685 ]
--------------------
[layer19_k_norm] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer19_k_norm] Token 0 First 3: [0.7443 -2.03693 0.01736 ]
[layer19_k_norm] Token 0 Last 3: [-1.35844 0.203546 -1.20136 ]
[layer19_k_norm] Token 1 First 3: [0.386915 -3.45337 -0.149578 ]
[layer19_k_norm] Token 1 Last 3: [-1.53532 2.16845 3.20049 ]
[layer19_k_norm] Token 2 First 3: [0.0649666 -0.205983 -0.0919502 ]
[layer19_k_norm] Token 2 Last 3: [-0.910849 0.99596 -3.99864 ]
--------------------
[layer19_wq input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer19_wq input] First 3: [8.73419 -0.571697 -2.84351 ]
[layer19_wq input] Last 3: [-0.949303 0.0181854 -0.474465 ]
--------------------
[layer19_wq_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer19_wq_weight] First 3: [-0.0137089 0.0117211 -0.000108455 ]
[layer19_wq_weight] Last 3: [0.00482905 -0.000674997 0.00520969 ]
--------------------
[layer19_wq] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer19_wq] First 3: [0.999726 -0.661179 1.15157 ]
[layer19_wq] Last 3: [-0.647555 -0.669789 -1.79934 ]
--------------------
[layer19_q_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer19_q_norm] Token 0 First 3: [2.11954 -0.698777 1.46221 ]
[layer19_q_norm] Token 0 Last 3: [-0.961602 -1.12803 -1.81204 ]
[layer19_q_norm] Token 1 First 3: [2.10293 -1.72523 0.958226 ]
[layer19_q_norm] Token 1 Last 3: [-0.0887478 -0.115991 -1.50529 ]
[layer19_q_norm] Token 2 First 3: [0.937556 -0.291624 0.702857 ]
[layer19_q_norm] Token 2 Last 3: [-0.803929 -0.829794 -1.52663 ]
--------------------
[layer19_attention_query_rope] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer19_attention_query_rope] Token 0 First 3: [2.11954 -0.698777 1.46221 ]
[layer19_attention_query_rope] Token 0 Last 3: [-0.961602 -1.12803 -1.81204 ]
[layer19_attention_query_rope] Token 1 First 3: [-1.59585 -1.95845 0.200088 ]
[layer19_attention_query_rope] Token 1 Last 3: [-0.088635 -0.1158 -1.50526 ]
[layer19_attention_query_rope] Token 2 First 3: [-0.460137 -0.0939322 -0.514726 ]
[layer19_attention_query_rope] Token 2 Last 3: [-0.80382 -0.829484 -1.52683 ]
--------------------
[layer19_attention_key_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer19_attention_value_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer19_attention_attn_probs] Shape: Shape: 1:1:6:3 [ FP32 : NCHW ]

[layer19_attention_attn_probs] Token 0 First 3: [1 1 1 ]
[layer19_attention_attn_probs] Token 0 Last 3: [1 1 1 ]
[layer19_attention_attn_probs] Token 1 First 3: [0.877583 0.816431 0.0141404 ]
[layer19_attention_attn_probs] Token 1 Last 3: [0.877583 0.816431 0.0141404 ]
[layer19_attention_attn_probs] Token 2 First 3: [0.122417 0.183569 0.98586 ]
[layer19_attention_attn_probs] Token 2 Last 3: [0.122417 0.183569 0.98586 ]
[layer19_attention_attn_probs] Token 3 First 3: [0.77317 0.269876 0.13063 ]
[layer19_attention_attn_probs] Token 3 Last 3: [0.77317 0.269876 0.13063 ]
[layer19_attention_attn_probs] Token 4 First 3: [0.0402324 0.0259327 0.0192819 ]
[layer19_attention_attn_probs] Token 4 Last 3: [0.0402324 0.0259327 0.0192819 ]
[layer19_attention_attn_probs] Token 5 First 3: [0.186598 0.704192 0.850088 ]
[layer19_attention_attn_probs] Token 5 Last 3: [0.186598 0.704192 0.850088 ]
--------------------
[layer19_attention_attention_out_core] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer19_attention_attention_out_core] Token 0 First 3: [-0.0931396 -1.01367 -0.523438 ]
[layer19_attention_attention_out_core] Token 0 Last 3: [0.205933 0.989746 0.37793 ]
[layer19_attention_attention_out_core] Token 1 First 3: [-0.124177 -1.16861 -0.637247 ]
[layer19_attention_attention_out_core] Token 1 Last 3: [1.12452 -0.322968 0.501643 ]
[layer19_attention_attention_out_core] Token 2 First 3: [-0.0573969 -1.36235 -0.353014 ]
[layer19_attention_attention_out_core] Token 2 Last 3: [0.328811 0.368844 -0.0610906 ]
--------------------
[layer19_attention_out input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer19_attention_out input] First 3: [-0.0931396 -1.01367 -0.523438 ]
[layer19_attention_out input] Last 3: [0.328811 0.368844 -0.0610906 ]
--------------------
[layer19_attention_out_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer19_attention_out_weight] First 3: [0.00792203 -0.00710257 0.000481663 ]
[layer19_attention_out_weight] Last 3: [0.00841813 0.00688555 0.0112219 ]
--------------------
[layer19_attention_out] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer19_attention_out] First 3: [-0.0416038 0.0990037 0.186933 ]
[layer19_attention_out] Last 3: [-0.0311114 0.126826 0.0815432 ]
--------------------
[layer19_post_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer19_post_attention_norm] Token 0 First 3: [-9.71821 121.009 114.65 ]
[layer19_post_attention_norm] Token 0 Last 3: [18.2664 0.919302 140.593 ]
[layer19_post_attention_norm] Token 1 First 3: [-0.114697 146.04 65.8215 ]
[layer19_post_attention_norm] Token 1 Last 3: [-17.2213 -11.3093 215.792 ]
[layer19_post_attention_norm] Token 2 First 3: [-31.4513 23.7783 24.2067 ]
[layer19_post_attention_norm] Token 2 Last 3: [-5.94329 38.6203 49.945 ]
--------------------
[layer19_post_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer19_post_attention_norm input] Token 0 First 3: [-0.0416038 0.0990037 0.186933 ]
[layer19_post_attention_norm input] Token 0 Last 3: [0.0644631 0.00203524 0.154748 ]
[layer19_post_attention_norm input] Token 1 First 3: [-0.000564521 0.137368 0.123385 ]
[layer19_post_attention_norm input] Token 1 Last 3: [-0.0698719 -0.0287856 0.273071 ]
[layer19_post_attention_norm input] Token 2 First 3: [-0.19972 0.0288569 0.0585443 ]
[layer19_post_attention_norm input] Token 2 Last 3: [-0.0311114 0.126826 0.0815432 ]
--------------------
[layer19_post_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer19_post_attention_norm_weight] First 3: [38.827 203.164 101.945 ]
[layer19_post_attention_norm_weight] Last 3: [47.1003 75.0797 151.015 ]
--------------------
[layer19pre_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer19pre_ffn_norm] Token 0 First 3: [1.39643 -0.02245 -0.347046 ]
[layer19pre_ffn_norm] Token 0 Last 3: [-0.581862 0.710836 0.188121 ]
[layer19pre_ffn_norm] Token 1 First 3: [1.69991 0.162261 0.924641 ]
[layer19pre_ffn_norm] Token 1 Last 3: [-1.07562 0.390592 0.498487 ]
[layer19pre_ffn_norm] Token 2 First 3: [0.591154 0.123073 -0.0787375 ]
[layer19pre_ffn_norm] Token 2 Last 3: [-0.282691 0.0760452 -0.0822722 ]
--------------------
[layer19_ffn_up input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer19_ffn_up input] First 3: [1.39643 -0.02245 -0.347046 ]
[layer19_ffn_up input] Last 3: [-0.282691 0.0760452 -0.0822722 ]
--------------------
[layer19_ffn_up_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer19_ffn_up_weight] First 3: [0.0010235 -0.0033854 0.00282617 ]
[layer19_ffn_up_weight] Last 3: [-0.00658452 0.00944358 0.00291992 ]
--------------------
[layer19_ffn_up] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer19_ffn_up] First 3: [-0.153421 -0.0170251 0.0166887 ]
[layer19_ffn_up] Last 3: [0.0984505 -0.0213435 0.0422278 ]
--------------------
[layer19_ffn_gate input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer19_ffn_gate input] First 3: [1.39643 -0.02245 -0.347046 ]
[layer19_ffn_gate input] Last 3: [-0.282691 0.0760452 -0.0822722 ]
--------------------
[layer19_ffn_gate_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer19_ffn_gate_weight] First 3: [0.00663748 0.0076395 -0.0117872 ]
[layer19_ffn_gate_weight] Last 3: [-0.00238463 0.00487912 0.00662928 ]
--------------------
[layer19_ffn_gate] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer19_ffn_gate] First 3: [-0.0281028 -0.0830301 0.262508 ]
[layer19_ffn_gate] Last 3: [-0.00964525 -0.124309 -0.103924 ]
--------------------
[layer19_ffn_down input] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer19_ffn_down input] First 3: [0.00210744 0.000660028 0.00264402 ]
[layer19_ffn_down input] Last 3: [-0.000471136 0.00119535 -0.00201262 ]
--------------------
[layer19_ffn_down_weight] Shape: Shape: 1:1:1152:768 [ FP32 : NCHW ]

[layer19_ffn_down_weight] First 3: [-0.00268528 -0.00754653 -0.00199259 ]
[layer19_ffn_down_weight] Last 3: [-0.00378567 -0.00637862 0.00656181 ]
--------------------
[layer19_ffn_down] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer19_ffn_down] First 3: [-0.000136661 -0.000655636 0.00256287 ]
[layer19_ffn_down] Last 3: [0.00125461 5.04177e-06 -0.00125631 ]
--------------------
[layer19post_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer19post_ffn_norm] Token 0 First 3: [-2.20702 -53.188 69.3277 ]
[layer19post_ffn_norm] Token 0 Last 3: [-11.6607 2.9464 -16.8979 ]
[layer19post_ffn_norm] Token 1 First 3: [0.850113 -7.71095 14.9279 ]
[layer19post_ffn_norm] Token 1 Last 3: [53.1815 -13.3849 -168.692 ]
[layer19post_ffn_norm] Token 2 First 3: [0.772193 139.316 49.6642 ]
[layer19post_ffn_norm] Token 2 Last 3: [65.5938 0.438789 -206.772 ]
--------------------
[layer20_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer20_attention_norm] Token 0 First 3: [10.7054 -0.508985 -0.346378 ]
[layer20_attention_norm] Token 0 Last 3: [-3.35382 4.27051 0.964571 ]
[layer20_attention_norm] Token 1 First 3: [12.3201 0.795451 4.71528 ]
[layer20_attention_norm] Token 1 Last 3: [-2.95303 1.76004 0.0281906 ]
[layer20_attention_norm] Token 2 First 3: [5.05672 1.43801 0.208192 ]
[layer20_attention_norm] Token 2 Last 3: [-0.502432 0.502242 -1.89904 ]
--------------------
[layer20_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer20_attention_norm input] Token 0 First 3: [258.582 -71.6747 -17.6874 ]
[layer20_attention_norm input] Token 0 Last 3: [-127.142 233.411 96.1469 ]
[layer20_attention_norm input] Token 1 First 3: [181.449 68.2997 146.814 ]
[layer20_attention_norm input] Token 1 Last 3: [-68.2594 58.6553 1.71336 ]
[layer20_attention_norm input] Token 2 First 3: [187.325 310.568 16.3046 ]
[layer20_attention_norm input] Token 2 Last 3: [-29.2119 42.1006 -290.313 ]
--------------------
[layer20_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer20_attention_norm_weight] First 3: [27.2671 4.67706 12.8979 ]
[layer20_attention_norm_weight] Last 3: [17.3734 12.0501 6.60744 ]
--------------------
[layer20_wv input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer20_wv input] First 3: [10.7054 -0.508985 -0.346378 ]
[layer20_wv input] Last 3: [-0.502432 0.502242 -1.89904 ]
--------------------
[layer20_wv_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer20_wv_weight] First 3: [-0.000548852 0.000215228 0.00588722 ]
[layer20_wv_weight] Last 3: [-0.00567457 0.00434222 -0.0112426 ]
--------------------
[layer20_wv] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer20_wv] First 3: [0.347764 0.853926 -0.150316 ]
[layer20_wv] Last 3: [-0.132208 1.15735 0.0970111 ]
--------------------
[layer20_wk input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer20_wk input] First 3: [10.7054 -0.508985 -0.346378 ]
[layer20_wk input] Last 3: [-0.502432 0.502242 -1.89904 ]
--------------------
[layer20_wk_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer20_wk_weight] First 3: [-0.0095862 0.00192273 0.0116807 ]
[layer20_wk_weight] Last 3: [0.00679196 -0.00643631 0.0114447 ]
--------------------
[layer20_wk] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer20_wk] First 3: [-1.55124 0.0685051 0.343179 ]
[layer20_wk] Last 3: [4.72417 -4.45353 2.22731 ]
--------------------
[layer20_k_norm] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer20_k_norm] Token 0 First 3: [-0.796535 0.078886 0.171201 ]
[layer20_k_norm] Token 0 Last 3: [5.46943 -3.43742 2.85724 ]
[layer20_k_norm] Token 1 First 3: [-1.04461 -2.89298 0.982444 ]
[layer20_k_norm] Token 1 Last 3: [5.38239 -3.27865 3.54318 ]
[layer20_k_norm] Token 2 First 3: [-0.380681 -0.778454 0.271095 ]
[layer20_k_norm] Token 2 Last 3: [5.30042 -3.46099 1.90246 ]
--------------------
[layer20_wq input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer20_wq input] First 3: [10.7054 -0.508985 -0.346378 ]
[layer20_wq input] Last 3: [-0.502432 0.502242 -1.89904 ]
--------------------
[layer20_wq_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer20_wq_weight] First 3: [0.00537252 -0.0076702 0.0188872 ]
[layer20_wq_weight] Last 3: [0.0031123 0.0112827 0.00410936 ]
--------------------
[layer20_wq] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer20_wq] First 3: [-1.34425 -0.370373 -1.13692 ]
[layer20_wq] Last 3: [1.04843 -0.913627 1.29066 ]
--------------------
[layer20_q_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer20_q_norm] Token 0 First 3: [-0.710445 -0.176991 -0.665124 ]
[layer20_q_norm] Token 0 Last 3: [0.522931 -0.957865 0.371745 ]
[layer20_q_norm] Token 1 First 3: [-2.6257 -3.42749 3.04793 ]
[layer20_q_norm] Token 1 Last 3: [0.478273 -1.49271 1.02084 ]
[layer20_q_norm] Token 2 First 3: [-0.405985 -0.245937 0.0119606 ]
[layer20_q_norm] Token 2 Last 3: [0.435521 -0.515607 0.696089 ]
--------------------
[layer20_attention_query_rope] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer20_attention_query_rope] Token 0 First 3: [-0.710445 -0.176991 -0.665124 ]
[layer20_attention_query_rope] Token 0 Last 3: [0.522931 -0.957865 0.371745 ]
[layer20_attention_query_rope] Token 1 First 3: [-4.79765 -2.08058 2.31268 ]
[layer20_attention_query_rope] Token 1 Last 3: [0.478335 -1.49266 1.0209 ]
[layer20_attention_query_rope] Token 2 First 3: [-0.673424 0.128936 0.491151 ]
[layer20_attention_query_rope] Token 2 Last 3: [0.435704 -0.51562 0.696028 ]
--------------------
[layer20_attention_key_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer20_attention_value_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer20_attention_attn_probs] Shape: Shape: 1:1:6:3 [ FP32 : NCHW ]

[layer20_attention_attn_probs] Token 0 First 3: [1 1 1 ]
[layer20_attention_attn_probs] Token 0 Last 3: [1 1 1 ]
[layer20_attention_attn_probs] Token 1 First 3: [0.0360093 0.406413 0.712732 ]
[layer20_attention_attn_probs] Token 1 Last 3: [0.0360093 0.406413 0.712732 ]
[layer20_attention_attn_probs] Token 2 First 3: [0.963991 0.593587 0.287268 ]
[layer20_attention_attn_probs] Token 2 Last 3: [0.963991 0.593587 0.287268 ]
[layer20_attention_attn_probs] Token 3 First 3: [0.139227 0.137549 0.501339 ]
[layer20_attention_attn_probs] Token 3 Last 3: [0.139227 0.137549 0.501339 ]
[layer20_attention_attn_probs] Token 4 First 3: [0.305173 0.179939 0.192874 ]
[layer20_attention_attn_probs] Token 4 Last 3: [0.305173 0.179939 0.192874 ]
[layer20_attention_attn_probs] Token 5 First 3: [0.555599 0.682512 0.305787 ]
[layer20_attention_attn_probs] Token 5 Last 3: [0.555599 0.682512 0.305787 ]
--------------------
[layer20_attention_attention_out_core] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer20_attention_attention_out_core] Token 0 First 3: [0.347656 0.854004 -0.150269 ]
[layer20_attention_attention_out_core] Token 0 Last 3: [0.310059 -1.6123 0.228027 ]
[layer20_attention_attention_out_core] Token 1 First 3: [-0.214475 -2.61457 -0.743937 ]
[layer20_attention_attention_out_core] Token 1 Last 3: [0.183432 -1.22692 -0.243413 ]
[layer20_attention_attention_out_core] Token 2 First 3: [-0.21458 -0.942486 -0.303517 ]
[layer20_attention_attention_out_core] Token 2 Last 3: [0.0898029 -0.506667 -0.128571 ]
--------------------
[layer20_attention_out input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer20_attention_out input] First 3: [0.347656 0.854004 -0.150269 ]
[layer20_attention_out input] Last 3: [0.0898029 -0.506667 -0.128571 ]
--------------------
[layer20_attention_out_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer20_attention_out_weight] First 3: [-0.00226221 -0.00604948 -0.00714985 ]
[layer20_attention_out_weight] Last 3: [0.00111354 -0.0127377 0.00859827 ]
--------------------
[layer20_attention_out] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer20_attention_out] First 3: [-0.0584837 0.496955 0.287781 ]
[layer20_attention_out] Last 3: [0.0406945 0.0105431 0.0770702 ]
--------------------
[layer20_post_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer20_post_attention_norm] Token 0 First 3: [-9.60781 448.794 107.677 ]
[layer20_post_attention_norm] Token 0 Last 3: [-29.0706 13.7586 145.309 ]
[layer20_post_attention_norm] Token 1 First 3: [-11.4398 36.5779 -18.9739 ]
[layer20_post_attention_norm] Token 1 Last 3: [20.6999 -44.7865 30.4237 ]
[layer20_post_attention_norm] Token 2 First 3: [-10.8954 158.225 -7.98716 ]
[layer20_post_attention_norm] Token 2 Last 3: [6.06238 3.23596 42.35 ]
--------------------
[layer20_post_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer20_post_attention_norm input] Token 0 First 3: [-0.0584837 0.496955 0.287781 ]
[layer20_post_attention_norm input] Token 0 Last 3: [-0.164054 0.0376858 0.222313 ]
[layer20_post_attention_norm input] Token 1 First 3: [-0.105129 0.0611478 -0.0765574 ]
[layer20_post_attention_norm input] Token 1 Last 3: [0.176357 -0.185201 0.0702711 ]
[layer20_post_attention_norm input] Token 2 First 3: [-0.0788888 0.208403 -0.0253917 ]
[layer20_post_attention_norm input] Token 2 Last 3: [0.0406945 0.0105431 0.0770702 ]
--------------------
[layer20_post_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer20_post_attention_norm_weight] First 3: [42.5925 234.139 97.0075 ]
[layer20_post_attention_norm_weight] Last 3: [45.9421 94.654 169.461 ]
--------------------
[layer20pre_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer20pre_ffn_norm] Token 0 First 3: [0.759528 0.255383 0.201388 ]
[layer20pre_ffn_norm] Token 0 Last 3: [-0.47691 0.414454 0.242436 ]
[layer20pre_ffn_norm] Token 1 First 3: [0.810868 0.111041 0.447296 ]
[layer20pre_ffn_norm] Token 1 Last 3: [-0.227011 0.0363589 0.0504493 ]
[layer20pre_ffn_norm] Token 2 First 3: [0.376272 0.221939 0.0130129 ]
[layer20pre_ffn_norm] Token 2 Last 3: [-0.0494087 0.053146 -0.174055 ]
--------------------
[layer20_ffn_up input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer20_ffn_up input] First 3: [0.759528 0.255383 0.201388 ]
[layer20_ffn_up input] Last 3: [-0.0494087 0.053146 -0.174055 ]
--------------------
[layer20_ffn_up_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer20_ffn_up_weight] First 3: [0.00631028 0.00520066 -0.00225082 ]
[layer20_ffn_up_weight] Last 3: [0.00516151 -0.00535321 0.009327 ]
--------------------
[layer20_ffn_up] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer20_ffn_up] First 3: [0.0601726 0.0592005 0.0351605 ]
[layer20_ffn_up] Last 3: [0.000941431 0.01176 0.0115256 ]
--------------------
[layer20_ffn_gate input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer20_ffn_gate input] First 3: [0.759528 0.255383 0.201388 ]
[layer20_ffn_gate input] Last 3: [-0.0494087 0.053146 -0.174055 ]
--------------------
[layer20_ffn_gate_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer20_ffn_gate_weight] First 3: [0.00022503 -0.00883016 -0.00902294 ]
[layer20_ffn_gate_weight] Last 3: [0.00327244 0.00440875 -0.0108399 ]
--------------------
[layer20_ffn_gate] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer20_ffn_gate] First 3: [-0.0217857 -0.0472391 0.0313031 ]
[layer20_ffn_gate] Last 3: [0.0597847 -0.105741 -0.0257666 ]
--------------------
[layer20_ffn_down input] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer20_ffn_down input] First 3: [-0.00064406 -0.00134561 0.00056406 ]
[layer20_ffn_down input] Last 3: [2.94832e-05 -0.000569398 -0.000145436 ]
--------------------
[layer20_ffn_down_weight] Shape: Shape: 1:1:1152:768 [ FP32 : NCHW ]

[layer20_ffn_down_weight] First 3: [-0.00900204 -0.0015332 -0.0010379 ]
[layer20_ffn_down_weight] Last 3: [-0.00239373 0.00179657 -0.00173402 ]
--------------------
[layer20_ffn_down] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer20_ffn_down] First 3: [0.000175978 0.000418469 0.00109198 ]
[layer20_ffn_down] Last 3: [-0.000271861 -0.000275345 -0.000858639 ]
--------------------
[layer20post_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer20post_ffn_norm] Token 0 First 3: [5.77623 84.5339 58.2881 ]
[layer20post_ffn_norm] Token 0 Last 3: [34.3393 -72.4064 -104.634 ]
[layer20post_ffn_norm] Token 1 First 3: [35.1192 -244.99 -29.4572 ]
[layer20post_ffn_norm] Token 1 Last 3: [43.0392 -58.6583 -31.7625 ]
[layer20post_ffn_norm] Token 2 First 3: [6.52579 134.388 -18.4479 ]
[layer20post_ffn_norm] Token 2 Last 3: [-10.3218 -16.3313 -108.138 ]
--------------------
[layer21_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer21_attention_norm] Token 0 First 3: [9.76411 2.25608 2.42168 ]
[layer21_attention_norm] Token 0 Last 3: [-3.35694 2.69562 1.13448 ]
[layer21_attention_norm] Token 1 First 3: [12.8934 -1.1229 2.63502 ]
[layer21_attention_norm] Token 1 Last 3: [-0.204186 -1.13295 0.00509349 ]
[layer21_attention_norm] Token 2 First 3: [6.02021 2.53066 -0.142041 ]
[layer21_attention_norm] Token 2 Last 3: [-0.79151 0.38409 -2.53491 ]
--------------------
[layer21_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer21_attention_norm input] Token 0 First 3: [254.75 461.654 148.278 ]
[layer21_attention_norm input] Token 0 Last 3: [-121.873 174.764 136.822 ]
[layer21_attention_norm input] Token 1 First 3: [205.128 -140.112 98.3826 ]
[layer21_attention_norm input] Token 1 Last 3: [-4.52028 -44.7894 0.374582 ]
[layer21_attention_norm input] Token 2 First 3: [182.955 603.18 -10.1304 ]
[layer21_attention_norm input] Token 2 Last 3: [-33.4713 29.0052 -356.101 ]
--------------------
[layer21_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer21_attention_norm_weight] First 3: [33.5715 4.28047 14.3052 ]
[layer21_attention_norm_weight] Last 3: [24.1262 13.5102 7.26265 ]
--------------------
[layer21_wv input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer21_wv input] First 3: [9.76411 2.25608 2.42168 ]
[layer21_wv input] Last 3: [-0.79151 0.38409 -2.53491 ]
--------------------
[layer21_wv_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer21_wv_weight] First 3: [0.00502245 -0.00625391 -0.00110819 ]
[layer21_wv_weight] Last 3: [-0.000704335 -0.00915811 0.00309912 ]
--------------------
[layer21_wv] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer21_wv] First 3: [-0.131972 0.0819242 -0.574917 ]
[layer21_wv] Last 3: [-0.279709 -1.08632 -0.502319 ]
--------------------
[layer21_wk input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer21_wk input] First 3: [9.76411 2.25608 2.42168 ]
[layer21_wk input] Last 3: [-0.79151 0.38409 -2.53491 ]
--------------------
[layer21_wk_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer21_wk_weight] First 3: [0.00520477 -0.0135654 -0.000435323 ]
[layer21_wk_weight] Last 3: [-0.00621499 -0.00371427 -0.00448637 ]
--------------------
[layer21_wk] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer21_wk] First 3: [-0.918792 -0.427371 -0.0779053 ]
[layer21_wk] Last 3: [-0.252388 -4.55786 -9.93329 ]
--------------------
[layer21_k_norm] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer21_k_norm] Token 0 First 3: [-0.330382 -0.0992626 -0.0156679 ]
[layer21_k_norm] Token 0 Last 3: [0.370851 0.857513 -4.6105 ]
[layer21_k_norm] Token 1 First 3: [-0.707098 -0.257273 0.0558798 ]
[layer21_k_norm] Token 1 Last 3: [0.442028 -1.39311 -4.92825 ]
[layer21_k_norm] Token 2 First 3: [-0.0484076 -0.0751346 0.0895291 ]
[layer21_k_norm] Token 2 Last 3: [-0.07403 -3.01793 -3.97491 ]
--------------------
[layer21_wq input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer21_wq input] First 3: [9.76411 2.25608 2.42168 ]
[layer21_wq input] Last 3: [-0.79151 0.38409 -2.53491 ]
--------------------
[layer21_wq_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer21_wq_weight] First 3: [-0.00775846 0.00723398 0.0048095 ]
[layer21_wq_weight] Last 3: [-0.00427708 -0.0122224 0.00126034 ]
--------------------
[layer21_wq] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer21_wq] First 3: [-0.192179 -0.255948 -0.144117 ]
[layer21_wq] Last 3: [0.84517 -4.14942 -1.38622 ]
--------------------
[layer21_q_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer21_q_norm] Token 0 First 3: [-0.153712 -0.132518 -0.0792703 ]
[layer21_q_norm] Token 0 Last 3: [0.361746 -1.10647 -0.326673 ]
[layer21_q_norm] Token 1 First 3: [-0.905069 -0.642898 0.657218 ]
[layer21_q_norm] Token 1 Last 3: [0.588825 -1.55503 -0.338669 ]
[layer21_q_norm] Token 2 First 3: [0.117018 -0.189673 -0.571009 ]
[layer21_q_norm] Token 2 Last 3: [0.345487 -0.953469 -0.362035 ]
--------------------
[layer21_attention_query_rope] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer21_attention_query_rope] Token 0 First 3: [-0.153712 -0.132518 -0.0792703 ]
[layer21_attention_query_rope] Token 0 Last 3: [0.361746 -1.10647 -0.326673 ]
[layer21_attention_query_rope] Token 1 First 3: [-0.607477 1.23558 0.506883 ]
[layer21_attention_query_rope] Token 1 Last 3: [0.588947 -1.5551 -0.338631 ]
[layer21_attention_query_rope] Token 2 First 3: [-0.280024 0.572431 0.018705 ]
[layer21_attention_query_rope] Token 2 Last 3: [0.345347 -0.953399 -0.361789 ]
--------------------
[layer21_attention_key_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer21_attention_value_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer21_attention_attn_probs] Shape: Shape: 1:1:6:3 [ FP32 : NCHW ]

[layer21_attention_attn_probs] Token 0 First 3: [1 1 1 ]
[layer21_attention_attn_probs] Token 0 Last 3: [1 1 1 ]
[layer21_attention_attn_probs] Token 1 First 3: [0.900768 0.316527 0.301488 ]
[layer21_attention_attn_probs] Token 1 Last 3: [0.900768 0.316527 0.301488 ]
[layer21_attention_attn_probs] Token 2 First 3: [0.0992317 0.683473 0.698512 ]
[layer21_attention_attn_probs] Token 2 Last 3: [0.0992317 0.683473 0.698512 ]
[layer21_attention_attn_probs] Token 3 First 3: [0.187732 0.392132 0.483198 ]
[layer21_attention_attn_probs] Token 3 Last 3: [0.187732 0.392132 0.483198 ]
[layer21_attention_attn_probs] Token 4 First 3: [0.13892 0.0862958 0.187558 ]
[layer21_attention_attn_probs] Token 4 Last 3: [0.13892 0.0862958 0.187558 ]
[layer21_attention_attn_probs] Token 5 First 3: [0.673349 0.521572 0.329244 ]
[layer21_attention_attn_probs] Token 5 Last 3: [0.673349 0.521572 0.329244 ]
--------------------
[layer21_attention_attention_out_core] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer21_attention_attention_out_core] Token 0 First 3: [-0.131958 0.0819092 -0.574707 ]
[layer21_attention_attention_out_core] Token 0 Last 3: [0.995605 -0.165771 -1.37012 ]
[layer21_attention_attention_out_core] Token 1 First 3: [-0.357252 -0.131659 -0.618169 ]
[layer21_attention_attention_out_core] Token 1 Last 3: [0.77391 1.23518 -1.26848 ]
[layer21_attention_attention_out_core] Token 2 First 3: [-1.26134 -0.105126 -1.57357 ]
[layer21_attention_attention_out_core] Token 2 Last 3: [0.516163 -0.0925616 -1.05715 ]
--------------------
[layer21_attention_out input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer21_attention_out input] First 3: [-0.131958 0.0819092 -0.574707 ]
[layer21_attention_out input] Last 3: [0.516163 -0.0925616 -1.05715 ]
--------------------
[layer21_attention_out_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer21_attention_out_weight] First 3: [-0.00692657 -0.00277807 0.0122174 ]
[layer21_attention_out_weight] Last 3: [0.0102044 -0.00652761 0.00804057 ]
--------------------
[layer21_attention_out] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer21_attention_out] First 3: [-0.0307221 -0.0487879 0.0178454 ]
[layer21_attention_out] Last 3: [0.353817 0.113848 0.213077 ]
--------------------
[layer21_post_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer21_post_attention_norm] Token 0 First 3: [-5.22411 -36.6088 4.66825 ]
[layer21_post_attention_norm] Token 0 Last 3: [-10.1304 22.8514 90.2575 ]
[layer21_post_attention_norm] Token 1 First 3: [-15.1419 -55.4024 45.6325 ]
[layer21_post_attention_norm] Token 1 Last 3: [18.1953 -24.5552 168.88 ]
[layer21_post_attention_norm] Token 2 First 3: [0.541293 96.4849 1.49113 ]
[layer21_post_attention_norm] Token 2 Last 3: [36.3587 23.7949 80.8624 ]
--------------------
[layer21_post_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer21_post_attention_norm input] Token 0 First 3: [-0.0307221 -0.0487879 0.0178454 ]
[layer21_post_attention_norm input] Token 0 Last 3: [-0.0642445 0.0712513 0.154993 ]
[layer21_post_attention_norm input] Token 1 First 3: [-0.0972289 -0.0806178 0.190468 ]
[layer21_post_attention_norm input] Token 1 Last 3: [0.125992 -0.0835987 0.316651 ]
[layer21_post_attention_norm input] Token 2 First 3: [0.00488465 0.19731 0.00874685 ]
[layer21_post_attention_norm input] Token 2 Last 3: [0.353817 0.113848 0.213077 ]
--------------------
[layer21_post_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer21_post_attention_norm_weight] First 3: [81.3982 359.191 125.222 ]
[layer21_post_attention_norm_weight] Last 3: [75.4821 153.523 278.757 ]
--------------------
[layer21pre_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer21pre_ffn_norm] Token 0 First 3: [0.668056 0.21451 0.265356 ]
[layer21pre_ffn_norm] Token 0 Last 3: [-0.355007 0.262498 0.172631 ]
[layer21pre_ffn_norm] Token 1 First 3: [0.832343 -0.161464 0.408867 ]
[layer21pre_ffn_norm] Token 1 Last 3: [0.0601819 -0.150732 0.210556 ]
[layer21pre_ffn_norm] Token 2 First 3: [0.431718 0.310298 -0.0131717 ]
[layer21pre_ffn_norm] Token 2 Last 3: [0.00682383 0.0616336 -0.183876 ]
--------------------
[layer21_ffn_up input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer21_ffn_up input] First 3: [0.668056 0.21451 0.265356 ]
[layer21_ffn_up input] Last 3: [0.00682383 0.0616336 -0.183876 ]
--------------------
[layer21_ffn_up_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer21_ffn_up_weight] First 3: [0.00206096 -0.000629346 0.00764908 ]
[layer21_ffn_up_weight] Last 3: [0.00779834 -0.00841896 0.00346776 ]
--------------------
[layer21_ffn_up] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer21_ffn_up] First 3: [0.0387797 -0.0592192 -0.0756797 ]
[layer21_ffn_up] Last 3: [-0.0229379 -0.0818064 0.121295 ]
--------------------
[layer21_ffn_gate input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer21_ffn_gate input] First 3: [0.668056 0.21451 0.265356 ]
[layer21_ffn_gate input] Last 3: [0.00682383 0.0616336 -0.183876 ]
--------------------
[layer21_ffn_gate_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer21_ffn_gate_weight] First 3: [0.00284282 -0.0064051 0.000861961 ]
[layer21_ffn_gate_weight] Last 3: [-0.0138904 0.00109003 -0.000372485 ]
--------------------
[layer21_ffn_gate] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer21_ffn_gate] First 3: [-0.0322619 -0.0493951 -0.195814 ]
[layer21_ffn_gate] Last 3: [-0.00432181 -0.0130082 0.0257757 ]
--------------------
[layer21_ffn_down input] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer21_ffn_down input] First 3: [-0.000609455 0.00140495 0.0062593 ]
[layer21_ffn_down input] Last 3: [4.93957e-05 0.000526556 0.00159538 ]
--------------------
[layer21_ffn_down_weight] Shape: Shape: 1:1:1152:768 [ FP32 : NCHW ]

[layer21_ffn_down_weight] First 3: [0.00424252 0.00251448 -0.00386566 ]
[layer21_ffn_down_weight] Last 3: [0.00563136 0.00107256 0.000357747 ]
--------------------
[layer21_ffn_down] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer21_ffn_down] First 3: [-0.000515931 -0.000419267 -0.000410755 ]
[layer21_ffn_down] Last 3: [-0.00120512 0.0013442 0.00162844 ]
--------------------
[layer21post_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer21post_ffn_norm] Token 0 First 3: [-26.9703 -113.944 -29.284 ]
[layer21post_ffn_norm] Token 0 Last 3: [64.6093 -49.6504 125.934 ]
[layer21post_ffn_norm] Token 1 First 3: [-30.5407 104.086 18.878 ]
[layer21post_ffn_norm] Token 1 Last 3: [38.8658 -61.2583 44.3359 ]
[layer21post_ffn_norm] Token 2 First 3: [-81.9817 309.402 86.4569 ]
[layer21post_ffn_norm] Token 2 Last 3: [-50.7154 75.5039 188.232 ]
--------------------
[layer22_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer22_attention_norm] Token 0 First 3: [6.72366 1.16349 1.90135 ]
[layer22_attention_norm] Token 0 Last 3: [-1.47511 1.57888 2.26147 ]
[layer22_attention_norm] Token 1 First 3: [7.73681 -0.549197 4.02265 ]
[layer22_attention_norm] Token 1 Last 3: [1.84707 -2.23836 2.19769 ]
[layer22_attention_norm] Token 2 First 3: [2.59073 3.18793 1.01072 ]
[layer22_attention_norm] Token 2 Last 3: [-0.884323 1.15654 -0.470845 ]
--------------------
[layer22_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer22_attention_norm input] Token 0 First 3: [222.556 311.101 123.662 ]
[layer22_attention_norm input] Token 0 Last 3: [-67.3944 147.965 353.013 ]
[layer22_attention_norm input] Token 1 First 3: [159.445 -91.4285 162.893 ]
[layer22_attention_norm input] Token 1 Last 3: [52.5408 -130.603 213.591 ]
[layer22_attention_norm input] Token 2 First 3: [101.515 1009.07 77.8177 ]
[layer22_attention_norm input] Token 2 Last 3: [-47.828 128.304 -87.0064 ]
--------------------
[layer22_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer22_attention_norm_weight] First 3: [32.7199 4.05049 16.6522 ]
[layer22_attention_norm_weight] Last 3: [23.7054 11.5568 6.93818 ]
--------------------
[layer22_wv input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer22_wv input] First 3: [6.72366 1.16349 1.90135 ]
[layer22_wv input] Last 3: [-0.884323 1.15654 -0.470845 ]
--------------------
[layer22_wv_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer22_wv_weight] First 3: [-0.00235797 0.00438762 0.00693382 ]
[layer22_wv_weight] Last 3: [0.00281052 -0.00337181 0.00183252 ]
--------------------
[layer22_wv] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer22_wv] First 3: [1.09662 0.78127 0.232303 ]
[layer22_wv] Last 3: [-0.418608 -1.06167 -0.659913 ]
--------------------
[layer22_wk input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer22_wk input] First 3: [6.72366 1.16349 1.90135 ]
[layer22_wk input] Last 3: [-0.884323 1.15654 -0.470845 ]
--------------------
[layer22_wk_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer22_wk_weight] First 3: [-0.00720579 -0.000672027 0.0152635 ]
[layer22_wk_weight] Last 3: [-0.00669905 0.00603476 -0.00897573 ]
--------------------
[layer22_wk] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer22_wk] First 3: [0.976554 -0.514817 0.62692 ]
[layer22_wk] Last 3: [0.748264 -6.82225 -0.733885 ]
--------------------
[layer22_k_norm] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer22_k_norm] Token 0 First 3: [0.674427 -0.317378 0.470937 ]
[layer22_k_norm] Token 0 Last 3: [1.43159 -2.63046 -0.629362 ]
[layer22_k_norm] Token 1 First 3: [-0.828295 -0.61793 1.05827 ]
[layer22_k_norm] Token 1 Last 3: [1.55153 -2.88331 -0.811367 ]
[layer22_k_norm] Token 2 First 3: [-0.0989307 -0.665913 0.262598 ]
[layer22_k_norm] Token 2 Last 3: [0.382661 -2.89087 -0.612493 ]
--------------------
[layer22_wq input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer22_wq input] First 3: [6.72366 1.16349 1.90135 ]
[layer22_wq input] Last 3: [-0.884323 1.15654 -0.470845 ]
--------------------
[layer22_wq_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer22_wq_weight] First 3: [0.00172648 0.000839106 0.00413995 ]
[layer22_wq_weight] Last 3: [-0.00361077 -0.00197031 0.000957061 ]
--------------------
[layer22_wq] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer22_wq] First 3: [-0.360644 0.251273 1.15461 ]
[layer22_wq] Last 3: [1.82156 -0.0543799 -1.16766 ]
--------------------
[layer22_q_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer22_q_norm] Token 0 First 3: [-0.0653919 0.19402 0.777686 ]
[layer22_q_norm] Token 0 Last 3: [0.442977 -0.307771 -0.0931471 ]
[layer22_q_norm] Token 1 First 3: [-0.728232 0.159549 0.976109 ]
[layer22_q_norm] Token 1 Last 3: [0.772182 -1.12024 -1.03129 ]
[layer22_q_norm] Token 2 First 3: [-0.0405474 0.140121 0.279562 ]
[layer22_q_norm] Token 2 Last 3: [0.847753 -0.032603 -0.398994 ]
--------------------
[layer22_attention_query_rope] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer22_attention_query_rope] Token 0 First 3: [-0.0653919 0.19402 0.777686 ]
[layer22_attention_query_rope] Token 0 Last 3: [0.442977 -0.307771 -0.0931471 ]
[layer22_attention_query_rope] Token 1 First 3: [-3.29001 0.461515 -0.41041 ]
[layer22_attention_query_rope] Token 1 Last 3: [0.772081 -1.12029 -1.03139 ]
[layer22_attention_query_rope] Token 2 First 3: [-0.267913 -0.0303564 -0.387447 ]
[layer22_attention_query_rope] Token 2 Last 3: [0.847242 -0.0325346 -0.399235 ]
--------------------
[layer22_attention_key_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer22_attention_value_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer22_attention_attn_probs] Shape: Shape: 1:1:6:3 [ FP32 : NCHW ]

[layer22_attention_attn_probs] Token 0 First 3: [1 1 1 ]
[layer22_attention_attn_probs] Token 0 Last 3: [1 1 1 ]
[layer22_attention_attn_probs] Token 1 First 3: [0.073114 0.0660807 0.285279 ]
[layer22_attention_attn_probs] Token 1 Last 3: [0.073114 0.0660807 0.285279 ]
[layer22_attention_attn_probs] Token 2 First 3: [0.926886 0.933919 0.714721 ]
[layer22_attention_attn_probs] Token 2 Last 3: [0.926886 0.933919 0.714721 ]
[layer22_attention_attn_probs] Token 3 First 3: [0.539084 0.16294 0.282579 ]
[layer22_attention_attn_probs] Token 3 Last 3: [0.539084 0.16294 0.282579 ]
[layer22_attention_attn_probs] Token 4 First 3: [0.12742 0.218548 0.387179 ]
[layer22_attention_attn_probs] Token 4 Last 3: [0.12742 0.218548 0.387179 ]
[layer22_attention_attn_probs] Token 5 First 3: [0.333496 0.618512 0.330242 ]
[layer22_attention_attn_probs] Token 5 Last 3: [0.333496 0.618512 0.330242 ]
--------------------
[layer22_attention_attention_out_core] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer22_attention_attention_out_core] Token 0 First 3: [1.09668 0.78125 0.2323 ]
[layer22_attention_attention_out_core] Token 0 Last 3: [-0.149414 -0.571777 -0.932617 ]
[layer22_attention_attention_out_core] Token 1 First 3: [1.87874 -0.206961 0.742924 ]
[layer22_attention_attention_out_core] Token 1 Last 3: [0.631265 -1.3713 -0.0859799 ]
[layer22_attention_attention_out_core] Token 2 First 3: [1.09265 0.702556 0.050461 ]
[layer22_attention_attention_out_core] Token 2 Last 3: [0.184565 -1.16663 -0.383999 ]
--------------------
[layer22_attention_out input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer22_attention_out input] First 3: [1.09668 0.78125 0.2323 ]
[layer22_attention_out input] Last 3: [0.184565 -1.16663 -0.383999 ]
--------------------
[layer22_attention_out_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer22_attention_out_weight] First 3: [0.00224497 0.00370765 -0.00653369 ]
[layer22_attention_out_weight] Last 3: [0.0047088 -0.0129197 -0.00917186 ]
--------------------
[layer22_attention_out] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer22_attention_out] First 3: [-0.219996 0.0531494 -0.217382 ]
[layer22_attention_out] Last 3: [-0.130085 0.0804864 0.0926808 ]
--------------------
[layer22_post_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer22_post_attention_norm] Token 0 First 3: [-38.7963 55.4282 -98.6067 ]
[layer22_post_attention_norm] Token 0 Last 3: [-7.08 16.9128 -88.8672 ]
[layer22_post_attention_norm] Token 1 First 3: [-2.1993 -253.868 31.3408 ]
[layer22_post_attention_norm] Token 1 Last 3: [-5.41938 51.0264 20.7144 ]
[layer22_post_attention_norm] Token 2 First 3: [0.377016 2.7387 -7.37341 ]
[layer22_post_attention_norm] Token 2 Last 3: [-24.2589 26.0637 60.4865 ]
--------------------
[layer22_post_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer22_post_attention_norm input] Token 0 First 3: [-0.219996 0.0531494 -0.217382 ]
[layer22_post_attention_norm input] Token 0 Last 3: [-0.0321539 0.044233 -0.115323 ]
[layer22_post_attention_norm input] Token 1 First 3: [-0.00948977 -0.185235 0.0525744 ]
[layer22_post_attention_norm input] Token 1 Last 3: [-0.0187283 0.101548 0.0204548 ]
[layer22_post_attention_norm input] Token 2 First 3: [0.00252429 0.00310076 -0.0191929 ]
[layer22_post_attention_norm input] Token 2 Last 3: [-0.130085 0.0804864 0.0926808 ]
--------------------
[layer22_post_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer22_post_attention_norm_weight] First 3: [68.2414 403.556 175.531 ]
[layer22_post_attention_norm_weight] Last 3: [85.2062 147.959 298.192 ]
--------------------
[layer22pre_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer22pre_ffn_norm] Token 0 First 3: [0.526223 0.144062 0.0279399 ]
[layer22pre_ffn_norm] Token 0 Last 3: [-0.156813 0.164256 0.149031 ]
[layer22pre_ffn_norm] Token 1 First 3: [0.692748 -0.20879 0.333211 ]
[layer22pre_ffn_norm] Token 1 Last 3: [0.152641 -0.121961 0.203373 ]
[layer22pre_ffn_norm] Token 2 First 3: [0.236649 0.32254 0.0637103 ]
[layer22pre_ffn_norm] Token 2 Last 3: [-0.123105 0.124727 -0.0121353 ]
--------------------
[layer22_ffn_up input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer22_ffn_up input] First 3: [0.526223 0.144062 0.0279399 ]
[layer22_ffn_up input] Last 3: [-0.123105 0.124727 -0.0121353 ]
--------------------
[layer22_ffn_up_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer22_ffn_up_weight] First 3: [-0.00295351 -0.00943249 -0.000219961 ]
[layer22_ffn_up_weight] Last 3: [-0.00372015 0.00739233 -0.00114419 ]
--------------------
[layer22_ffn_up] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer22_ffn_up] First 3: [-0.0594707 0.0518811 -0.0107585 ]
[layer22_ffn_up] Last 3: [0.17988 0.0164445 -0.00853896 ]
--------------------
[layer22_ffn_gate input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer22_ffn_gate input] First 3: [0.526223 0.144062 0.0279399 ]
[layer22_ffn_gate input] Last 3: [-0.123105 0.124727 -0.0121353 ]
--------------------
[layer22_ffn_gate_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer22_ffn_gate_weight] First 3: [-0.00718454 1.52788e-05 -0.00503012 ]
[layer22_ffn_gate_weight] Last 3: [0.00384635 0.00109284 -0.00147796 ]
--------------------
[layer22_ffn_gate] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer22_ffn_gate] First 3: [0.0503343 0.00428508 0.0498531 ]
[layer22_ffn_gate] Last 3: [0.0185765 -0.0343364 0.00956792 ]
--------------------
[layer22_ffn_down input] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer22_ffn_down input] First 3: [-0.00155679 0.000111537 -0.000278835 ]
[layer22_ffn_down input] Last 3: [0.00169553 -0.000274588 -4.11619e-05 ]
--------------------
[layer22_ffn_down_weight] Shape: Shape: 1:1:1152:768 [ FP32 : NCHW ]

[layer22_ffn_down_weight] First 3: [-0.00697178 0.00455505 0.00540591 ]
[layer22_ffn_down_weight] Last 3: [0.00474298 0.00764626 -0.000831032 ]
--------------------
[layer22_ffn_down] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer22_ffn_down] First 3: [-0.000417246 0.000153039 -0.000285094 ]
[layer22_ffn_down] Last 3: [-0.00167968 -0.00210909 -0.000626246 ]
--------------------
[layer22post_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer22post_ffn_norm] Token 0 First 3: [-28.1869 57.6393 -28.8992 ]
[layer22post_ffn_norm] Token 0 Last 3: [-9.05654 66.261 179.384 ]
[layer22post_ffn_norm] Token 1 First 3: [-16.8201 169.429 -25.9861 ]
[layer22post_ffn_norm] Token 1 Last 3: [-72.9443 28.9249 176.735 ]
[layer22post_ffn_norm] Token 2 First 3: [-23.3122 99.578 -107.896 ]
[layer22post_ffn_norm] Token 2 Last 3: [-44.9627 -76.3164 -45.0977 ]
--------------------
[layer23_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer23_attention_norm] Token 0 First 3: [1.45593 0.739781 -0.0153817 ]
[layer23_attention_norm] Token 0 Last 3: [-0.562888 0.803671 0.814113 ]
[layer23_attention_norm] Token 1 First 3: [0.957922 -0.223576 0.4908 ]
[layer23_attention_norm] Token 1 Last 3: [-0.12684 -0.128374 0.549949 ]
[layer23_attention_norm] Token 2 First 3: [0.710911 1.87382 -0.144895 ]
[layer23_attention_norm] Token 2 Last 3: [-0.762505 0.262352 -0.127081 ]
--------------------
[layer23_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer23_attention_norm input] Token 0 First 3: [155.573 424.168 -3.84349 ]
[layer23_attention_norm input] Token 0 Last 3: [-83.5309 231.138 443.529 ]
[layer23_attention_norm input] Token 1 First 3: [140.426 -175.867 168.248 ]
[layer23_attention_norm input] Token 1 Last 3: [-25.8228 -50.6516 411.04 ]
[layer23_attention_norm input] Token 2 First 3: [78.5797 1111.38 -37.4522 ]
[layer23_attention_norm input] Token 2 Last 3: [-117.05 78.0513 -71.6176 ]
--------------------
[layer23_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer23_attention_norm_weight] First 3: [13.1849 2.45717 5.63832 ]
[layer23_attention_norm_weight] Last 3: [9.49393 4.89866 2.58603 ]
--------------------
[layer23_wv input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer23_wv input] First 3: [1.45593 0.739781 -0.0153817 ]
[layer23_wv input] Last 3: [-0.762505 0.262352 -0.127081 ]
--------------------
[layer23_wv_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer23_wv_weight] First 3: [-0.000550023 -0.0045282 0.0026544 ]
[layer23_wv_weight] Last 3: [-0.000134221 -0.0128389 -0.0131741 ]
--------------------
[layer23_wv] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer23_wv] First 3: [0.180592 0.100757 0.577276 ]
[layer23_wv] Last 3: [0.885496 0.554588 0.653632 ]
--------------------
[layer23_wk input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer23_wk input] First 3: [1.45593 0.739781 -0.0153817 ]
[layer23_wk input] Last 3: [-0.762505 0.262352 -0.127081 ]
--------------------
[layer23_wk_weight] Shape: Shape: 1:1:768:256 [ FP32 : NCHW ]

[layer23_wk_weight] First 3: [-0.00714521 0.00807089 -0.00306184 ]
[layer23_wk_weight] Last 3: [-0.00273118 -0.0081204 0.00422632 ]
--------------------
[layer23_wk] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer23_wk] First 3: [-0.0355944 -0.500342 -0.691308 ]
[layer23_wk] Last 3: [0.306637 3.04318 -0.401902 ]
--------------------
[layer23_k_norm] Shape: Shape: 1:1:3:256 [ FP32 : NCHW ]

[layer23_k_norm] Token 0 First 3: [-0.000441336 -0.0126075 1.75714 ]
[layer23_k_norm] Token 0 Last 3: [6.80989 1.94014 -1.67257 ]
[layer23_k_norm] Token 1 First 3: [-0.00335463 -0.0115304 1.62721 ]
[layer23_k_norm] Token 1 Last 3: [4.39664 1.18494 -5.56482 ]
[layer23_k_norm] Token 2 First 3: [-0.00505348 -0.0261375 0.36997 ]
[layer23_k_norm] Token 2 Last 3: [1.59044 7.12637 -0.937108 ]
--------------------
[layer23_wq input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer23_wq input] First 3: [1.45593 0.739781 -0.0153817 ]
[layer23_wq input] Last 3: [-0.762505 0.262352 -0.127081 ]
--------------------
[layer23_wq_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer23_wq_weight] First 3: [0.00573669 -0.0052358 0.00516688 ]
[layer23_wq_weight] Last 3: [-0.00414325 0.00199415 0.00988872 ]
--------------------
[layer23_wq] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer23_wq] First 3: [-0.501959 3.26757 0.938283 ]
[layer23_wq] Last 3: [-0.183558 1.46914 0.794102 ]
--------------------
[layer23_q_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer23_q_norm] Token 0 First 3: [-0.240337 -0.00229241 -1.03261 ]
[layer23_q_norm] Token 0 Last 3: [-0.117101 0.267699 -0.0471438 ]
[layer23_q_norm] Token 1 First 3: [0.0812709 -0.00286232 -1.2915 ]
[layer23_q_norm] Token 1 Last 3: [-0.0292177 0.0753276 0.136621 ]
[layer23_q_norm] Token 2 First 3: [-0.0997817 -0.00218027 -0.998331 ]
[layer23_q_norm] Token 2 Last 3: [-0.029919 0.396816 0.297982 ]
--------------------
[layer23_attention_query_rope] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer23_attention_query_rope] Token 0 First 3: [-0.240337 -0.00229241 -1.03261 ]
[layer23_attention_query_rope] Token 0 Last 3: [-0.117101 0.267699 -0.0471438 ]
[layer23_attention_query_rope] Token 1 First 3: [0.912418 0.0038573 -0.567372 ]
[layer23_attention_query_rope] Token 1 Last 3: [-0.0292319 0.0753494 0.136569 ]
[layer23_attention_query_rope] Token 2 First 3: [0.989883 0.00827387 0.906601 ]
[layer23_attention_query_rope] Token 2 Last 3: [-0.0300189 0.396811 0.297923 ]
--------------------
[layer23_attention_key_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer23_attention_value_rope] Shape: Shape: 1:1:3:256 [ UINT16 : NCHW ]

--------------------
[layer23_attention_attn_probs] Shape: Shape: 1:1:6:3 [ FP32 : NCHW ]

[layer23_attention_attn_probs] Token 0 First 3: [1 1 1 ]
[layer23_attention_attn_probs] Token 0 Last 3: [1 1 1 ]
[layer23_attention_attn_probs] Token 1 First 3: [0.0558119 0.0852398 0.0617068 ]
[layer23_attention_attn_probs] Token 1 Last 3: [0.0558119 0.0852398 0.0617068 ]
[layer23_attention_attn_probs] Token 2 First 3: [0.944188 0.91476 0.938293 ]
[layer23_attention_attn_probs] Token 2 Last 3: [0.944188 0.91476 0.938293 ]
[layer23_attention_attn_probs] Token 3 First 3: [0.0719237 0.140349 0.115613 ]
[layer23_attention_attn_probs] Token 3 Last 3: [0.0719237 0.140349 0.115613 ]
[layer23_attention_attn_probs] Token 4 First 3: [0.186127 0.38126 0.301386 ]
[layer23_attention_attn_probs] Token 4 Last 3: [0.186127 0.38126 0.301386 ]
[layer23_attention_attn_probs] Token 5 First 3: [0.74195 0.47839 0.583001 ]
[layer23_attention_attn_probs] Token 5 Last 3: [0.74195 0.47839 0.583001 ]
--------------------
[layer23_attention_attention_out_core] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer23_attention_attention_out_core] Token 0 First 3: [0.180542 0.100769 0.577148 ]
[layer23_attention_attention_out_core] Token 0 Last 3: [0.000741482 -0.0700684 0.0175018 ]
[layer23_attention_attention_out_core] Token 1 First 3: [-0.00745714 0.0802532 0.208094 ]
[layer23_attention_attention_out_core] Token 1 Last 3: [0.0173696 -0.062652 -0.175652 ]
[layer23_attention_attention_out_core] Token 2 First 3: [-0.0587156 -0.0599613 1.07752 ]
[layer23_attention_attention_out_core] Token 2 Last 3: [0.521754 0.296547 0.326427 ]
--------------------
[layer23_attention_out input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer23_attention_out input] First 3: [0.180542 0.100769 0.577148 ]
[layer23_attention_out input] Last 3: [0.521754 0.296547 0.326427 ]
--------------------
[layer23_attention_out_weight] Shape: Shape: 1:1:768:768 [ FP32 : NCHW ]

[layer23_attention_out_weight] First 3: [0.0109306 0.00519522 0.0165998 ]
[layer23_attention_out_weight] Last 3: [0.00792366 -0.00677993 0.00140562 ]
--------------------
[layer23_attention_out] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer23_attention_out] First 3: [-0.0805983 -0.0341419 -0.0330223 ]
[layer23_attention_out] Last 3: [0.0412772 -0.0131423 -0.0635043 ]
--------------------
[layer23_post_attention_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer23_post_attention_norm] Token 0 First 3: [-33.6002 -85.48 -19.1661 ]
[layer23_post_attention_norm] Token 0 Last 3: [43.967 -43.7332 -43.7367 ]
[layer23_post_attention_norm] Token 1 First 3: [-48.5188 76.5225 36.6606 ]
[layer23_post_attention_norm] Token 1 Last 3: [28.8391 -16.0966 12.7864 ]
[layer23_post_attention_norm] Token 2 First 3: [-48.9541 -39.4722 36.0068 ]
[layer23_post_attention_norm] Token 2 Last 3: [10.0469 -5.56373 -48.5508 ]
--------------------
[layer23_post_attention_norm input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer23_post_attention_norm input] Token 0 First 3: [-0.0805983 -0.0341419 -0.0330223 ]
[layer23_post_attention_norm input] Token 0 Last 3: [0.0863975 -0.0494097 -0.0273621 ]
[layer23_post_attention_norm input] Token 1 First 3: [-0.0785388 0.0206254 0.042625 ]
[layer23_post_attention_norm input] Token 1 Last 3: [0.0382426 -0.0122723 0.00539811 ]
[layer23_post_attention_norm input] Token 2 First 3: [-0.245514 -0.0329623 0.129706 ]
[layer23_post_attention_norm input] Token 2 Last 3: [0.0412772 -0.0131423 -0.0635043 ]
--------------------
[layer23_post_attention_norm_weight] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[layer23_post_attention_norm_weight] First 3: [80.8633 485.637 112.58 ]
[layer23_post_attention_norm_weight] Last 3: [98.7098 171.686 310.05 ]
--------------------
[layer23pre_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer23pre_ffn_norm] Token 0 First 3: [0.572439 0.18386 -0.0390267 ]
[layer23pre_ffn_norm] Token 0 Last 3: [-0.201255 0.273926 0.32804 ]
[layer23pre_ffn_norm] Token 1 First 3: [0.32556 -0.0407049 0.262318 ]
[layer23pre_ffn_norm] Token 1 Last 3: [0.0115807 -0.0736388 0.26248 ]
[layer23pre_ffn_norm] Token 2 First 3: [0.132353 0.553918 -0.00233374 ]
[layer23pre_ffn_norm] Token 2 Last 3: [-0.518133 0.100859 -0.0938605 ]
--------------------
[layer23_ffn_up input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer23_ffn_up input] First 3: [0.572439 0.18386 -0.0390267 ]
[layer23_ffn_up input] Last 3: [-0.518133 0.100859 -0.0938605 ]
--------------------
[layer23_ffn_up_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer23_ffn_up_weight] First 3: [-0.00420907 -0.00513976 -0.00933616 ]
[layer23_ffn_up_weight] Last 3: [0.000952771 0.0141273 0.00325199 ]
--------------------
[layer23_ffn_up] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer23_ffn_up] First 3: [0.0193448 0.0320087 -0.048487 ]
[layer23_ffn_up] Last 3: [0.0610987 0.0152325 -0.047648 ]
--------------------
[layer23_ffn_gate input] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer23_ffn_gate input] First 3: [0.572439 0.18386 -0.0390267 ]
[layer23_ffn_gate input] Last 3: [-0.518133 0.100859 -0.0938605 ]
--------------------
[layer23_ffn_gate_weight] Shape: Shape: 1:1:768:1152 [ FP32 : NCHW ]

[layer23_ffn_gate_weight] First 3: [-0.00661951 9.38185e-05 -0.00309384 ]
[layer23_ffn_gate_weight] Last 3: [0.00345216 0.00447924 0.00245323 ]
--------------------
[layer23_ffn_gate] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer23_ffn_gate] First 3: [0.0133918 -0.0381492 -0.195954 ]
[layer23_ffn_gate] Last 3: [0.550007 -0.234392 -0.106169 ]
--------------------
[layer23_ffn_down input] Shape: Shape: 1:1:3:1152 [ FP32 : NCHW ]

[layer23_ffn_down input] First 3: [0.000130914 -0.000591974 0.00401261 ]
[layer23_ffn_down input] Last 3: [0.023819 -0.00145437 0.00231551 ]
--------------------
[layer23_ffn_down_weight] Shape: Shape: 1:1:1152:768 [ FP32 : NCHW ]

[layer23_ffn_down_weight] First 3: [0.00617235 -0.0173638 0.00365379 ]
[layer23_ffn_down_weight] Last 3: [0.005383 0.00965743 0.00663115 ]
--------------------
[layer23_ffn_down] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer23_ffn_down] First 3: [0.00125649 -0.00041196 -0.00308774 ]
[layer23_ffn_down] Last 3: [0.00345006 -0.0145149 -0.00649811 ]
--------------------
[layer23post_ffn_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[layer23post_ffn_norm] Token 0 First 3: [28.1756 -33.8504 -101.644 ]
[layer23post_ffn_norm] Token 0 Last 3: [-15.5765 -79.6868 157.079 ]
[layer23post_ffn_norm] Token 1 First 3: [-12.1734 -43.6978 -45.4404 ]
[layer23post_ffn_norm] Token 1 Last 3: [-18.3585 -111.722 -110.531 ]
[layer23post_ffn_norm] Token 2 First 3: [23.6209 -38.1229 -21.5984 ]
[layer23post_ffn_norm] Token 2 Last 3: [45.317 -166.236 -141.862 ]
--------------------
[output_norm] Shape: Shape: 1:1:3:768 [ FP32 : NCHW ]

[output_norm] Token 0 First 3: [2.86007 1.06433 -1.01061 ]
[output_norm] Token 0 Last 3: [-1.37321 0.84344 2.13575 ]
[output_norm] Token 1 First 3: [0.864811 -0.284377 0.736165 ]
[output_norm] Token 1 Last 3: [-0.217561 -0.795709 0.684184 ]
[output_norm] Token 2 First 3: [1.42177 5.05964 -0.261887 ]
[output_norm] Token 2 Last 3: [-2.15344 -1.02899 -1.40873 ]
--------------------
[fully_connected1 input] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[fully_connected1 input] First 3: [1.71555 1.94653 -0.178776 ]
[fully_connected1 input] Last 3: [-1.24807 -0.327085 0.470401 ]
--------------------
[fully_connected1_weight] Shape: Shape: 1:1:768:3072 [ FP32 : NCHW ]

[fully_connected1_weight] First 3: [-0.024782 -0.0503029 0.10914 ]
[fully_connected1_weight] Last 3: [0.0940023 -0.139223 0.0289109 ]
--------------------
[fully_connected1] Shape: Shape: 1:1:1:3072 [ FP32 : NCHW ]

[fully_connected1] First 3: [1.13797 -3.22019 -9.42822 ]
[fully_connected1] Last 3: [1.75002 1.56106 4.01779 ]
--------------------
[fully_connected2 input] Shape: Shape: 1:1:1:3072 [ FP32 : NCHW ]

[fully_connected2 input] First 3: [1.13797 -3.22019 -9.42822 ]
[fully_connected2 input] Last 3: [1.75002 1.56106 4.01779 ]
--------------------
[fully_connected2_weight] Shape: Shape: 1:1:3072:768 [ FP32 : NCHW ]

[fully_connected2_weight] First 3: [0.06444 -0.0256345 -0.0134667 ]
[fully_connected2_weight] Last 3: [0.0687252 -0.0441383 -0.0533497 ]
--------------------
[fully_connected2] Shape: Shape: 1:1:1:768 [ FP32 : NCHW ]

[fully_connected2] First 3: [-218.303 -4.37666 28.7344 ]
[fully_connected2] Last 3: [-80.5331 -23.292 2.67842 ]
--------------------
Embedding Result (1 batch(es)):
Batch 0: [-0.207577, -0.00416162, 0.0273226, 0.00333201, 0.0139069, 0.0113238, -0.0435704, 0.0261169, 0.0373047, -0.0619011, ...] (Total DIM: 768)
Max Resident Set Size: 1551792 KB
