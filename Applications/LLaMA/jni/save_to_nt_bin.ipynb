{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e750ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# w1 = torch.load('./llama_7b/pytorch_model-00001-of-00002.bin')\n",
    "# w2 = torch.load('./llama_7b/pytorch_model-00002-of-00002.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f04f5679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import fairscale.nn.model_parallel.initialize as fs_init\n",
    "from fairscale.nn.model_parallel.initialize import (\n",
    "    get_model_parallel_rank,\n",
    "    initialize_model_parallel,\n",
    "    model_parallel_is_initialized,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from fairscale.nn.model_parallel.layers import (\n",
    "    ColumnParallelLinear,\n",
    "    ParallelEmbedding,\n",
    "    RowParallelLinear,\n",
    ")\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "seed = 2021\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 4096\n",
    "    n_layers: int = 2\n",
    "    n_heads: int = 2\n",
    "    n_kv_heads: Optional[int] = None\n",
    "    vocab_size: int = 2  # defined later by tokenizer\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    norm_eps: float = 1e-5\n",
    "\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 2048\n",
    "\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)  # type: ignore\n",
    "    freqs = torch.outer(t, freqs).float()  # type: ignore\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n",
    "    bs, slen, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:, :, :, None, :]\n",
    "        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n",
    "        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n",
    "    )\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        model_parallel_size = 1 # fs_init.get_model_parallel_world_size()\n",
    "        self.n_local_heads = args.n_heads // model_parallel_size\n",
    "        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.wq = nn.Linear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False\n",
    "        )\n",
    "        self.wk = nn.Linear(\n",
    "            args.dim,\n",
    "            self.n_kv_heads * self.head_dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.wv = nn.Linear(\n",
    "            args.dim,\n",
    "            self.n_kv_heads * self.head_dim,\n",
    "            bias=False\n",
    "        )\n",
    "        self.wo = nn.Linear(\n",
    "            args.n_heads * self.head_dim,\n",
    "            args.dim,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        self.cache_k = torch.zeros(\n",
    "            (\n",
    "                args.max_batch_size,\n",
    "                args.max_seq_len,\n",
    "                self.n_local_kv_heads,\n",
    "                self.head_dim,\n",
    "            )\n",
    "        )\n",
    "        self.cache_v = torch.zeros(\n",
    "            (\n",
    "                args.max_batch_size,\n",
    "                args.max_seq_len,\n",
    "                self.n_local_kv_heads,\n",
    "                self.head_dim,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        start_pos: int,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "    ):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "#         print('output of xq values:', xq.shape, xq)\n",
    "#         print('output of xk values:', xk.shape, xk)\n",
    "#         print('output of xv values:', xv.shape, xv)\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "        \n",
    "        self.cache_k = self.cache_k.to(xq)\n",
    "        self.cache_v = self.cache_v.to(xq)\n",
    "\n",
    "        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n",
    "        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n",
    "        \n",
    "        keys = self.cache_k[:bsz, : start_pos + seqlen]\n",
    "        values = self.cache_v[:bsz, : start_pos + seqlen]\n",
    "#         print('output of xq cache values:', xq.shape, xq)\n",
    "#         print('output of xk cache values:', xk.shape, xk)\n",
    "\n",
    "        # repeat k/v heads if n_kv_heads < n_heads\n",
    "        keys = repeat_kv(keys, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)\n",
    "        values = repeat_kv(values, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)\n",
    "\n",
    "        xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        print('output of xq values:', xq.shape, xq)\n",
    "        print('output of xk values:', keys.shape, keys)\n",
    "        print('output of xv values:', values.shape, values)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "#         if mask is not None:\n",
    "#             scores = scores + mask  # (bs, n_local_heads, seqlen, cache_len + seqlen)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        output = torch.matmul(scores, values)  # (bs, n_local_heads, seqlen, head_dim)\n",
    "        print('output of dot-query-attention:', output.shape, output)\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        print('output of dot-query-attention transposed:', output.shape, output)\n",
    "        output = output.view(bsz, seqlen, -1)\n",
    "        print('output of dot-query-attention flattened:', output.shape, output)\n",
    "        output = self.wo(output)\n",
    "        print('output of attention layer:', output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        multiple_of: int,\n",
    "        ffn_dim_multiplier: Optional[float],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        # custom dim factor multiplier\n",
    "        if ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "        \n",
    "        self.w1 = nn.Linear(\n",
    "            dim, hidden_dim, bias=False\n",
    "        )\n",
    "        self.w2 = nn.Linear(\n",
    "            hidden_dim, dim, bias=False\n",
    "        )\n",
    "        self.w3 = nn.Linear(\n",
    "            dim, hidden_dim, bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('ffn_w1 input:', x)\n",
    "        our_w1 = self.w1(x)\n",
    "        our_w2 = self.w3(x)\n",
    "        our_w3 = self.w2(F.silu(our_w1) * our_w2)\n",
    "        print('our ffn_w1:', our_w1.shape, our_w1)\n",
    "        print('our ffn_w2:', our_w2.shape, our_w2)\n",
    "        print('our ffn_w3:', our_w3.shape, our_w3)\n",
    "        return our_w3\n",
    "        # return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.attention = Attention(args)\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim=args.dim,\n",
    "            hidden_dim=4 * args.dim,\n",
    "            multiple_of=args.multiple_of,\n",
    "            ffn_dim_multiplier=args.ffn_dim_multiplier,\n",
    "        )\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        start_pos: int,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "    ):\n",
    "        print('output of attention_norm', self.attention_norm(x))\n",
    "        h = x + self.attention.forward(\n",
    "            self.attention_norm(x), start_pos, freqs_cis, mask\n",
    "        )\n",
    "        \n",
    "        print('output of ffn_norm:', self.ffn_norm(h))\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, params: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.vocab_size = params.vocab_size\n",
    "        self.n_layers = params.n_layers\n",
    "\n",
    "        self.tok_embeddings = nn.Embedding(\n",
    "            params.vocab_size, params.dim\n",
    "        )\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(params.n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id, params))\n",
    "\n",
    "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
    "        self.output = nn.Linear(\n",
    "            params.dim, params.vocab_size, bias=False\n",
    "        )\n",
    "\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            self.params.dim // self.params.n_heads, self.params.max_seq_len * 2\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
    "        _bsz, seqlen = tokens.shape\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        # print('output of embedding:', h)\n",
    "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
    "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
    "\n",
    "        mask = None\n",
    "        if seqlen > 1:\n",
    "            mask = torch.full(\n",
    "                (1, 1, seqlen, seqlen), float(\"-inf\"), device=tokens.device\n",
    "            )\n",
    "            mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_cis, mask)\n",
    "        h = self.norm(h)\n",
    "        print('output of out_norm: ', h)\n",
    "        output = self.output(h).float()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f55018a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ModelArgs()\n",
    "model = Transformer(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d871f75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input of embedding: tensor([[0, 1]])\n",
      "output of attention_norm tensor([[[-0.1475,  0.9904,  0.6120,  ..., -1.4483, -1.0170,  0.4832],\n",
      "         [ 0.0691,  1.2589, -1.7623,  ...,  0.1090,  0.0561, -0.2484]]])\n",
      "output of xq values: torch.Size([1, 2, 2, 2048]) tensor([[[[-0.5556,  0.5313, -0.2729,  ...,  0.1867, -0.0459,  0.7846],\n",
      "          [-0.3013, -0.2177,  0.0854,  ...,  0.5726,  0.4144,  0.6117]],\n",
      "\n",
      "         [[-0.5060,  1.3699,  0.3650,  ..., -0.0445, -0.8422, -0.7842],\n",
      "          [ 0.0044,  0.0099,  0.2998,  ..., -0.9850,  0.0838,  0.3099]]]])\n",
      "output of xk values: torch.Size([1, 2, 2, 2048]) tensor([[[[ 0.0187, -0.3398, -0.0273,  ..., -0.4150, -0.9774,  0.2375],\n",
      "          [-0.1316,  0.2630, -0.1119,  ...,  1.0869,  0.1280,  0.0048]],\n",
      "\n",
      "         [[ 0.6447,  0.4425,  0.3625,  ..., -0.6063, -0.1210,  0.6284],\n",
      "          [-0.1395,  0.8152, -0.4388,  ..., -0.0609,  0.0440, -1.2018]]]])\n",
      "output of xv values: torch.Size([1, 2, 2, 2048]) tensor([[[[ 1.0623,  0.3809,  0.2658,  ...,  0.5437, -0.1955,  0.5064],\n",
      "          [-0.3912, -0.6001,  0.4004,  ...,  0.4121, -0.2116, -0.9573]],\n",
      "\n",
      "         [[ 0.0171, -0.1632,  0.5169,  ...,  0.6872,  0.7183, -0.3094],\n",
      "          [ 1.0892,  1.3322, -0.0701,  ..., -0.5476,  0.0993, -0.2363]]]])\n",
      "output of dot-query-attention: torch.Size([1, 2, 2, 2048]) tensor([[[[ 0.1414, -0.2406,  0.3510,  ...,  0.4603, -0.2057, -0.4209],\n",
      "          [ 0.5709,  0.0493,  0.3113,  ...,  0.4992, -0.2010,  0.0116]],\n",
      "\n",
      "         [[ 0.7371,  0.8411,  0.1227,  ..., -0.1421,  0.3026, -0.2603],\n",
      "          [ 0.6560,  0.7281,  0.1671,  ..., -0.0487,  0.3494, -0.2658]]]])\n",
      "output of dot-query-attention transposed: torch.Size([1, 2, 2, 2048]) tensor([[[[ 0.1414, -0.2406,  0.3510,  ...,  0.4603, -0.2057, -0.4209],\n",
      "          [ 0.7371,  0.8411,  0.1227,  ..., -0.1421,  0.3026, -0.2603]],\n",
      "\n",
      "         [[ 0.5709,  0.0493,  0.3113,  ...,  0.4992, -0.2010,  0.0116],\n",
      "          [ 0.6560,  0.7281,  0.1671,  ..., -0.0487,  0.3494, -0.2658]]]])\n",
      "output of dot-query-attention flattened: torch.Size([1, 2, 4096]) tensor([[[ 0.1414, -0.2406,  0.3510,  ..., -0.1421,  0.3026, -0.2603],\n",
      "         [ 0.5709,  0.0493,  0.3113,  ..., -0.0487,  0.3494, -0.2658]]])\n",
      "output of attention layer: tensor([[[ 0.0076,  0.3680,  0.1578,  ..., -0.1560, -0.2015,  0.2143],\n",
      "         [ 0.0377,  0.3148,  0.1327,  ..., -0.1918, -0.1449,  0.1789]]])\n",
      "output of ffn_norm: tensor([[[-0.1360,  1.3270,  0.7513,  ..., -1.5630, -1.1884,  0.6818],\n",
      "         [ 0.1042,  1.5354, -1.5897,  ..., -0.0809, -0.0867, -0.0677]]])\n",
      "ffn_w1 input: tensor([[[-0.1360,  1.3270,  0.7513,  ..., -1.5630, -1.1884,  0.6818],\n",
      "         [ 0.1042,  1.5354, -1.5897,  ..., -0.0809, -0.0867, -0.0677]]])\n",
      "our ffn_w1: torch.Size([1, 2, 11008]) tensor([[[-1.2888, -0.2329,  0.3455,  ..., -0.0253,  0.3137,  0.0696],\n",
      "         [ 0.5508,  0.0052,  0.4122,  ..., -0.3309,  0.2219,  0.8136]]])\n",
      "our ffn_w2: torch.Size([1, 2, 11008]) tensor([[[ 0.2816,  0.5240,  0.2137,  ..., -0.0203, -0.9595, -0.4044],\n",
      "         [-0.0826,  0.2926, -0.6767,  ...,  0.4012,  0.5932, -0.1037]]])\n",
      "our ffn_w3: torch.Size([1, 2, 4096]) tensor([[[ 0.1338,  0.0076,  0.1397,  ...,  0.1107, -0.0264,  0.0690],\n",
      "         [-0.1447, -0.0889,  0.0200,  ...,  0.0704,  0.0075, -0.2108]]])\n",
      "output of attention_norm tensor([[[-0.0038,  1.3301,  0.8863,  ..., -1.4486, -1.2104,  0.7475],\n",
      "         [-0.0368,  1.4400, -1.5608,  ..., -0.0120, -0.0789, -0.2719]]])\n",
      "output of xq values: torch.Size([1, 2, 2, 2048]) tensor([[[[-0.0344,  0.4197,  0.4048,  ...,  0.3169, -0.5180, -0.1788],\n",
      "          [-0.9061, -0.2753, -0.7891,  ...,  0.6786,  0.1036, -0.2330]],\n",
      "\n",
      "         [[-0.2652, -1.7345, -0.1367,  ..., -0.0141, -0.1161, -0.0060],\n",
      "          [ 0.1312,  0.0260,  0.8507,  ..., -0.3669,  0.1606, -0.6743]]]])\n",
      "output of xk values: torch.Size([1, 2, 2, 2048]) tensor([[[[-0.0569, -1.0149, -0.5469,  ...,  0.9611, -0.1484, -0.6570],\n",
      "          [-0.2831,  0.4345, -0.2209,  ...,  0.2907,  0.4002, -0.9128]],\n",
      "\n",
      "         [[-0.3688,  0.0895, -0.7468,  ..., -0.6141,  0.2539,  0.3245],\n",
      "          [ 0.8081,  0.9349, -0.9444,  ..., -0.3824, -0.2276, -0.8097]]]])\n",
      "output of xv values: torch.Size([1, 2, 2, 2048]) tensor([[[[ 0.4919,  0.6787, -0.0489,  ..., -0.0177,  0.6364,  0.7939],\n",
      "          [ 0.6877,  0.1593, -0.5129,  ..., -0.4801, -0.7986,  0.7971]],\n",
      "\n",
      "         [[-0.4032, -0.6109,  0.5032,  ..., -0.7182,  0.7269, -0.5631],\n",
      "          [-0.4697, -0.4968, -0.1947,  ..., -0.1552,  0.5194, -0.5299]]]])\n",
      "output of dot-query-attention: torch.Size([1, 2, 2, 2048]) tensor([[[[ 0.6090,  0.3680, -0.3265,  ..., -0.2944, -0.2222,  0.7958],\n",
      "          [ 0.6110,  0.3629, -0.3310,  ..., -0.2989, -0.2361,  0.7958]],\n",
      "\n",
      "         [[-0.4346, -0.5570,  0.1739,  ..., -0.4525,  0.6290, -0.5474],\n",
      "          [-0.4287, -0.5671,  0.2356,  ..., -0.5023,  0.6473, -0.5504]]]])\n",
      "output of dot-query-attention transposed: torch.Size([1, 2, 2, 2048]) tensor([[[[ 0.6090,  0.3680, -0.3265,  ..., -0.2944, -0.2222,  0.7958],\n",
      "          [-0.4346, -0.5570,  0.1739,  ..., -0.4525,  0.6290, -0.5474]],\n",
      "\n",
      "         [[ 0.6110,  0.3629, -0.3310,  ..., -0.2989, -0.2361,  0.7958],\n",
      "          [-0.4287, -0.5671,  0.2356,  ..., -0.5023,  0.6473, -0.5504]]]])\n",
      "output of dot-query-attention flattened: torch.Size([1, 2, 4096]) tensor([[[ 0.6090,  0.3680, -0.3265,  ..., -0.4525,  0.6290, -0.5474],\n",
      "         [ 0.6110,  0.3629, -0.3310,  ..., -0.5023,  0.6473, -0.5504]]])\n",
      "output of attention layer: tensor([[[-0.4178, -0.1223,  0.4822,  ..., -0.1668,  0.0764,  0.0929],\n",
      "         [-0.4215, -0.1394,  0.4859,  ..., -0.1092,  0.1163,  0.1066]]])\n",
      "output of ffn_norm: tensor([[[-0.4040,  1.1769,  1.3244,  ..., -1.5693, -1.1044,  0.8163],\n",
      "         [-0.4341,  1.2704, -1.0606,  ..., -0.1149,  0.0331, -0.1640]]])\n",
      "ffn_w1 input: tensor([[[-0.4040,  1.1769,  1.3244,  ..., -1.5693, -1.1044,  0.8163],\n",
      "         [-0.4341,  1.2704, -1.0606,  ..., -0.1149,  0.0331, -0.1640]]])\n",
      "our ffn_w1: torch.Size([1, 2, 11008]) tensor([[[-0.1053,  0.2265,  1.2946,  ...,  0.5814,  0.9288, -0.5885],\n",
      "         [ 0.9810,  0.3665, -0.6036,  ...,  0.7272,  0.0780, -0.0301]]])\n",
      "our ffn_w2: torch.Size([1, 2, 11008]) tensor([[[ 0.0426, -0.9825,  0.3248,  ...,  0.6142,  0.0762, -0.2250],\n",
      "         [ 0.3937, -0.9402,  0.3668,  ...,  0.4052,  0.4925, -0.2271]]])\n",
      "our ffn_w3: torch.Size([1, 2, 4096]) tensor([[[-0.1068, -0.0937,  0.0643,  ...,  0.0879, -0.1069, -0.0163],\n",
      "         [ 0.1212, -0.1134, -0.1203,  ..., -0.0230, -0.1077, -0.1378]]])\n",
      "output of out_norm:  tensor([[[-0.5031,  1.0801,  1.3771,  ..., -1.4755, -1.1991,  0.7956],\n",
      "         [-0.3173,  1.1551, -1.1660,  ..., -0.1357, -0.0681, -0.2922]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3023, -0.1086],\n",
       "         [-0.7458,  0.0938]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.zeros((1, 2), dtype=torch.long)\n",
    "X[0][0] = 0\n",
    "X[0][1] = 1\n",
    "\n",
    "print('input of embedding:', X)\n",
    "model(X, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a08cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c4b8214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['norm.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "478347a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.state_dict()['tok_embeddings.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36f50e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['tok_embeddings.weight', 'layers.0.attention.wq.weight', 'layers.0.attention.wk.weight', 'layers.0.attention.wv.weight', 'layers.0.attention.wo.weight', 'layers.0.feed_forward.w1.weight', 'layers.0.feed_forward.w2.weight', 'layers.0.feed_forward.w3.weight', 'layers.0.attention_norm.weight', 'layers.0.ffn_norm.weight', 'layers.1.attention.wq.weight', 'layers.1.attention.wk.weight', 'layers.1.attention.wv.weight', 'layers.1.attention.wo.weight', 'layers.1.feed_forward.w1.weight', 'layers.1.feed_forward.w2.weight', 'layers.1.feed_forward.w3.weight', 'layers.1.attention_norm.weight', 'layers.1.ffn_norm.weight', 'norm.weight', 'output.weight'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "573085c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 11008])\n",
      "torch.Size([4096, 11008])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict()['layers.0.feed_forward.w1.weight'].transpose(1, 0).shape)\n",
    "print(model.state_dict()['layers.0.feed_forward.w3.weight'].transpose(1, 0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b14bc055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "file = open(\"/home/seungbaek/hdd/projects/nntrainer/build/Applications/LLaMA/jni/llama_v2.bin\", \"wb\")\n",
    "\n",
    "args = ModelArgs()\n",
    "\n",
    "def save_llama_to_bin(params, n_layer = 32, n_head = 32, args=[]):\n",
    "    def save_weight(weight):\n",
    "        np.array(weight).tofile(file)\n",
    "\n",
    "    def save_embedding(weight):\n",
    "        save_weight(weight)\n",
    "\n",
    "    def save_attention(weights, layer_name, n_head = 32):        \n",
    "        save_weight(params[layer_name + 'attention_norm' + '.weight'])\n",
    "        split_size = (args.dim // n_head)\n",
    "        for head_idx in range(1, n_head+1):            \n",
    "            st_idx = (args.dim - split_size * head_idx)\n",
    "            end_idx = st_idx + split_size\n",
    "            save_weight(params[layer_name + 'attention.wv' + '.weight'][st_idx:end_idx, :].permute(1, 0))\n",
    "            \n",
    "        for head_idx in range(1, n_head+1):\n",
    "            st_idx = (args.dim - split_size * head_idx)\n",
    "            end_idx = st_idx + split_size\n",
    "            save_weight(params[layer_name + 'attention.wk' + '.weight'][st_idx:end_idx, :].permute(1, 0))\n",
    "\n",
    "        for head_idx in range(1, n_head+1):\n",
    "            st_idx = (args.dim - split_size * head_idx)\n",
    "            end_idx = st_idx + split_size\n",
    "            save_weight(params[layer_name + 'attention.wq' + '.weight'][st_idx:end_idx, :].permute(1, 0)) # It includes multiple heads\n",
    "        \n",
    "        save_weight(params[layer_name + 'attention.wo' + '.weight'].permute(1, 0))\n",
    "\n",
    "    def save_feed_forward(weights, layer_name):\n",
    "        save_weight(params[layer_name + 'ffn_norm' + '.weight'])        \n",
    "        \n",
    "        save_weight(params[layer_name + 'feed_forward.w3' + '.weight'].permute(1, 0))\n",
    "        save_weight(params[layer_name + 'feed_forward.w1' + '.weight'].permute(1, 0))        \n",
    "        save_weight(params[layer_name + 'feed_forward.w2' + '.weight'].permute(1, 0))\n",
    "\n",
    "    # save weights of embedding layer\n",
    "    save_embedding(params['tok_embeddings.weight'])\n",
    "    \n",
    "    # save weights of attention layers\n",
    "    for layer_idx in range(n_layer):\n",
    "        save_attention(params, 'layers.{}.'.format(layer_idx), n_head)\n",
    "        save_feed_forward(params, 'layers.{}.'.format(layer_idx))\n",
    "        \n",
    "    save_weight(params['norm.weight'])\n",
    "    \n",
    "    save_weight(params['output.weight'].permute(1, 0))\n",
    "    \n",
    "save_llama_to_bin(model.state_dict(), n_layer = params.n_layers, n_head = params.n_heads, args=args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
