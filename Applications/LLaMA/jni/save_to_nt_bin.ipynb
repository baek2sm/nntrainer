{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e750ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# w1 = torch.load('./llama_7b/pytorch_model-00001-of-00002.bin')\n",
    "# w2 = torch.load('./llama_7b/pytorch_model-00002-of-00002.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f04f5679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import fairscale.nn.model_parallel.initialize as fs_init\n",
    "from fairscale.nn.model_parallel.initialize import (\n",
    "    get_model_parallel_rank,\n",
    "    initialize_model_parallel,\n",
    "    model_parallel_is_initialized,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from fairscale.nn.model_parallel.layers import (\n",
    "    ColumnParallelLinear,\n",
    "    ParallelEmbedding,\n",
    "    RowParallelLinear,\n",
    ")\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "seed = 2021\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 4096\n",
    "    n_layers: int = 1\n",
    "    n_heads: int = 1\n",
    "    n_kv_heads: Optional[int] = None\n",
    "    vocab_size: int = 2  # defined later by tokenizer\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    norm_eps: float = 1e-5\n",
    "\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 2048\n",
    "\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)  # type: ignore\n",
    "    freqs = torch.outer(t, freqs).float()  # type: ignore\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n",
    "    bs, slen, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:, :, :, None, :]\n",
    "        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n",
    "        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n",
    "    )\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        model_parallel_size = 1 # fs_init.get_model_parallel_world_size()\n",
    "        self.n_local_heads = args.n_heads // model_parallel_size\n",
    "        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.wq = nn.Linear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False\n",
    "        )\n",
    "        self.wk = nn.Linear(\n",
    "            args.dim,\n",
    "            self.n_kv_heads * self.head_dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.wv = nn.Linear(\n",
    "            args.dim,\n",
    "            self.n_kv_heads * self.head_dim,\n",
    "            bias=False\n",
    "        )\n",
    "        self.wo = nn.Linear(\n",
    "            args.n_heads * self.head_dim,\n",
    "            args.dim,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        self.cache_k = torch.zeros(\n",
    "            (\n",
    "                args.max_batch_size,\n",
    "                args.max_seq_len,\n",
    "                self.n_local_kv_heads,\n",
    "                self.head_dim,\n",
    "            )\n",
    "        )\n",
    "        self.cache_v = torch.zeros(\n",
    "            (\n",
    "                args.max_batch_size,\n",
    "                args.max_seq_len,\n",
    "                self.n_local_kv_heads,\n",
    "                self.head_dim,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        start_pos: int,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "    ):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "#         print('output of xq values:', xq.shape, xq)\n",
    "#         print('output of xk values:', xk.shape, xk)\n",
    "#         print('output of xv values:', xv.shape, xv)\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "        \n",
    "        self.cache_k = self.cache_k.to(xq)\n",
    "        self.cache_v = self.cache_v.to(xq)\n",
    "\n",
    "        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n",
    "        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n",
    "        \n",
    "        keys = self.cache_k[:bsz, : start_pos + seqlen]\n",
    "        values = self.cache_v[:bsz, : start_pos + seqlen]\n",
    "#         print('output of xq cache values:', xq.shape, xq)\n",
    "#         print('output of xk cache values:', xk.shape, xk)\n",
    "\n",
    "        # repeat k/v heads if n_kv_heads < n_heads\n",
    "        keys = repeat_kv(keys, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)\n",
    "        values = repeat_kv(values, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)\n",
    "\n",
    "        xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        print('output of xq values:', xq.shape, xq)\n",
    "        print('output of xk values:', keys.shape, keys)\n",
    "        print('output of xv values:', values.shape, values)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "#         if mask is not None:\n",
    "#             scores = scores + mask  # (bs, n_local_heads, seqlen, cache_len + seqlen)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        output = torch.matmul(scores, values)  # (bs, n_local_heads, seqlen, head_dim)\n",
    "        print('output of dot-query-attention:', output)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "        print('output of dot-query-attention:', output)\n",
    "        output = self.wo(output)\n",
    "        print('output of attention layer:', output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        multiple_of: int,\n",
    "        ffn_dim_multiplier: Optional[float],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        # custom dim factor multiplier\n",
    "        if ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "        \n",
    "        self.w1 = nn.Linear(\n",
    "            dim, hidden_dim, bias=False\n",
    "        )\n",
    "        self.w2 = nn.Linear(\n",
    "            hidden_dim, dim, bias=False\n",
    "        )\n",
    "        self.w3 = nn.Linear(\n",
    "            dim, hidden_dim, bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('ffn_w1 input:', x)\n",
    "        our_w1 = self.w1(x)\n",
    "        our_w2 = self.w3(x)\n",
    "        our_w3 = self.w2(F.silu(our_w1) * our_w2)\n",
    "        print('our ffn_w1:', our_w1.shape, our_w1)\n",
    "        print('our ffn_w2:', our_w2.shape, our_w2)\n",
    "        print('our ffn_w3:', our_w3.shape, our_w3)\n",
    "        return our_w3\n",
    "        # return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.attention = Attention(args)\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim=args.dim,\n",
    "            hidden_dim=4 * args.dim,\n",
    "            multiple_of=args.multiple_of,\n",
    "            ffn_dim_multiplier=args.ffn_dim_multiplier,\n",
    "        )\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        start_pos: int,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "    ):\n",
    "        print('output of attention_norm', self.attention_norm(x))\n",
    "        h = x + self.attention.forward(\n",
    "            self.attention_norm(x), start_pos, freqs_cis, mask\n",
    "        )\n",
    "        \n",
    "        print('output of ffn_norm:', self.ffn_norm(h))\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, params: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.vocab_size = params.vocab_size\n",
    "        self.n_layers = params.n_layers\n",
    "\n",
    "        self.tok_embeddings = nn.Embedding(\n",
    "            params.vocab_size, params.dim\n",
    "        )\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(params.n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id, params))\n",
    "\n",
    "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
    "        self.output = nn.Linear(\n",
    "            params.dim, params.vocab_size, bias=False\n",
    "        )\n",
    "\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            self.params.dim // self.params.n_heads, self.params.max_seq_len * 2\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
    "        _bsz, seqlen = tokens.shape\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        # print('output of embedding:', h)\n",
    "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
    "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
    "\n",
    "        mask = None\n",
    "        if seqlen > 1:\n",
    "            mask = torch.full(\n",
    "                (1, 1, seqlen, seqlen), float(\"-inf\"), device=tokens.device\n",
    "            )\n",
    "            mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_cis, mask)\n",
    "        h = self.norm(h)\n",
    "        print('output of out_norm: ', h)\n",
    "        output = self.output(h).float()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f55018a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ModelArgs()\n",
    "model = Transformer(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d871f75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input of embedding: tensor([[0, 1]])\n",
      "output of attention_norm tensor([[[-0.1475,  0.9904,  0.6120,  ..., -1.4483, -1.0170,  0.4832],\n",
      "         [ 0.0691,  1.2589, -1.7623,  ...,  0.1090,  0.0561, -0.2484]]])\n",
      "output of xq values: torch.Size([1, 1, 2, 4096]) tensor([[[[-0.5556,  0.5313, -0.2729,  ..., -0.0445, -0.8422, -0.7842],\n",
      "          [-0.3013, -0.2177,  0.0813,  ..., -0.9850,  0.0838,  0.3099]]]])\n",
      "output of xk values: torch.Size([1, 1, 2, 4096]) tensor([[[[ 0.0187, -0.3398, -0.0273,  ..., -0.6063, -0.1210,  0.6284],\n",
      "          [-0.1316,  0.2630, -0.1104,  ..., -0.0609,  0.0440, -1.2018]]]])\n",
      "output of xv values: torch.Size([1, 1, 2, 4096]) tensor([[[[ 1.0623,  0.3809,  0.2658,  ...,  0.6872,  0.7183, -0.3094],\n",
      "          [-0.3912, -0.6001,  0.4004,  ..., -0.5476,  0.0993, -0.2363]]]])\n",
      "output of dot-query-attention: tensor([[[[ 0.0416, -0.3080,  0.3603,  ..., -0.1799,  0.2836, -0.2580],\n",
      "          [ 0.4193, -0.0531,  0.3253,  ...,  0.1410,  0.4445, -0.2770]]]])\n",
      "output of dot-query-attention: tensor([[[ 0.0416, -0.3080,  0.3603,  ..., -0.1799,  0.2836, -0.2580],\n",
      "         [ 0.4193, -0.0531,  0.3253,  ...,  0.1410,  0.4445, -0.2770]]])\n",
      "output of attention layer: tensor([[[ 0.0071,  0.3832,  0.1635,  ..., -0.1463, -0.2247,  0.2247],\n",
      "         [-0.0625,  0.2945,  0.1445,  ..., -0.1993, -0.0273,  0.1618]]])\n",
      "output of ffn_norm: tensor([[[-0.1363,  1.3404,  0.7559,  ..., -1.5515, -1.2098,  0.6912],\n",
      "         [ 0.0065,  1.5189, -1.5816,  ..., -0.0884,  0.0282, -0.0846]]])\n",
      "ffn_w1 input: tensor([[[-0.1363,  1.3404,  0.7559,  ..., -1.5515, -1.2098,  0.6912],\n",
      "         [ 0.0065,  1.5189, -1.5816,  ..., -0.0884,  0.0282, -0.0846]]])\n",
      "our ffn_w1: torch.Size([1, 2, 11008]) tensor([[[-1.2659, -0.2445,  0.3540,  ..., -0.0406,  0.3222,  0.0649],\n",
      "         [ 0.5843, -0.0013,  0.4688,  ..., -0.3533,  0.2198,  0.7443]]])\n",
      "our ffn_w2: torch.Size([1, 2, 11008]) tensor([[[ 0.2960,  0.5197,  0.2276,  ..., -0.0221, -0.9339, -0.4228],\n",
      "         [-0.0939,  0.2793, -0.6708,  ...,  0.4294,  0.6446, -0.0934]]])\n",
      "our ffn_w3: torch.Size([1, 2, 4096]) tensor([[[ 0.1322,  0.0023,  0.1406,  ...,  0.1103, -0.0309,  0.0643],\n",
      "         [-0.1358, -0.0933,  0.0273,  ...,  0.0732,  0.0076, -0.2111]]])\n",
      "output of out_norm:  tensor([[[-0.0059,  1.3381,  0.8917,  ..., -1.4376, -1.2361,  0.7521],\n",
      "         [-0.1256,  1.4190, -1.5456,  ..., -0.0167,  0.0354, -0.2894]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0270,  0.3960],\n",
       "         [-0.7029,  0.6300]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.zeros((1, 2), dtype=torch.long)\n",
    "X[0][0] = 0\n",
    "X[0][1] = 1\n",
    "\n",
    "print('input of embedding:', X)\n",
    "model(X, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a08cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c4b8214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['norm.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "478347a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.state_dict()['tok_embeddings.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36f50e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['tok_embeddings.weight', 'layers.0.attention.wq.weight', 'layers.0.attention.wk.weight', 'layers.0.attention.wv.weight', 'layers.0.attention.wo.weight', 'layers.0.feed_forward.w1.weight', 'layers.0.feed_forward.w2.weight', 'layers.0.feed_forward.w3.weight', 'layers.0.attention_norm.weight', 'layers.0.ffn_norm.weight', 'norm.weight', 'output.weight'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "573085c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 11008])\n",
      "torch.Size([4096, 11008])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict()['layers.0.feed_forward.w1.weight'].transpose(1, 0).shape)\n",
    "print(model.state_dict()['layers.0.feed_forward.w3.weight'].transpose(1, 0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b14bc055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "file = open(\"/home/seungbaek/hdd/projects/nntrainer/build/Applications/LLaMA/jni/llama_v2.bin\", \"wb\")\n",
    "\n",
    "def save_llama_to_bin(params, n_layer = 32, n_head = 32):\n",
    "    def save_weight(weight):\n",
    "        np.array(weight).tofile(file)\n",
    "\n",
    "    def save_embedding(weight):\n",
    "        save_weight(weight)\n",
    "\n",
    "    def save_attention(weights, layer_name, n_head = 32):        \n",
    "        save_weight(params[layer_name + 'attention_norm' + '.weight'])\n",
    "        split_size = (4096 // n_head)\n",
    "        for head_idx in range(1, n_head+1):            \n",
    "            st_idx = (4096 - split_size * head_idx)\n",
    "            end_idx = st_idx + split_size\n",
    "            save_weight(params[layer_name + 'attention.wv' + '.weight'][st_idx:end_idx, :].permute(1, 0))\n",
    "            \n",
    "        for head_idx in range(1, n_head+1):\n",
    "            st_idx = (4096 - split_size * head_idx)\n",
    "            end_idx = st_idx + split_size\n",
    "            save_weight(params[layer_name + 'attention.wk' + '.weight'][st_idx:end_idx, :].permute(1, 0))\n",
    "\n",
    "        for head_idx in range(1, n_head+1):\n",
    "            st_idx = (4096 - split_size * head_idx)\n",
    "            end_idx = st_idx + split_size\n",
    "            save_weight(params[layer_name + 'attention.wq' + '.weight'][st_idx:end_idx, :].permute(1, 0)) # It includes multiple heads\n",
    "        \n",
    "        save_weight(params[layer_name + 'attention.wo' + '.weight'].permute(1, 0))\n",
    "\n",
    "    def save_feed_forward(weights, layer_name):\n",
    "        save_weight(params[layer_name + 'ffn_norm' + '.weight'])        \n",
    "        \n",
    "        save_weight(params[layer_name + 'feed_forward.w3' + '.weight'].permute(1, 0))\n",
    "        save_weight(params[layer_name + 'feed_forward.w1' + '.weight'].permute(1, 0))        \n",
    "        save_weight(params[layer_name + 'feed_forward.w2' + '.weight'].permute(1, 0))\n",
    "\n",
    "    # save weights of embedding layer\n",
    "    save_embedding(params['tok_embeddings.weight'])\n",
    "    \n",
    "    # save weights of attention layers\n",
    "    for layer_idx in range(n_layer):\n",
    "        save_attention(params, 'layers.{}.'.format(layer_idx), n_head)\n",
    "        save_feed_forward(params, 'layers.{}.'.format(layer_idx))\n",
    "        \n",
    "    save_weight(params['norm.weight'])\n",
    "    \n",
    "    save_weight(params['output.weight'].permute(1, 0))\n",
    "    \n",
    "save_llama_to_bin(model.state_dict(), n_layer = params.n_layers, n_head = params.n_heads)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
