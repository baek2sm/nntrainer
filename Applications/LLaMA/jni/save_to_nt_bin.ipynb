{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e750ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# w1 = torch.load('./llama_7b/pytorch_model-00001-of-00002.bin')\n",
    "# w2 = torch.load('./llama_7b/pytorch_model-00002-of-00002.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f04f5679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import fairscale.nn.model_parallel.initialize as fs_init\n",
    "from fairscale.nn.model_parallel.initialize import (\n",
    "    get_model_parallel_rank,\n",
    "    initialize_model_parallel,\n",
    "    model_parallel_is_initialized,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from fairscale.nn.model_parallel.layers import (\n",
    "    ColumnParallelLinear,\n",
    "    ParallelEmbedding,\n",
    "    RowParallelLinear,\n",
    ")\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "seed = 2021\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 4096\n",
    "    n_layers: int = 2\n",
    "    n_heads: int = 2\n",
    "    n_kv_heads: Optional[int] = None\n",
    "    vocab_size: int = 2  # defined later by tokenizer\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    norm_eps: float = 1e-5\n",
    "\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 2048\n",
    "\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)  # type: ignore\n",
    "    freqs = torch.outer(t, freqs).float()  # type: ignore\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n",
    "    bs, slen, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:, :, :, None, :]\n",
    "        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n",
    "        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n",
    "    )\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        model_parallel_size = 1 # fs_init.get_model_parallel_world_size()\n",
    "        self.n_local_heads = args.n_heads // model_parallel_size\n",
    "        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.wq = nn.Linear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False\n",
    "        )\n",
    "        self.wk = nn.Linear(\n",
    "            args.dim,\n",
    "            self.n_kv_heads * self.head_dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.wv = nn.Linear(\n",
    "            args.dim,\n",
    "            self.n_kv_heads * self.head_dim,\n",
    "            bias=False\n",
    "        )\n",
    "        self.wo = nn.Linear(\n",
    "            args.n_heads * self.head_dim,\n",
    "            args.dim,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        self.cache_k = torch.zeros(\n",
    "            (\n",
    "                args.max_batch_size,\n",
    "                args.max_seq_len,\n",
    "                self.n_local_kv_heads,\n",
    "                self.head_dim,\n",
    "            )\n",
    "        )\n",
    "        self.cache_v = torch.zeros(\n",
    "            (\n",
    "                args.max_batch_size,\n",
    "                args.max_seq_len,\n",
    "                self.n_local_kv_heads,\n",
    "                self.head_dim,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        start_pos: int,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "    ):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "#         print('output of xq values:', xq)\n",
    "#         print('output of xk values:', xk)\n",
    "#         print('output of xv values:', xv)\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "\n",
    "        self.cache_k = self.cache_k.to(xq)\n",
    "        self.cache_v = self.cache_v.to(xq)\n",
    "\n",
    "        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n",
    "        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n",
    "\n",
    "        keys = self.cache_k[:bsz, : start_pos + seqlen]\n",
    "        values = self.cache_v[:bsz, : start_pos + seqlen]\n",
    "\n",
    "        # repeat k/v heads if n_kv_heads < n_heads\n",
    "        keys = repeat_kv(keys, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)\n",
    "        values = repeat_kv(values, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)\n",
    "\n",
    "        xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores + mask  # (bs, n_local_heads, seqlen, cache_len + seqlen)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        output = torch.matmul(scores, values)  # (bs, n_local_heads, seqlen, head_dim)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "        output = self.wo(output)\n",
    "        print('output of attention layer:', output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        multiple_of: int,\n",
    "        ffn_dim_multiplier: Optional[float],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        # custom dim factor multiplier\n",
    "        if ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "        \n",
    "        self.w1 = nn.Linear(\n",
    "            dim, hidden_dim, bias=False\n",
    "        )\n",
    "        self.w2 = nn.Linear(\n",
    "            hidden_dim, dim, bias=False\n",
    "        )\n",
    "        self.w3 = nn.Linear(\n",
    "            dim, hidden_dim, bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('ffn_w1 input:', x)\n",
    "        our_w1 = self.w1(x)\n",
    "        our_w2 = self.w3(x)\n",
    "        our_w3 = self.w2(F.silu(our_w1) * our_w2)\n",
    "        print('our ffn_w1:', our_w1.shape, our_w1)\n",
    "        print('our ffn_w2:', our_w2.shape, our_w2)\n",
    "        print('our ffn_w3:', our_w3.shape, our_w3)\n",
    "        return our_w3\n",
    "        # return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.attention = Attention(args)\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim=args.dim,\n",
    "            hidden_dim=4 * args.dim,\n",
    "            multiple_of=args.multiple_of,\n",
    "            ffn_dim_multiplier=args.ffn_dim_multiplier,\n",
    "        )\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        start_pos: int,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "    ):\n",
    "        print('output of attention_norm', self.attention_norm(x))\n",
    "        h = x + self.attention.forward(\n",
    "            self.attention_norm(x), start_pos, freqs_cis, mask\n",
    "        )\n",
    "        \n",
    "        print('output of ffn_norm:', self.ffn_norm(h))\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, params: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.vocab_size = params.vocab_size\n",
    "        self.n_layers = params.n_layers\n",
    "\n",
    "        self.tok_embeddings = nn.Embedding(\n",
    "            params.vocab_size, params.dim\n",
    "        )\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(params.n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id, params))\n",
    "\n",
    "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
    "        self.output = nn.Linear(\n",
    "            params.dim, params.vocab_size, bias=False\n",
    "        )\n",
    "\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            self.params.dim // self.params.n_heads, self.params.max_seq_len * 2\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
    "        _bsz, seqlen = tokens.shape\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        # print('output of embedding:', h)\n",
    "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
    "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
    "\n",
    "        mask = None\n",
    "        if seqlen > 1:\n",
    "            mask = torch.full(\n",
    "                (1, 1, seqlen, seqlen), float(\"-inf\"), device=tokens.device\n",
    "            )\n",
    "            mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_cis, mask)\n",
    "        h = self.norm(h)\n",
    "        print('output of out_norm: ', h)\n",
    "        output = self.output(h).float()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f55018a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ModelArgs()\n",
    "model = Transformer(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d871f75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input of embedding: tensor([[1]])\n",
      "output of attention_norm tensor([[[ 0.0691,  1.2589, -1.7623,  ...,  0.1090,  0.0561, -0.2484]]])\n",
      "output of attention layer: tensor([[[ 0.0868,  0.4848,  0.1852,  ..., -0.0855, -0.4508,  0.2967]]])\n",
      "output of ffn_norm: tensor([[[ 0.1484,  1.6586, -1.4997,  ...,  0.0223, -0.3757,  0.0461]]])\n",
      "ffn_w1 input: tensor([[[ 0.1484,  1.6586, -1.4997,  ...,  0.0223, -0.3757,  0.0461]]])\n",
      "our ffn_w1: torch.Size([1, 1, 11008]) tensor([[[ 0.7358, -0.1107,  0.4632,  ..., -0.4656,  0.3076,  0.7836]]])\n",
      "our ffn_w2: torch.Size([1, 1, 11008]) tensor([[[ 0.0739,  0.2551, -0.5209,  ...,  0.3584,  0.7982, -0.2969]]])\n",
      "our ffn_w3: torch.Size([1, 1, 4096]) tensor([[[-0.1175, -0.0929,  0.0018,  ...,  0.1145, -0.0024, -0.1818]]])\n",
      "output of attention_norm tensor([[[ 0.0362,  1.5597, -1.4880,  ...,  0.1304, -0.3754, -0.1261]]])\n",
      "output of attention layer: tensor([[[-0.4230,  0.1057,  0.2668,  ..., -0.4855, -0.1360,  0.1278]]])\n",
      "output of ffn_norm: tensor([[[-0.3463,  1.5800, -1.1765,  ..., -0.3128, -0.4798, -0.0050]]])\n",
      "ffn_w1 input: tensor([[[-0.3463,  1.5800, -1.1765,  ..., -0.3128, -0.4798, -0.0050]]])\n",
      "our ffn_w1: torch.Size([1, 1, 11008]) tensor([[[ 1.2172,  0.4269, -0.7997,  ...,  0.6046, -0.1687, -0.0163]]])\n",
      "our ffn_w2: torch.Size([1, 1, 11008]) tensor([[[ 0.1678, -0.9722,  0.4118,  ...,  0.3703,  0.5884, -0.2776]]])\n",
      "our ffn_w3: torch.Size([1, 1, 4096]) tensor([[[ 0.0606, -0.0790, -0.1659,  ..., -0.0182, -0.0525, -0.0775]]])\n",
      "output of out_norm:  tensor([[[-0.2897,  1.4985, -1.3167,  ..., -0.3269, -0.5234, -0.0742]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2173,  0.1302]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((1, 1), dtype=torch.long)\n",
    "print('input of embedding:', X)\n",
    "model(X, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a08cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c4b8214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['norm.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "478347a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.state_dict()['tok_embeddings.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36f50e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['tok_embeddings.weight', 'layers.0.attention.wq.weight', 'layers.0.attention.wk.weight', 'layers.0.attention.wv.weight', 'layers.0.attention.wo.weight', 'layers.0.feed_forward.w1.weight', 'layers.0.feed_forward.w2.weight', 'layers.0.feed_forward.w3.weight', 'layers.0.attention_norm.weight', 'layers.0.ffn_norm.weight', 'layers.1.attention.wq.weight', 'layers.1.attention.wk.weight', 'layers.1.attention.wv.weight', 'layers.1.attention.wo.weight', 'layers.1.feed_forward.w1.weight', 'layers.1.feed_forward.w2.weight', 'layers.1.feed_forward.w3.weight', 'layers.1.attention_norm.weight', 'layers.1.ffn_norm.weight', 'norm.weight', 'output.weight'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "573085c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 11008])\n",
      "torch.Size([4096, 11008])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict()['layers.0.feed_forward.w1.weight'].transpose(1, 0).shape)\n",
    "print(model.state_dict()['layers.0.feed_forward.w3.weight'].transpose(1, 0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b14bc055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "file = open(\"/home/seungbaek/hdd/projects/nntrainer/build/Applications/LLaMA/jni/llama_v2.bin\", \"wb\")\n",
    "\n",
    "def save_llama_to_bin(params, n_layer = 32, n_head = 32):\n",
    "    def save_weight(weight):\n",
    "        np.array(weight).tofile(file)\n",
    "\n",
    "    def save_embedding(weight):\n",
    "        save_weight(weight)\n",
    "\n",
    "    def save_attention(weights, layer_name, n_head = 32):        \n",
    "        save_weight(params[layer_name + 'attention_norm' + '.weight'])\n",
    "        split_size = (4096 // n_head)\n",
    "        for head_idx in range(1, n_head+1):            \n",
    "            st_idx = (4096 - split_size * head_idx)\n",
    "            end_idx = st_idx + split_size\n",
    "            save_weight(params[layer_name + 'attention.wv' + '.weight'][st_idx:end_idx, :].permute(1, 0))\n",
    "            \n",
    "        for head_idx in range(1, n_head+1):\n",
    "            st_idx = (4096 - split_size * head_idx)\n",
    "            end_idx = st_idx + split_size\n",
    "            save_weight(params[layer_name + 'attention.wk' + '.weight'][st_idx:end_idx, :].permute(1, 0))\n",
    "\n",
    "        for head_idx in range(1, n_head+1):\n",
    "            st_idx = (4096 - split_size * head_idx)\n",
    "            end_idx = st_idx + split_size\n",
    "            save_weight(params[layer_name + 'attention.wq' + '.weight'][st_idx:end_idx, :].permute(1, 0)) # It includes multiple heads\n",
    "        \n",
    "        save_weight(params[layer_name + 'attention.wo' + '.weight'].permute(1, 0))\n",
    "\n",
    "    def save_feed_forward(weights, layer_name):\n",
    "        save_weight(params[layer_name + 'ffn_norm' + '.weight'])        \n",
    "        \n",
    "        save_weight(params[layer_name + 'feed_forward.w3' + '.weight'].permute(1, 0))\n",
    "        save_weight(params[layer_name + 'feed_forward.w1' + '.weight'].permute(1, 0))        \n",
    "        save_weight(params[layer_name + 'feed_forward.w2' + '.weight'].permute(1, 0))\n",
    "\n",
    "    # save weights of embedding layer\n",
    "    save_embedding(params['tok_embeddings.weight'])\n",
    "    \n",
    "    # save weights of attention layers\n",
    "    for layer_idx in range(n_layer):\n",
    "        save_attention(params, 'layers.{}.'.format(layer_idx), n_head)\n",
    "        save_feed_forward(params, 'layers.{}.'.format(layer_idx))\n",
    "        \n",
    "    save_weight(params['norm.weight'])\n",
    "    \n",
    "    save_weight(params['output.weight'].permute(1, 0))\n",
    "    \n",
    "save_llama_to_bin(model.state_dict(), n_layer = params.n_layers, n_head = params.n_heads)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
